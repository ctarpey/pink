{
 "metadata": {
  "name": "",
  "signature": "sha256:b8b15163fa7b76d5e1cf288c82b63e16e4933215d92a25b441cf617e9aa99019"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Building pink salmon linkage maps\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Step 1. Filter the haplotypes and flag duplicates using Ryan's pipeline\n",
      "\n",
      "- Prepare the haplotype file from Stacks \n",
      "- Edit the pipeline; hardcode our file paths and family names\n",
      "\n",
      "Because I did not export the haplotype file in the .sql format, I made formatting changes to the haplotype file to make it ready for the pipeline. \n",
      "\n",
      "The first change to the file is to cut out all the consensus genotypes, they are not useful in the map and take up space. Garrett wrote this script in perl that takes the haplotype file and outputs a version witout the consensus:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#removeConsensus.pl\n",
      "#March 30 2015\n",
      "\n",
      "#!/usr/bin/perl -w\n",
      "use strict;\n",
      "\n",
      "my$header=<>;\n",
      "chomp $header;\n",
      "my@headers=split \"\\t\", $header;\n",
      "print \"$header\\n\";\n",
      "\n",
      "while(my$line=<>){\n",
      "\tmy$consensus=0;\n",
      "\tmy$other=0;\n",
      "    chomp $line;\n",
      "    my@genos=split \"\\t\", $line;\n",
      "    foreach my$i (2..$#genos){\n",
      "        if ($genos[$i] =~ /consensus/){\n",
      "\t\t\t$consensus=1;\n",
      "        }\n",
      "    }\n",
      "\tunless($consensus==1){\n",
      "\t\tprint \"$line\\n\";\n",
      "\t}\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After removing the consensus genotypes, I opened up the haplotype file in Excel to add some columns that the sql file has but the haplotype file is missing.\n",
      "\n",
      "The file needs 5 empty columns after the catalog id column. It also needs 4 empty columns after the count column and one column with the header 'deleveraging' after these newly added 4 empty columns. The deleveraging column should be autofilled with zeros. These are the headers: # Catalog ID, Annotation, Chr, BP, Consensus, Num Parents, Num Progeny, Num SNPs, SNPs, Num Alleles, Alleles, Deleveraging. \n",
      "\n",
      "After the deleveraging column should be columns for the female parents of the families represented. They do not have to have the actual genotypes, Ryan's pipeline only requires their names, but I added the genotypes. Because my parents had been run through Stacks with another set of samples and had not had their consensus sequences removed, I used vlookups to populate their genopytes. The pipeline takes the offspring genotypes and rebuilds the parent genotypes, so I wanted to be able to compare those genotypes to those from Stacks for the parents. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With the haplotype file saved as a text file and ready to be used on the pipeline, the next step is to edit the code for the pipeline. \n",
      "\n",
      "Change:\n",
      "- the family names \n",
      "- the sample numbers \n",
      "- the file paths \n",
      "\n",
      "I used find replace to update the names of his four families with mine: \n",
      "\n",
      "- Hap_A_all to PBIRD13X_108H\n",
      "- Hap_B_all to PBIRD13x_110H\n",
      "- Hap10_Parent_140 PHOOD11x_110H\n",
      "- Hap14 to PHOOD11x_05H\n",
      "\n",
      "I added the number of offspring for each family: \n",
      "\n",
      "- PBIRD13X_108H 81\n",
      "- PBIRD13x_110H 82\n",
      "- PHOOD11x_110H 93\n",
      "- PHOOD11x_05H 92\n",
      "\n",
      "Finally, I changed all the file paths to folders on my external hard drive that have the haplotype file and the pipeline code. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Run the pipeline\n",
      "Here is the actual pipeline code, the supporting programs and files can all be found on the network drives. I copied the whole file locally and I ran the pipeline in Canopy, one chunck at a time to make sure that there were no issues. The pipeline that Ryan wrote is much longer than I have printed in this notebook, but we only used the portion here that ends with writing the loci in the Mst format. In the original script this is about line 244. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "os.chdir(\"F:/Analysis/Pop_analysis/Mapping/Stacks_mapping/Python\")\n",
      "\n",
      "import cPickle as pickle\n",
      "import numpy\n",
      "\n",
      "import Stacks_SQL_class\n",
      "from models_to_test import models_to_test\n",
      "\n",
      "#Added to attempt phase switching\n",
      "import switch_allele_functions as switchAlleles\n",
      "\n",
      "# Load data created with export_sql.pl \n",
      "chin_fams = Stacks_SQL_class.Stacks_SQL(path = \"F:/Analysis/Pop_analysis/Mapping/Tarpey_Mapping_haplotypes_formatted.txt\", \n",
      "    name = 'pink',\n",
      "    num_parents = 4, \n",
      "    num_offspring = 348, \n",
      "    parent_offspring = (81, 82, 93, 92)\n",
      "    )\n",
      "    \n",
      "# Kick out Loci (within families) with too many missing genotypes\n",
      "too_many_missing_01 = chin_fams.remove_missing_within_cross(max_miss = .25, parent = 'PBIRD13X_108H')\n",
      "too_many_missing_02 = chin_fams.remove_missing_within_cross(max_miss = .25, parent = 'PBIRD13X_110H')\n",
      "too_many_missing_03 = chin_fams.remove_missing_within_cross(max_miss = .25, parent = 'PHOOD11X_01H')\n",
      "too_many_missing_04 = chin_fams.remove_missing_within_cross(max_miss = .25, parent = 'PHOOD11X_05H')\n",
      "\n",
      "\n",
      "# Kick out loci (within families) with too many alleles\n",
      "#too_many_alleles_01 = chum_fams.remove_max_alleles_within_cross(max_alleles = 4, parent = 'CMUW10X_0001')\n",
      "too_many_alleles_01 = chin_fams.remove_max_alleles_within_cross(max_alleles = 4, parent = 'PBIRD13X_108H')\n",
      "too_many_alleles_02 = chin_fams.remove_max_alleles_within_cross(max_alleles = 4, parent = 'PBIRD13X_110H')\n",
      "too_many_alleles_03 = chin_fams.remove_max_alleles_within_cross(max_alleles = 4, parent = 'PHOOD11X_01H')\n",
      "too_many_alleles_04 = chin_fams.remove_max_alleles_within_cross(max_alleles = 4, parent = 'PHOOD11X_05H')\n",
      "\n",
      "\n",
      "# Purge CatIDs that failed previous tests in all families\n",
      "purged = chin_fams.purge_absent_catIDs()\n",
      "\n",
      "# Calculate model results using models found in in \"models_to_test.py\"\n",
      "print(\"Calculating PSV Model Likelihoods\")\n",
      "chin_fams.set_model_results(to_test = models_to_test, epsilon_bound_high = 0.1, epsilon_bound_low = 0.01)\n",
      "\n",
      "#mappable_01, stats_01 = chum_fams.find_mappable_markers(parent = 'CMUW10X_0001', models_to_test = models_to_test)\n",
      "mappable_01, stats_01 = chin_fams.find_mappable_markers(parent = 'PBIRD13X_108H', models_to_test = models_to_test)\n",
      "mappable_02, stats_02 = chin_fams.find_mappable_markers(parent = 'PBIRD13X_110H', models_to_test = models_to_test)\n",
      "mappable_03, stats_03 = chin_fams.find_mappable_markers(parent = 'PHOOD11X_01H', models_to_test = models_to_test)\n",
      "mappable_04, stats_04 = chin_fams.find_mappable_markers(parent = 'PHOOD11X_05H', models_to_test = models_to_test)\n",
      "\n",
      "\n",
      "def write_stats(filename, stats):\n",
      "    with open(filename, 'w') as OUTFILE:\n",
      "        for catID, values in stats.items():\n",
      "            OUTFILE.write(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n\".format(\n",
      "                catID, \n",
      "                values.epsilon,\n",
      "                values.best_model,\n",
      "                values.best_likelihood,\n",
      "                values.second_best_model,\n",
      "                values.second_best_likelihood,\n",
      "                values.x1_seg_pval,\n",
      "                values.x2_seg_pval\n",
      "                )\n",
      "            )\n",
      "            \n",
      "#from psv_working import stats_08\n",
      "#write_stats('/olympus/WORK/WAPLES/Stacks_mapping/Chum_data/psv/chum_01.stats', stats_01)\n",
      "write_stats(\"F:/Analysis/Pop_analysis/Mapping/PBIRD13X_108H_stats_unfiltered.txt\", stats_01)\n",
      "write_stats(\"F:/Analysis/Pop_analysis/Mapping/PBIRD13X_110H_stats_unfiltered.txt\", stats_02)\n",
      "write_stats(\"F:/Analysis/Pop_analysis/Mapping/PHOOD11X_01H_stats_unfiltered.txt\", stats_03)\n",
      "write_stats(\"F:/Analysis/Pop_analysis/Mapping/PHOOD11X_05H_stats_unfiltered.txt\", stats_04)\n",
      "\n",
      "#########################################################################################################################\n",
      "##############################Write Mst Map File for Unfiltered Data#####################################################\n",
      "# Write loci to MSTmap format\n",
      "chin_fams.write_mstmap('PBIRD13X_108H', 'F:/Analysis/Pop_analysis/Mapping/PBIRD13X_108H_mstmap_unfiltered.txt', mappable_01)\n",
      "chin_fams.write_mstmap('PBIRD13X_110H', 'F:/Analysis/Pop_analysis/Mapping/PBIRD13X_110H_mstmap_unfiltered.txt', mappable_02)\n",
      "chin_fams.write_mstmap('PHOOD11X_01H', 'F:/Analysis/Pop_analysis/Mapping/PHOOD11X_01H_mstmap_unfiltered.txt', mappable_03)\n",
      "chin_fams.write_mstmap('PHOOD11X_05H', 'F:/Analysis/Pop_analysis/Mapping/PHOOD11X_05H_mstmap_unfiltered.txt', mappable_04)\n",
      "#########################################################################################################################\n",
      "\n",
      "\n",
      "# Conduct filtering on loci and catIDs\n",
      "# Kick out failed loci\n",
      "# segregation testing\n",
      "import mne\n",
      "import collections\n",
      "def filter_mappable_segregation(stats, alpha = .05):\n",
      "    stats_of_locus = dict()\n",
      "    loci = list()\n",
      "    raw_p_vals = list()\n",
      "    for catID in stats:\n",
      "        if stats[catID].x1_seg_pval == \"NA\":\n",
      "            pass\n",
      "        else:\n",
      "            raw_p_vals.append(stats[catID].x1_seg_pval)\n",
      "            loci.append(catID + \"_x1\")\n",
      "        if stats[catID].x2_seg_pval == \"NA\":\n",
      "            pass\n",
      "        else:\n",
      "            raw_p_vals.append(stats[catID].x2_seg_pval)\n",
      "            loci.append(catID + \"_x2\")\n",
      "    reject, pval_corrected = mne.stats.fdr_correction(pvals = raw_p_vals, alpha = alpha, method = 'indep')\n",
      "    seg_test_fdr = collections.namedtuple('seg_test_fdr', ['reject', 'pval_corrected', 'pval_raw'])\n",
      "    for locus, rej, pvc, pvr in zip(loci, reject, pval_corrected, raw_p_vals):\n",
      "        stats_of_locus[locus] = seg_test_fdr(rej, pvc, pvr)\n",
      "    return(stats_of_locus)\n",
      "# do segregation testing with FDR    \n",
      "\n",
      "seg_test_01 = filter_mappable_segregation(stats_01, alpha = 0.05)\n",
      "seg_test_02 = filter_mappable_segregation(stats_02, alpha = 0.05)\n",
      "seg_test_03 = filter_mappable_segregation(stats_03, alpha = 0.05)\n",
      "seg_test_04 = filter_mappable_segregation(stats_04, alpha = 0.05)\n",
      "\n",
      "# Print how many *would be* excluded\n",
      "print(\"Reject {} out of {} loci in fam_01\".format(sum([x.reject for x in seg_test_01.values()]), len([x.reject for x in seg_test_01.values()])))\n",
      "print(\"Reject {} out of {} loci in fam_02\".format(sum([x.reject for x in seg_test_02.values()]), len([x.reject for x in seg_test_02.values()])))\n",
      "print(\"Reject {} out of {} loci in fam_03\".format(sum([x.reject for x in seg_test_03.values()]), len([x.reject for x in seg_test_03.values()])))\n",
      "print(\"Reject {} out of {} loci in fam_04\".format(sum([x.reject for x in seg_test_04.values()]), len([x.reject for x in seg_test_04.values()])))\n",
      "\n",
      "failed_seg_test_01 = [k for k,v in seg_test_01.items() if v.reject]\n",
      "failed_seg_test_02 = [k for k,v in seg_test_02.items() if v.reject]\n",
      "failed_seg_test_03 = [k for k,v in seg_test_03.items() if v.reject]\n",
      "failed_seg_test_04 = [k for k,v in seg_test_04.items() if v.reject]\n",
      "\n",
      "failed_epsilon_test_01 = [k for k,v in stats_01.items() if v.epsilon > .2]\n",
      "failed_epsilon_test_02 = [k for k,v in stats_02.items() if v.epsilon > .2]\n",
      "failed_epsilon_test_03 = [k for k,v in stats_03.items() if v.epsilon > .2]\n",
      "failed_epsilon_test_04 = [k for k,v in stats_04.items() if v.epsilon > .2]\n",
      "\n",
      "failed_missing_test_01 = [k for k,v in mappable_01.items() if v.count('-')/float(len(v)) > .25]\n",
      "failed_missing_test_02 = [k for k,v in mappable_02.items() if v.count('-')/float(len(v)) > .25]\n",
      "failed_missing_test_03 = [k for k,v in mappable_03.items() if v.count('-')/float(len(v)) > .25]\n",
      "failed_missing_test_04 = [k for k,v in mappable_04.items() if v.count('-')/float(len(v)) > .25]\n",
      "\n",
      "\n",
      "def remove_locus(mappable, locus):\n",
      "    if locus in mappable:\n",
      "        del mappable[locus]\n",
      "        return(1)\n",
      "    else:\n",
      "        return(0)\n",
      "    \n",
      "def remove_catID(mappable, catID):\n",
      "    removed_count = 0\n",
      "    if catID+\"_x1\" in mappable:\n",
      "        del mappable[ catID+\"_x1\"]\n",
      "        removed_count +=1\n",
      "    if catID+\"_x2\" in mappable:\n",
      "        del mappable[ catID+\"_x2\"]\n",
      "        removed_count +=1\n",
      "    return(removed_count)\n",
      "\n",
      "def remove_catIDs(mappable, catIDs):\n",
      "    num_removed = 0\n",
      "    for catID in catIDs:\n",
      "        num_removed += remove_catID(mappable, catID)\n",
      "    print(\"Removed {} loci\".format(num_removed))\n",
      "    \n",
      "def remove_loci(mappable, loci):\n",
      "    num_removed = 0\n",
      "    for locus in loci:\n",
      "        num_removed += remove_locus(mappable, locus)\n",
      "    print(\"Removed {} loci\".format(num_removed))  \n",
      "    \n",
      "remove_catIDs(mappable_01, failed_epsilon_test_01)\n",
      "remove_catIDs(mappable_02, failed_epsilon_test_02)\n",
      "remove_catIDs(mappable_03, failed_epsilon_test_03)\n",
      "remove_catIDs(mappable_04, failed_epsilon_test_04)\n",
      "\n",
      "remove_loci(mappable_01, failed_missing_test_01)\n",
      "remove_loci(mappable_02, failed_missing_test_02)\n",
      "remove_loci(mappable_03, failed_missing_test_03)\n",
      "remove_loci(mappable_04, failed_missing_test_04)\n",
      "\n",
      "remove_loci(mappable_01, failed_seg_test_01)\n",
      "remove_loci(mappable_02, failed_seg_test_02)\n",
      "remove_loci(mappable_03, failed_seg_test_03)\n",
      "remove_loci(mappable_04, failed_seg_test_04)\n",
      "\n",
      "def write_mappable_stats(filename, stats, mappable):\n",
      "    with open(filename, 'w') as OUTFILE:\n",
      "        for catID, values in stats.items():\n",
      "            locally_mappable = sum((catID+\"_x1\" in mappable, catID+\"_x2\" in mappable))\n",
      "            OUTFILE.write(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n\".format(\n",
      "                catID,\n",
      "                locally_mappable, \n",
      "                values.epsilon,\n",
      "                values.best_model,\n",
      "                values.best_likelihood,\n",
      "                values.second_best_model,\n",
      "                values.second_best_likelihood,\n",
      "                values.x1_seg_pval,\n",
      "                values.x2_seg_pval\n",
      "                 \n",
      "                )\n",
      "            )\n",
      "\n",
      "write_mappable_stats(\"F:/Analysis/Pop_analysis/Mapping/PBIRD13X_108H_filtered.stats\", stats_01, mappable_01)\n",
      "write_mappable_stats(\"F:/Analysis/Pop_analysis/Mapping/PBIRD13X_110H_filtered.stats\", stats_02, mappable_02)\n",
      "write_mappable_stats(\"F:/Analysis/Pop_analysis/Mapping/PHOOD11X_01H_filtered.stats\", stats_03, mappable_03)\n",
      "write_mappable_stats(\"F:/Analysis/Pop_analysis/Mapping/PHOOD11X_05H_filtered.stats\", stats_04, mappable_04)\n",
      "\n",
      "# Write loci to MSTmap format\n",
      "chin_fams.write_mstmap('PBIRD13X_108H', 'F:/Analysis/Pop_analysis/Mapping/PBIRD13X_108H_mstmap_filtered.txt', mappable_01)\n",
      "chin_fams.write_mstmap('PBIRD13X_110H', 'F:/Analysis/Pop_analysis/Mapping/PBIRD13X_110H_mstmap_filtered.txt', mappable_02)\n",
      "chin_fams.write_mstmap('PHOOD11X_01H', 'F:/Analysis/Pop_analysis/Mapping/PHOOD11X_01H_mstmap_filtered.txt', mappable_03)\n",
      "chin_fams.write_mstmap('PHOOD11X_05H', 'F:/Analysis/Pop_analysis/Mapping/PHOOD11X_05H_mstmap_filtered.txt', mappable_04)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There were some errors that had to be addressed as I went along, here is the condensed version as a series of tips: \n",
      "\n",
      "- The families should be in order\n",
      "- The header row of the haplotype file should start with a # \n",
      "- Make sure that the family names that you put in the code of the pipeline match the beginning of the parent names in the haplotype file and that they match the beginninig of the offspring names. That's how the pipeline identifies the parents and the offpsring.\n",
      "\n",
      "As the pipeline runs, it prints some standard output to the screen. Here is some of the output that was generated while running all the offspring: \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "In [25]: # Kick out Loci (within families) with too many missing genotypes\n",
      "Removed 51945 catIDs from parent: PBIRD13X_108H for missingness\n",
      "Removed 72018 catIDs from parent: PBIRD13X_110H for missingness\n",
      "Removed 31878 catIDs from parent: PHOOD11X_01H for missingness\n",
      "Removed 32306 catIDs from parent: PHOOD11X_05H for missingness\n",
      "\n",
      "In [26]: # Kick out loci (within families) with too many alleles\n",
      "\n",
      "Removed 102 catIDs from parent: PBIRD13X_108H for too many alleles\n",
      "Removed 23 catIDs from parent: PBIRD13X_110H for too many alleles\n",
      "Removed 400 catIDs from parent: PHOOD11X_01H for too many alleles\n",
      "Removed 384 catIDs from parent: PHOOD11X_05H for too many alleles\n",
      "\n",
      "In [32]: # Conduct filtering on loci and catIDs\n",
      "    ...: # Kick out failed loci\n",
      "    ...: # segregation testing\n",
      "\t\n",
      "Reject 153 out of 3183 loci in fam_01\n",
      "Reject 69 out of 861 loci in fam_02\n",
      "Reject 441 out of 6064 loci in fam_03\n",
      "Reject 403 out of 5907 loci in fam_04"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Output and troubleshooting\n",
      "\n",
      "The pipeline produces files for each of the families, a the top of each is a summary. Here are the four summaries for the families run through the pipeline. The output is much lower for the 2013 haploid families than we would expect. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "population_type DH\n",
      "population_name PBIRD13X_108H\n",
      "distance_function kosambi\n",
      "cut_off_p_value 1e-07\n",
      "no_map_dist 15.0\n",
      "no_map_size 2\n",
      "missing_threshold 0.25\n",
      "estimation_before_clustering yes\n",
      "detect_bad_data yes\n",
      "objective_function COUNT\n",
      "number_of_loci 2575\n",
      "number_of_individual 81\n",
      "\n",
      "population_type DH\n",
      "population_name PBIRD13X_110H\n",
      "distance_function kosambi\n",
      "cut_off_p_value 1e-07\n",
      "no_map_dist 15.0\n",
      "no_map_size 2\n",
      "missing_threshold 0.25\n",
      "estimation_before_clustering yes\n",
      "detect_bad_data yes\n",
      "objective_function COUNT\n",
      "number_of_loci 384\n",
      "number_of_individual 82\n",
      "\n",
      "population_type DH\n",
      "population_name PHOOD11X_01H\n",
      "distance_function kosambi\n",
      "cut_off_p_value 1e-07\n",
      "no_map_dist 15.0\n",
      "no_map_size 2\n",
      "missing_threshold 0.25\n",
      "estimation_before_clustering yes\n",
      "detect_bad_data yes\n",
      "objective_function COUNT\n",
      "number_of_loci 5519\n",
      "number_of_individual 93\n",
      "\n",
      "population_type DH\n",
      "population_name PHOOD11X_05H\n",
      "distance_function kosambi\n",
      "cut_off_p_value 1e-07\n",
      "no_map_dist 15.0\n",
      "no_map_size 2\n",
      "missing_threshold 0.25\n",
      "estimation_before_clustering yes\n",
      "detect_bad_data yes\n",
      "objective_function COUNT\n",
      "number_of_loci 5363\n",
      "number_of_individual 92\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We looked for sources of the low number of filtered loci, and the main culprit seems to be a discernable difference in the amount of missing data per individual per family. (This was identified using the haplotype file) "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "average missing loci per fam\n",
      "PBIRD13X_108H\t41540.44444\n",
      "PBIRD13X_110H\t35492.33714\n",
      "PHOOD11X_01H\t26983.22581\n",
      "PHOOD11X_05H\t27143.47826"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Because the two 2013 families have so much more missing data I went back to the process_radtags output and looked for differences in the amount of sequence at that stage.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "average rad_tag output per family \n",
      "PBIRD13X_108H\t1,289,984\n",
      "PBIRD13X_110H\t1,376,567\n",
      "PHOOD11X_01H\t2,648,896\n",
      "PHOOD11X_05H\t2,825,765"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the cut off of a million retained reads, I counted the number of individuals in the 2013 families that would meet that threshold: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PBIRD13X_108H\t50\n",
      "PBIRD13X_110H\t33"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To test whether we got the low number of filtered loci from Ryans pipeline because of individuals that had abnormally low retained reads, and by extension probably high rates of missingness, I made a new input file to run in the pipeline that only has those samples that are greater than the 1 million sample reads. I called it _millions and re-ran the pipeline. The results for both 2013 families we were between 5,000 and 6,500 loci which is a marked improvement in number of filtered loci and in the range that we would expect. \n",
      "\n",
      "Family sizes of 50 and 33 are too small to make the linkage maps that we want, so we have to figure out a way to salvage more of the haploid data.\n",
      "\n",
      "Maybe one million is too stringent of a cut off? Wes looked through his haploids and said that he had individuals that were useful in his linkage map that had a read depth of about 500,000 sequences so somewhere around there might be a good place to start. \n",
      "\n",
      "Why is there such a large disparity between the number of retained reads between individuals when the bulk pooling and resequencing method was supposed to even out the number of reads per individuals. As of now, it looks like quite a few haploid individuals that should have been resequenecd based on the original retained reads, but were not. \n",
      "\n",
      "After digging around in old spreadsheets I identified two errors that means that we resequenced individuals that we did not need to resequence at the expence of those that we should have resequenced. This is what lead to the uneven coverage of sequence. \n",
      "\n",
      "The next steps are to test different levels of retained reads to find the lowest threshold of retained reads. The lower the retained read threshold, the greater the number of individuals avialable to build the map, which will help marker placement and reducing the fragmentation of the linkage groups. The flipside of that is, the lower the threshold, the fewer filtered loci are returned at the end of the pipeline, also leading to framgentation and low density of markers on the map. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To find the optimal balance where there are enough retained reads per individual to get about 5 thousand filtered loci out at the end of the pipeline but at the same time there are enough samples to make a useful map, I decided on a testing scheme. I will test different levels of retaiend reads and see through trial and error which has both the greatest number of individuals and produces aroung 5 thousand filtered loci. \n",
      "\n",
      "Here are the levels of retained reads that I will test: \n",
      "\n",
      "- one million \n",
      "- 900 thou\n",
      "- 800 thou\n",
      "- 700 thou\n",
      "- 600 thou\n",
      "- 500 thou \n",
      "- 400 thou\n",
      " \n",
      "Talking to Ryan, he recommended that I also take a look at the filter in the pipeline that removes individuals based on a missingness threshold. \n",
      "\n",
      "So I added another layer to my test. I will run the pipeline twice for each of the retained read numbers, once with the default of .25% misssingness and again with a lower threshold of .30% missingness. \n",
      "\n",
      "I constructed all the import files and made folders for each of the outputs of the tests, writing the pipeline to send the results into the folders. I put all the code for the pipeline back to back in one long script to run it all at once. The one think I was worried about was the standard output that gets printed to the screen as the code runs. I think it is really usefull and would certainly be lost from the earlier runs if it is not redirected to a log. \n",
      "\n",
      "I found a bit of code on stack overflow that redirects the standard output to a log file and added it to the beginning of the pipeline so that all the print commands would be saved somewhere. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This prints the standard output from an interactive console into a log file of your choosing\n",
      "\n",
      "import sys\n",
      "\n",
      "class Tee(object):\n",
      "    def __init__(self, *files):\n",
      "        self.files = files\n",
      "    def write(self, obj):\n",
      "        for f in self.files:\n",
      "            f.write(obj)\n",
      "\n",
      "f = open('run_psv_log_may6.txt', 'w')\n",
      "original = sys.stdout\n",
      "sys.stdout = Tee(sys.stdout, f)\n",
      "print \"test\"  # This will go to stdout and the file out.txt\n",
      "\n",
      "#use the original\n",
      "sys.stdout = original\n",
      "print \"This won't appear on file\"  # Only on stdout\n",
      "f.close()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Step 2: MST Map \n",
      "\n",
      "We have a better sense of the data we have to build the linkage maps, but the ultimate test of what maps we are able to build is to try to build them. We do not want to include individuals or loci at the expense of map quality. Without enough individuals, there will not be enough resolution in the map, but if we dont have enough loci (fewer individuals here leads to more loci) we won't have a dense  map. Using the results generated in the above tests, we will build 6 maps and compare them for mapped loci, linkage group number and size. Each family is run individually with MSTMap. \n",
      "\n",
      "We will use: \n",
      "\n",
      "- one million retained reads\n",
      "- 500 thousand retained reads \n",
      "- 700 thousand retained reads \n",
      "\n",
      "at both the .25 and .30 % missingness filters. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will build the final maps with LEP-Map as it does a better job calculating the map distance, but for these tests we will build the linkage maps with MSTMap because it is so much faster. \n",
      "\n",
      "MSTMap requires the data be phased, which ours is not. To get past this we will phase it by duplicating it: bulk phase method. We will duplicate the data and swap phase by making a copy of the data that has the exact opposite genotypes for all the individuals and combining the two sets.\n",
      "\n",
      "The resulting maps will have twice the number of linkage groups that we expect, one version represents the opposite phase on the same linkage gropu. Pink salmon have between 52 and 54 chromosomes, so for these haploids we would expect about half of that, ~26 linkage groups. Using the bulk phasing method doubles that, so the likage maps should have around 52 linkage groups. \n",
      "\n",
      "MSTMap takes the Mst output from Ryans pipeline. The genotypes are coded as a and b, with - demarkating missing data. There are no heterozygotes, Ryan's pipeline has split those out. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Background on the parameters used\n",
      "\n",
      "These are located at the top of the MST file that was created in Ryan's pipeline, not all the information there is covered here, some is self explanatory. \n",
      "\n",
      "The cutoff p-value:  decreasing it (more stringent) makes a more fragmented map, but if this is too stringent, then it causes overmerging of the linkage groups. The p-value is typically settled on by trial and error, where you change it until you have the right number of linkage groups. For small families you expect to use a smaller pvalue. We will test pvalue cutoffs between 1e-7 to 1e-3, though we expect the final one to be somewhere between 1e-7 and 1e-5. \n",
      "\n",
      "No map distance: at what point are the markers at the end left off the linkage group, typically it is set at 10cM or 15cM\n",
      "\n",
      "No map size: works in tandem with the no map distance to make the call on whether to make a gap in the linkage group or kick out the makers. 2 here means that if there are only two loci and they are 15 cM away from other markers do not include them.\n",
      "\n",
      "Missingness threshold: this was set in Ryans's pipeline. Here we used either .25 or .30\n",
      "\n",
      "Detect bad data: the program trusts the phasing you give it as an imput and it will flag any phase change as an error and it will not include the marker for that individuals. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Bulk phasing\n",
      "\n",
      "To bulk phase the data, Garrett wrote a perl script that takes the haplotypes for a family and converts all the a's to b's and vice versa. The translated file is then manually copied and pasted to the end of the original file, resulting in a file that is twice as long, with reversed genotypes comprising the addition. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#reveseMSTphase.pl\n",
      "\n",
      "#!/usr/bin/perl -w\n",
      "use strict;\n",
      "\n",
      "while(my$line=<>){\n",
      "\tchomp $line;\n",
      "\tif(($line=~\"x1\")||($line=~\"x2\")){\n",
      "\t\t$line=~tr/ab/ba/;\n",
      "\t}\n",
      "\tprint \"$line\\n\";\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Running MSTMap\n",
      "\n",
      "From a command line inside the directory of MSTMap (or from the directory with the files that has a copy of the program), the command should be:\n",
      "\n",
      "./MSTmap.exe infilename outputfilename\n",
      "\n",
      "Here is one that I used, where each command tests a different pvalue, and there are two families represented:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#MSTMap_may.sh\n",
      "\n",
      "./MSTMap.exe PHOOD11X_01H_mstmap_filtered_combined_5.txt PHOOD11X_01H_mstmap_filtered_combined_5_out.txt\n",
      "./MSTMap.exe PHOOD11X_01H_mstmap_filtered_combined_4.txt PHOOD11X_01H_mstmap_filtered_combined_4_out.txt\n",
      "./MSTMap.exe PHOOD11X_01H_mstmap_filtered_combined_3.txt PHOOD11X_01H_mstmap_filtered_combined_3_out.txt\n",
      "./MSTMap.exe PHOOD11X_05H_mstmap_filtered_combined_7.txt PHOOD11X_05H_mstmap_filtered_combined_7_out.txt\n",
      "./MSTMap.exe PHOOD11X_05H_mstmap_filtered_combined_6.txt PHOOD11X_05H_mstmap_filtered_combined_6_out.txt\n",
      "./MSTMap.exe PHOOD11X_05H_mstmap_filtered_combined_5.txt PHOOD11X_05H_mstmap_filtered_combined_5_out.txt\n",
      "./MSTMap.exe PHOOD11X_05H_mstmap_filtered_combined_4.txt PHOOD11X_05H_mstmap_filtered_combined_4_out.txt\n",
      "./MSTMap.exe PHOOD11X_05H_mstmap_filtered_combined_3.txt PHOOD11X_05H_mstmap_filtered_combined_3_out.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Example output\n",
      "\n",
      "This is the output for the PBIRD13X_110H family at a retained read threshold of 500 thousand and a missingness threshold of 0.25%\n",
      "\n",
      "The number of bins shows the resolution of the map, it is based on the inheritance patterns of the individuals. Loci that are in the same bin did not have any recombination between them in the individuals. The greater the number of individuals in the map, the greater the resolution. \n",
      "\n",
      ";number of linkage groups: 302\n",
      "\n",
      ";The size of the linkage groups are: \n",
      "\n",
      ";\t\t196\t203\t78\t213\t198\t26\t80\t158\t161\t181\t261\t161\t145\t252\t182\t94\t181\t248\t80\t246\t78\t161\t145\t248\t76\t82\t203\t140\t114\t87\t237\t87\t67\t106\t238\t87\t88\t196\t100\t83\t87\t261\t89\t201\t26\t163\t82\t94\t93\t161\t21\t82\t26\t1\t246\t1\t47\t42\t23\t163\t42\t198\t213\t88\t13\t82\t100\t1\t29\t1\t1\t93\t61\t114\t38\t1\t13\t47\t1\t94\t29\t1\t83\t1\t23\t89\t1\t67\t1\t1\t1\t1\t38\t26\t1\t21\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t\n",
      "\n",
      ";The number of bins in each linkage group: \n",
      "\n",
      ";\t\t195\t203\t78\t213\t198\t26\t80\t158\t161\t180\t261\t161\t145\t247\t182\t94\t180\t248\t80\t246\t78\t161\t145\t248\t76\t82\t203\t140\t114\t87\t237\t87\t61\t106\t238\t83\t84\t195\t99\t83\t84\t261\t89\t201\t26\t163\t82\t94\t93\t161\t21\t82\t26\t1\t246\t1\t47\t42\t23\t163\t42\t198\t213\t88\t13\t82\t100\t1\t29\t1\t1\t93\t61\t113\t38\t1\t13\t47\t1\t94\t29\t1\t83\t1\t23\t89\t1\t64\t1\t1\t1\t1\t38\t26\t1\t21\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Summarizing MSTMap output"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To more easily identify the results of the MSTMap program, Garrett wrote a perl script that summarizes the output by reporting on the length, the number of markers, and the number of bins per linkage group"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#MST_summary.pl\n",
      "\n",
      "#!/usr/bin/perl -w\n",
      "use strict;\n",
      "\n",
      "my%loci;\n",
      "my%LGlociNum;\n",
      "my%LGlength;\n",
      "my%LGbins;\n",
      "my$lociNum=0;\n",
      "my%binNum;\n",
      "my$currentLG;\n",
      "my$currentPosition;\n",
      "my$store_loci=0;\n",
      "while(my$line=<>){\n",
      "\tchomp $line;\n",
      "\tif($line=~/^group/){\n",
      "\t\tmy($discard, $LG)=split \" \", $line, 2;\n",
      "\t\t$currentLG=$LG;\n",
      "\t}\n",
      "\tif($line=~/\\;BEGINOFGROUP/){\n",
      "\t\t$store_loci=1;\n",
      "\t\tnext;\n",
      "\t}\n",
      "\tif($line=~/\\;ENDOFGROUP/){\n",
      "\t\t$LGlociNum{$currentLG}=$lociNum;\n",
      "\t\t$LGlength{$currentLG}=$currentPosition;\n",
      "\t\tmy$binNum=0;\n",
      "\t\tforeach my$key (keys %binNum){\n",
      "\t\t\t$binNum++;\n",
      "\t\t}\n",
      "\t\t$LGbins{$currentLG}=$binNum;\n",
      "\t\t%binNum=();\n",
      "\t\t$store_loci=0;\n",
      "\t\t$lociNum=0;\n",
      "\t\t$currentLG=\"\";\n",
      "\t\t$currentPosition=\"\";\n",
      "\t}\n",
      "\tif($store_loci==1){\n",
      "\t\tmy($locus, $position)=split \"\\t\", $line, 2;\n",
      "\t\t$loci{$locus}=$currentLG;\n",
      "\t\t$currentPosition=$position;\n",
      "\t\t$binNum{$position}++;\n",
      "\t\t$lociNum++;\n",
      "\t}\n",
      "}\n",
      "\n",
      "print \"LinkageGroup\\tLength(cM)\\t#Markers\\t#Bins\\n\";\n",
      "foreach my$key (sort keys %LGlength){\n",
      "\tprint \"$key\\t$LGlength{$key}\\t$LGlociNum{$key}\\t$LGbins{$key}\\n\";\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The command that I used: \n",
      "perl MST_summary.pl PHOOD11X_01H_4_split_edit2.txt >PHOOD11X_01H_4_split_edit_summary2.txt\n",
      "\n",
      "An example of the output of this program: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "LinkageGroup\tLength(cM)\t#Markers\t#Bins\n",
      "lg620\t105.722\t           237\t          51\n",
      "lg621\t135.058\t           185\t          45\n",
      "lg622\t103.572\t           187\t          38\n",
      "lg623\t95.241\t           116\t          34\n",
      "lg624\t97.654\t           271\t          56\n",
      "lg625\t100.050\t           170\t          39\n",
      "lg626\t4.302\t           41\t          5\n",
      "lg628\t59.926\t           125\t          27\n",
      "..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Identifying and removing duplicated linkage groups"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are several scripts that Garrett wrote in perl that, when used together, take the output of MSTMap and identify which linkage groups are duplicates of each other, identify which markers belong to a single set of the linkage groups (phase) and finally, split out the the linkage groups into a single set, removing the duplicated linkage groups and markers. \n",
      "\n",
      "The first of these programs is idMSTdupLGs.pl, it identifies the linkage groups that are duplicates of eachother. We expect to have duplicate linkage groups because of the bulk phasing method that we used. This perl sript takes the raw MSTMap output as the first argument, and using > you name the output. The output of the program is a tab seperated list of the paired chromosomes. \n",
      "    \n",
      "perl idMSTdupLGs.pl PHOOD11X_01H_mstmap_filtered_combined_4_out.txt >PHOOD11X_01H_4_idMSTdup.txt\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#idMSTdupLGs.pl\n",
      "\n",
      "#!/usr/bin/perl -w\n",
      "use strict;\n",
      "\n",
      "my%loci;\n",
      "my$store_loci=0;\n",
      "my$currentLG;\n",
      "while(my$line=<>){\n",
      "\tchomp $line;\n",
      "\tchomp $line;\n",
      "\tif($line=~/^group/){\n",
      "\t\tmy($discard, $LG)=split \" \", $line, 2;\n",
      "\t\t$currentLG=$LG;\n",
      "\t}\n",
      "\tif($line=~/\\;BEGINOFGROUP/){\n",
      "\t\t$store_loci=1;\n",
      "\t\tnext;\n",
      "\t}\n",
      "\tif($line=~/\\;ENDOFGROUP/){\n",
      "\t\t$store_loci=0;\n",
      "\t\t$currentLG=\"\";\n",
      "\t}\n",
      "\tif($store_loci==1){\n",
      "\t\tmy($locus, $position)=split \"\\t\", $line, 2;\n",
      "\t\t$locus=~s/R//;\n",
      "\t\tif(exists $loci{$locus}){\n",
      "\t\t\t$loci{$locus}.=\"\\t\".$currentLG;\n",
      "\t\t}else{\n",
      "\t\t\t$loci{$locus}=$currentLG;\n",
      "\t\t}\n",
      "\t}\n",
      "}\n",
      "\n",
      "my%groupings;\n",
      "foreach my$key (keys %loci){\n",
      "\t$groupings{$loci{$key}}++;\n",
      "}\n",
      "\n",
      "foreach my$key (keys %groupings){\n",
      "\tprint \"$key\\n\";\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The beginning of the output file from this program is shown below. The next step to 'unphase' the markers, getLGmarkersBatchPhase.pl, requires a single set of the chromosomes, one column of this list. I open it in excel and delete one column, saving it with another name. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lg139\tlg449\n",
      "lg47\tlg357\n",
      "lg159\tlg469\n",
      "lg69\tlg379\n",
      "lg14\tlg324\n",
      "lg87\tlg397\n",
      "lg25\tlg335\n",
      "lg221\tlg531\n",
      "lg119\tlg429\n",
      "lg305\tlg615\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "getLGmarkersBatchPhase.pl is a perl script that takes the original MSTMap output file as its first argument and the single list of linkage groups made by editing the output of the idMSTdupLGs.pl program as its second. It returns a single set of markers for that list of linkage groups, basically reducing the makers down to a single phase. \n",
      "\n",
      "perl getLGmarkersBatchPhase.pl PHOOD11X_01H_mstmap_filtered_combined_4_out.txt PHOOD11X_01H_4_idMSTdup.txt >PHOOD11X_01H_4_getLG.txt"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#getLGmarkersBatchPhase.pl\n",
      "\n",
      "#!/usr/bin/perl -w\n",
      "use strict;\n",
      "\n",
      "my$MSTfile=$ARGV[0];\n",
      "my$LGfile=$ARGV[1];\n",
      "\n",
      "my%LGs;\n",
      "open(LGFILE, \"<$LGfile\")||die \"cannot open $LGfile:$!\";\n",
      "while(my$line=<LGFILE>){\n",
      "\tchomp $line;\n",
      "\t$LGs{$line}++;\n",
      "}\n",
      "close LGFILE;\n",
      "\n",
      "open(MSTFILE, \"$MSTfile\")||die \"cannot open $MSTfile:$!\";\n",
      "my%loci;\n",
      "my$store_loci=0;\n",
      "my$currentLG;\n",
      "while(my$line=<MSTFILE>){\n",
      "\tchomp $line;\n",
      "\tchomp $line;\n",
      "\tif($line=~/^group/){\n",
      "\t\tmy($discard, $LG)=split \" \", $line, 2;\n",
      "\t\t$currentLG=$LG;\n",
      "\t}\n",
      "\tif($line=~/\\;BEGINOFGROUP/){\n",
      "\t\t$store_loci=1;\n",
      "\t\tnext;\n",
      "\t}\n",
      "\tif($line=~/\\;ENDOFGROUP/){\n",
      "\t\t$store_loci=0;\n",
      "\t\t$currentLG=\"\";\n",
      "\t}\n",
      "\tif($store_loci==1){\n",
      "\t\tmy($locus, $position)=split \"\\t\", $line, 2;\n",
      "\t\t#$locus=~s/R//;\n",
      "\t\tif(exists $LGs{$currentLG}){\n",
      "\t\t\tprint \"$locus\\n\";\n",
      "\t\t}\n",
      "\t}\n",
      "}\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The final perl script in this series is called splitMSTbulkPhase.pl. It takes the output from MSTMap as the first argument and for the second argument, the output of the above program, the list of markers at one set of chromosomes. It returns an output similar to the MSTMap original output, but for one set of linkage groups, one phase of the markers. \n",
      "\n",
      "perl splitMSTbulkPhase.pl PHOOD11X_01H_mstmap_filtered_combined_3_5_out.txt PHOOD11X_01H_3_5_markers.txt > PHOOD11X_01H_3_5_spl\n",
      "it.txt"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#splitMSTbulkPhase.pl\n",
      "\n",
      "#!/usr/bin/perl -w\n",
      "use strict;\n",
      "\n",
      "my$MSTfile=$ARGV[0];\n",
      "my$markerList=$ARGV[1];\n",
      "\n",
      "my%printMarkers;\n",
      "open(MARKERLIST, \"<$markerList\")||die \"cannot open $markerList:$!\";\n",
      "while(my$line=<MARKERLIST>){\n",
      "\tchomp $line;\n",
      "\t$printMarkers{$line}++;\n",
      "}\n",
      "close MARKERLIST;\n",
      "\n",
      "open(MSTFILE, \"<$MSTfile\")||die \"cannot open $MSTfile:$!\";\n",
      "my$lineCount=0;\n",
      "while(my$line=<MSTFILE>){\n",
      "\tchomp $line;\n",
      "\t$lineCount++;\n",
      "\tmy($locus,$rest)=split \"\\t\", $line, 2;\n",
      "\tif($lineCount<=14){\n",
      "\t\tprint \"$line\\n\";\n",
      "\t}elsif(exists $printMarkers{$locus}){\n",
      "\t\tprint \"$line\\n\";\n",
      "\t}\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#####Formatting the output to match the MSTMap output format\n",
      "\n",
      "The output for this program, for the most part, looks like the output from MSTMap, except for some formatting differences. The original output has the linkage groups split up with a header and footer and some blank space between the groups, like this: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "13948_x1\t105.722\n",
      "63466_x1\t105.722\n",
      "6722_x1\t105.722\n",
      "27026_x1\t105.722\n",
      ";ENDOFGROUP\n",
      "\n",
      ";lowerbound:110.422 upperbound: 124.525 cost after initialization:125.046\n",
      "group odd_lg621\n",
      ";BEGINOFGROUP\n",
      "23634_x1\t0.000\n",
      "23634_x2\t13.779\n",
      "29512_x1\t24.140\n",
      "60673_x1\t26.292\n",
      "30026_x2\t27.368"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The output from the splitMSTbulkPhase.pl script doesnt retain those headers. They are not necessary, but for the perl script MST_summary.pl to work, those headers need to be in place. In the output that I wanted to look into more in depth, I went by hand, comparing the original output from MSTMap to the output here, and added the headers from the original MSTMap output in manually."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Comparing Odd and Even year linkage groups"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tyler has gone through the same process with a family of even year lineage pink salmon. To test how similar our maps are, we want to compare the results from MSTMap, specifically the markers in each linkage group to find which linkage groups are comparable between the two families. \n",
      "\n",
      "To do this, we will take our even and odd year linkage map results that have been split into one set of linkage groups, and make some formatting changes:\n",
      "\n",
      "- Add the headers to the linkage groups, as detailed above.\n",
      "- Use find/replace to remove the R from the loci that have had the phase reversed.\n",
      "- Change the names of the linkage groups so that we can distinguish from which map they come. For instance, change the names from lg456 to odd_lg456. \n",
      "\n",
      "Once formatted, combine the two sets of linkage groups into one file and run the program idMSTdupLGs.pl on the combined file in the same way as before to identify which linkage groups are duplicates. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Filtering the MSTmap ouput\n",
      "Following this header summary are the linkage groups with the loci listed in order on each. Garrett wrote a perl script that takes this output and formats to make it easier to work with in excel and R. It collapses the output into a format that is the linkage group positions and the locus. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#MST_format.pl\n",
      "\n",
      "#!/usr/bin/perl -w\n",
      "use strict;\n",
      "\n",
      "my$MSTfile=$ARGV[0];\n",
      "#my$CRIfile=$ARGV[1];\n",
      "\n",
      "my%loci;\n",
      "my$currentLG;\n",
      "my$store_loci=0;\n",
      "open(MST, \"<$MSTfile\")||die \"cannot open $MSTfile:$!\";\n",
      "while(my $line=<MST>){\n",
      "\tchomp $line;\n",
      "\tif($line=~/^group/){\n",
      "\t\tmy($discard, $LG)=split \" \", $line, 2;\n",
      "\t\t$currentLG=$LG;\n",
      "\t}\n",
      "\tif($line=~/\\;BEGINOFGROUP/){\n",
      "\t\t$store_loci=1;\n",
      "\t\tnext;\n",
      "\t}\n",
      "\tif($line=~/\\;ENDOFGROUP/){\n",
      "\t\t$store_loci=0;\n",
      "\t\t$currentLG=\"\";\n",
      "\t}\n",
      "\tif($store_loci==1){\n",
      "\t\tmy($locus, $position)=split \"\\t\", $line, 2;\n",
      "\t\t$loci{$locus}=$currentLG;\n",
      "\t\tprint \"$locus\\t$position\\t$currentLG\\n\";\n",
      "\t}\n",
      "}\n",
      "close MST;\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A note on plotting the maps: \n",
      "    Garrett uses the lattice package in R \n",
      "\n",
      "A note for naming consistency: \n",
      "    The linkage groups will be named arbitrarily each time they are run, and they will be different between families and projects. For this reason, re-name your linkage groups to the chromosome names as soon as possible. \n",
      "\n",
      "A note for using Lep-Map: \n",
      "    Lep-Map changes your loci names. To get over this, copy the names to excel and do conversions. Each family will have a differnt set of loci after being run in Lep-map so keep each conversion key with the family for which it was used, to prevent confusion. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}