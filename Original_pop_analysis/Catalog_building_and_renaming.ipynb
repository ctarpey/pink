{
 "metadata": {
  "name": "",
  "signature": "sha256:087f9cf05cd1bcc0c1bd5b602bb8e290339409a01a65c540805e848a3fd72ea7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Stacks Workflow for Pink Data: Building a joint catalog with cstacks, and renaming the loci to match published names"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Building a joint catalog with cstacks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Catalog background:\n",
      "The stacks catalog is a set of consensus loci against which the loci of each individual will be compared. It can be made with any set of samples, and requires that the samples have been processed by the ustacks progam. We wanted to build a catalog for this project that would both incorporate past data sets and be able to be used future data sets as a way to bridge projects. Any samples that are genotyped using this catalog will have the same loci names, which is so beneficial in cross project comparisons.\n",
      "\n",
      "Our thought process for building the catalog: In terms of the population data, the catalog should incorporate the variation found in the earlier population samples from North America, as well the newer Asian population samples. It should be representative of the variation found throughout the range of the fish from the most southern Asian populations, through those from the Bering Sea and around to the southern North American populations. In addition, it should represent the variation found in both even- and odd-year lineages.\n",
      "\n",
      "The catalog will also be used to genotype the haploid samples used in the linkage map and should therefore contain all the variation found in those families. For this reason, it should contain the female parents of the offspring of all the families used in linkage mapping.\n",
      "To include all these sources of variation in the catalog (called the joint catalog in some places because it is used for genotyping/haplotyping both the population samples and the linkage mapping samples) we decided that four individuals from each lineage of each population (7 populations total, so 56 individuals) should be sufficient to represent the local variation of that group. In addition, the female parents of each of the linkage map families will be included in the catalog to represent the variation present in each of the families (2 Haploid 2013 families, 2 Haploid 2011 families and one 2013 Gynogenetic Diploid family means 5 female parents). I have 5 families, but tyler has 2 more. They are 2014 Bird Creek Alaska pink salmon and this catalog will also be used to run stacks on his samples. So his two dams will be added to this catalog.\n",
      "\n",
      "The catalog will thus have 63 individuals (56 population individuals and 7 linkage map individuals). They will be chosen based on the number of retained reads after initial and possible resequencing, as the number of retained reads is likely a decent proxy for the quality and depth of coverage of the sequencing."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#The cstacks command\n",
      "\n",
      "There are not a lot of components in the cstacks command, and most are straightforward: \n",
      "\n",
      "cstacks -b batch_id -s sample_file [-s sample_file_2 ...] [-o path] [-n num]  \n",
      "\n",
      "    p \u2014 enable parallel execution with num_threads threads.\n",
      "    b \u2014 MySQL ID of this batch.\n",
      "    s \u2014 TSV file from which to load radtags.\n",
      "    o \u2014 output path to write results.\n",
      "    m \u2014 include tags in the catalog that match to more than one entry.\n",
      "    n \u2014 number of mismatches allowed between sample tags when generating the catalog.\n",
      "\n",
      "The two parameters that we considered in particular while building this catalog were the -n (a flag here in cstacks) and the -M flag in ustacks. \n",
      "\n",
      "In ustacks, the number of differences allowed between sequences to make them a stack (the minimum distance in nucleotides allowed between stacks) is -M. When we ran ustacks, we ran it with -M of 2 (the default). This is related to the -n here, which is the number of mismatches allowed between sample tags when generating the catalog.\n",
      "\n",
      "Garret says it is a little strange to have the -n here in cstacks be larger than the -M used in ustacks; ideally, they should be either the same or the -n should be smaller than the -M. This is really just speculation though, since there is no indication that when you are working with populations that have a really large geographical spread (where you might be expecting a fair number of genetic differences between the populations) the number of mismatches allowed between the sample tags when generating the catalog should not be bigger than the -M, the minimum distance in nucleotides allowed between stacks. \n",
      "\n",
      "I decided to use the same -n (2) as the -M that was used in ustacks (the default, 2). \n",
      " "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#cstacks\n",
      "\n",
      "#cstacks_april2015.sh\n",
      "#April 2015\n",
      "\n",
      "#This shellscript is used to make the joint catalog from the samples that were run through ustacks\n",
      "#using the bounded model and the removal algorithm\n",
      "#it has an n of 2 (the same as what was used in ustacks) and a batch number 2\n",
      "\n",
      "cstacks -b 2 -s all/PAMUR11_0023_comb -s all/PAMUR11_0029_comb -s all/PAMUR11_0031_comb -s all/PAMUR11_0033_comb -s all/PHAYLY09_0014_comb -s all/PHAYLY09_0030_comb -s all/PHAYLY09_0038_comb -s all/PHAYLY09_0042_comb -s all/PHAYLY10_0011_comb -s all/PHAYLY10_0012_comb -s all/PHAYLY10_0023_comb -s all/PHAYLY10_0046_comb -s all/PKOPE91T_0009 -s all/PKOPE91T_0013 -s all/PKOPE91T_0014 -s all/PKOPE91T_0020 -s all/PKOPE96T_0001 -s all/PKOPE96T_0009 -s all/PKOPE96T_0017 -s all/PKOPE96T_0018 -s all/PKUSHI06_0006_comb -s all/PKUSHI06_0009_comb -s all/PKUSHI06_0030_comb -s all/PKUSHI07_0029_comb -s all/PKUSHI07_0031_comb -s all/PSNOH03_0017 -s all/PSNOH03_0024 -s all/PSNOH03_0043 -s all/PSNOH03_0082 -s all/PSNOH96_0011 -s all/PSNOH96_0012 -s all/PSNOH96_0013 -s all/PSNOH96_0014 -s all/PNOME91_0001 -s all/PNOME91_0003 -s all/PNOME91_0016 -s all/PNOME91_0018 -s all/PNOME94_0007 -s all/PNOME94_0013 -s all/PNOME94_0014 -s all/PNOME94_0019 -s all/PTAUY09_0042_comb -s all/PTAUY12_0041_comb -s all/PAMUR10_0014 -s all/PAMUR10_0020 -s all/PAMUR10_0021 -s all/PAMUR10_0028 -s all/PBIRD13X_0102_comb -s all/PBIRD13X_0108_comb -s all/PBIRD13X_0110_comb -s all/PHOOD11X_0001 -s all/PHOOD11X_0005 -s all/PKUSHI06_0023 -s all/PKUSHI07_0015 -s all/PKUSHI07_0022 -s all/PTAUY09_0002 -s all/PTAUY09_0023 -s all/PTAUY09_0038 -s all/PTAUY12_0019 -s all/PTAUY12_0027 -s all/PTAUY12_0032 -s all/PBIRD14X_0101 -s all/PBIRD14X_0107 -o all -n 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There was one problem with the above script. The catalog was made, no interuptions, but the file name for one of the samples was incorrect PTAUY12_0032 is actually a _comb so that sample was not included in the catalog.  I had to add that sample to the catalog separately \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Adding Sample PTAUY12_0032_comb to the catalog"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "from the stacks manual, this is how you add a sample to the catalog: \n",
      "Catalog editing:\n",
      "\n",
      "    --catalog [path] \u2014 provide the path to an existing catalog. cstacks will add data to this existing catalog.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "  cstacks -b 3 -s editedcatalog/PTAUY12_0032_comb --catalog editedcatalog/batch_2.catalog -o editedcatalog -n 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Renaming the catalog\n",
      "\n",
      "Stacks names the loci in the catalog randomly each time it builds a catalog, regardless of whether the command or the input files have changed. We wanted the loci called in this project and future pink salmon studies that rely on this joint catalog to be easily comparable to the pink salmon studies that have preceded it. For that reason we renamed the loci in the catalog so that any identifal loci between this and earlier studies had the same name, the one assigned in the previous study. This process did not rebuild the catalog, instead, it simply changed the names of the loci in the three output files produced by cstacks. \n",
      "\n",
      "The steps and a more in-depth reasoning for this process are described in a separate ipython notebook entitled: \n",
      "\n",
      "Renaming_the_catalog.ipynb \n",
      "\n",
      "The analysis that follows uses those stacks catalog files that have the early/original loci names. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Background: \n",
      "The first pink paper used population data that was analyzed with the Miller pipeline, and the second pink paper used those data (and haploid data to make a linkage map) and reanalyzed them in the program Stacks. The Miller pipeline and Stacks use different naming schemes. We want to make sure that the outliers that were identified in the first paper are represented here in this paper with the same names (those given by the Miller pipeline). To make sure that future data sets dont have to go through this blast alignment and renaming process, we will rename our Stacks catalog (constructed with 63 individuals, four from each lineage of each population, and with each female parent of the 7 haploid/gynodip mapping families) with the names given by the Miller pipeline so that the loci aligned against this catalog will all be named the same, regardless of project, in perpetuity. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#The plan: \n",
      "    1. identify and organize loci (and their sequences) named with the Miller pipeline (from the supplementary table in Limborg)\n",
      "    2. blast align the Miller loci sequences against the catalog tags to find matches \n",
      "    3. filter the blast results to find the best alignments\n",
      "    4. identify one to one matches in non-outlier loci and the best alignment for all outlier loci\n",
      "    5. create a MasterKey (list of all current catalog names and matching desired names)\n",
      "    6. rename all three catalog files using the MasterKey\n",
      "    7. reorder the catalog files numerically by loci number"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Step 1: Make a useful list of the loci identified with the Miller pipeline \n",
      "\n",
      "The supplementary tables from the Limborg 2014 paper have the loci with their sequences in an excel table. The loci names and sequences need to be in FASTA format to be blasted against the current catalog. The following script in python takes an excel file and converts the contents into FASTA format. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####python for converting table s2 to FASTA format\n",
      "## feb 18 2015 \n",
      "# carolyn tarpey (&garrett)\n",
      "\n",
      "## this needs two arguments:\n",
      "#the first is the name of the excel file that needs converting \n",
      "#the second is the name of the output file you want (the FASTA file)\n",
      "\n",
      "import sys\n",
      "import re\n",
      "\n",
      "#open file\n",
      "excel_file = open(sys.argv[1], \"r\")\n",
      "FASTA = open(sys.argv[2],\"w\") \n",
      "\n",
      "for line in excel_file:#read one line  of the excel file at a time and \n",
      "\tcolumns = line.split(\"\\t\")#take that line and split it up by the tabs\n",
      "\t#print columns \n",
      "\tnewline =[ \">\", \"\\t\", columns[1], \"\\t\", columns[2], \"\\t\", columns[3], \"\\n\" ] #> the second column tab third column tab fourth column end line\n",
      "\t#print newline\n",
      "\tFASTA.write(''.join(newline)) # write this to the output file: > the second column tab third column tab fourth column end line\n",
      "\tseq = columns[4]\n",
      "\tpat1 = r'(\\[)'\n",
      "\tpat2 = r'(/\\w])'\n",
      "\tseq_new = \"\"\n",
      "\tx = re.sub(pat1, seq_new, seq)\n",
      "\t#print x\n",
      "\ty = re.sub(pat2, seq_new, x)\n",
      "\t#print y\n",
      "\t\n",
      "\tFASTA.write(''.join(y)) # write this to the output file: the edited sequence and the end of line \n",
      "\tFASTA.write(\"\\n\") #skip a line in the output\n",
      "\n",
      "excel_file.close()\n",
      "FASTA.close()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Step 1.5: Convert the catalog tags file to FASTA format\n",
      "The .tags.tsv file from the catalog also needs to be converted to a FASTA format to make a blast database with it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#convert_tags_to_FASTA.py\n",
      "####python plan for catalog.tags.tsv to FASTA format\n",
      "## april 2015\n",
      "# carolyn tarpey (&garrett)\n",
      "\n",
      "##this needs two arguments:\n",
      "#the first is the name of the tags file that needs converting \n",
      "#the second is the name of the output file you want (the FASTA file)\n",
      "\n",
      "import sys\n",
      "import re\n",
      "\n",
      "#open file\n",
      "tag_file = open(sys.argv[1], \"r\")\n",
      "FASTA = open(sys.argv[2],\"w\") \n",
      "\n",
      "for line in tag_file:                   #read one line  of the excel file at a time and \n",
      "\tcolumns = line.split(\"\\t\")          #take that line and split it up by the tabs\n",
      "\tnewline =[ \">\", columns[2], \"\\n\" ]  #>the locus name\n",
      "\t#print newline\n",
      "\tFASTA.write(''.join(newline))       #write this to the output file: > the second column tab third column tab fourth column end line\n",
      "\tseq = columns[9]\n",
      "\t#print seq\n",
      "\t\n",
      "\tFASTA.write(''.join(seq))           #write this to the output file: the sequence\n",
      "\tFASTA.write(\"\\n\")                   #skip a line in the output\n",
      "\n",
      "tag_file.close()\n",
      "FASTA.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Step 2: Blast the Miller pipeline loci against the database of catalog loci to find best alignments\n",
      "\n",
      "To run a blast search you need two things, a database and the query. The database are the catalog tags here because there are 666,000+ of them and the query are the sequences that we are looking for matches for in the database. The command to make the atabase out of the catalog tags is below, where the catalog sequences are in batch_3_FASTA.txt (created in the previous step)\n",
      "\n",
      "makeblastdb -in batch_3_FASTA.txt -dbtype nucl -out newcatalog_blastdb\n",
      " \n",
      "The query is just the FASTA file that we made out of the sequences. To blast them against the catalog, you specify the catalog and the query. In the following blast command, the Miller pipeline loci are FASTA_table_S2.txt. The first couple of times it crashed while running because we didnt have the max target seq set to return only the 10 best matches \n",
      "\n",
      "blastn -query FASTA_table_S2.txt -db newcatalog_blastdb -evalue 10 -outfmt 6 -max_target_seqs 10 -out outliers_newcatalog_blastout\n",
      "\n",
      "#Step 3: Filter the results of the blast to find the most useful and best alignments.\n",
      "\n",
      "The following python script filters the output of the blast alignment for the alignment length, alignment start place, and gaps. The old sequences are all 88bp in length, so we filtered out anything that is smaller than 87, as well as any alignment that doesn't start at the first spot. Since the sequences start with the radtags, the alignments should all begin at the first spot. There should also be no gaps in the alignment.\n",
      "   "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#blast_filter_april.py \n",
      "####python filtering the output of the BLAST search aligning the old population loci with the new catalog loci\n",
      "## april 2015 (edited from feb)\n",
      "# carolyn tarpey (&garrett)\n",
      "\n",
      "## this needs two arguments:\n",
      "#the first is the name of the blast output file that needs filtering\n",
      "#the second is the name of the filtered output file \n",
      "#This is the command I used: blast_filter_april.py outliers_newcatalog_blastout filtered_blast_results.txt\n",
      "\n",
      "import sys\n",
      "import re\n",
      "\n",
      "#open file\n",
      "in_file = open(sys.argv[1], \"r\")\n",
      "filtered = open(sys.argv[2],\"w\") \n",
      "\n",
      "#write a title line for the output to name the columns\n",
      "filtered.write('Query\tDatabase_seq\t %_ID\tAlign_length\tMismatches\tGaps\tQ_start\tQ_end\tDb_start\tDb_end\tEval\tBit \\n')\n",
      "\n",
      "for line in in_file:\t\t\t\t\t\t\t#read one line  of the file at a time and \n",
      "\tcolumns = line.split(\"\\t\")\t\t\t\t\t#take that line and split it up by the tabs\n",
      "\tlength = (\"\".join(columns[3])) \t\t\t\t#make the alignment length a string\n",
      "\tintlength = float(length) \t\t\t\t\t#make the alignment length string an integer\n",
      "\tgaps = (\"\".join(columns[5])) \t\t\t\t#make the gaps a string\n",
      "\tintgaps = float(gaps) \t\t\t\t\t    #make the gaps string an integer\n",
      "\tq_start = (\"\".join(columns[6]))\n",
      "\tq_start_len = float(q_start) \n",
      "\n",
      "\tif (intlength >= 87) and (q_start_len == 1) and (intgaps == 0):\t# if the alignment length is greater or equal to 87 and the q_start is one and the gaps is zero\n",
      "\t\tfiltered.write(line)\t\t\t\t\t# write this line to the output file\n",
      "\telse:\n",
      "\t\tcontinue\n",
      " \n",
      "in_file.close()\n",
      "filtered.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Step 4a: Identify which old loci match to new ones and vice versa, using the filtered alignments\n",
      "\n",
      "This is a script that takes the results of the filtering step above and sorts them into matching pairs and returns two files. One that show all the Catalog IDs that match to each Miller pipeline locus (new keys) and another that show the complement, all the Miller pipeline loci that mach to each Catalog ID locus (old keys). It really has not been optimized for formatting, the two output files have to be manually editited. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#compare_matches2.py\n",
      "####python pull out the old loci that match to more than one new loci and vice versa\n",
      "## april 2105 (edited from feb)\n",
      "# carolyn tarpey (&garrett)\n",
      "\n",
      "## this needs three arguments:\n",
      "#the first is the name of the filtered blast output file that needs parsing\n",
      "#the second is the output name of the file that will have the old loci as the key\n",
      "#the third is the output name of the file that will have the new loci as the key\n",
      "#it has been edited to cut out the first line of the filtered file, so that query isnt in the mix\n",
      "\n",
      "#used this command: \n",
      "#compare_matches2.py filtered_blast_results.txt old_key_matches new_key_matches\n",
      "\n",
      "import sys\n",
      "import re\n",
      "from collections import defaultdict\n",
      "\n",
      "#open file\n",
      "in_file_filtered = open(sys.argv[1], \"r\")\n",
      "old_key_matches = open(sys.argv[2],\"w\") \n",
      "new_key_matches = open(sys.argv[3],\"w\") \n",
      "\n",
      "#write a title line for the output to name the columns\n",
      "new_key_matches.write('Query\tDatabase_seq \\n')\n",
      "\n",
      "#write a title line for the output to name the columns\n",
      "old_key_matches.write('Query\tDatabase_seq \\n')\n",
      "\n",
      "old_key_dict_list = []\t\t\t\t\t\t\t# a list for the pair of loci names, old then new\n",
      "\n",
      "with in_file_filtered as f:\n",
      "    next(f)\n",
      "    for line in f:     #read one line  of the file at a time and \t\t\t\t\t\n",
      "\t\tcolumns = line.strip().split(\"\\t\")\t\t\t\t\t#take that line and split it up by the tabs, take off the new line \n",
      "\t\told_key_dict_list.append((columns[0], columns[1]))\t #add the two loci names to the end of the list\n",
      "\t\t#print old_key_dict_list\t\n",
      "\n",
      "d_old = defaultdict(list)\t\t\t\t\t\t#make the default dict\n",
      "\n",
      "for oldloci, newloci in old_key_dict_list:\t\t#every time it takes something in the list, look at the old loci and either\n",
      "\td_old[oldloci].append(newloci)\t\t\t\t# add it to the dictionary, or find the entry and append more to the end\n",
      "\n",
      "for oldloci, newloci in d_old.iteritems():\n",
      "\t#print oldloci,\t\", \".join(newloci), \"\\n\"\n",
      "\tf = oldloci, \", \".join(newloci)\n",
      "\tg = str(f)\n",
      "\told_key_matches.write(g)\n",
      "\told_key_matches.write('\\n')\n",
      "\n",
      "new_key_dict_list = []\t\t\t\t\t\t\t# a list for the pair of loci names, new then old\n",
      "\n",
      "in_file_filtered.close()\n",
      "in_file_filtered = open(sys.argv[1], \"r\")\n",
      "\n",
      "with in_file_filtered as f:\n",
      "    next(f)\n",
      "    for line in f:     #read one line  of the file at a time and \n",
      "\t\tcolumns2 = line.strip().split(\"\\t\")\t\t\t\t\t#take that line and split it up by the tabs, take off the new line \n",
      "\t\tnew_key_dict_list.append((columns2[1], columns2[0]))\t #add the two loci names to the end of the list\n",
      "\t\t#print new_key_dict_list\n",
      "\n",
      "d_new = defaultdict(list)\n",
      "\n",
      "for newloci2, oldloci2 in new_key_dict_list:\n",
      "    d_new[newloci2].append(oldloci2)\n",
      "\t#print d_new\n",
      "\n",
      "for newloci2, oldloci2 in d_new.iteritems():\n",
      "\t#print newloci,\t\", \".join(oldloci), \"\\n\"\n",
      "\th = newloci2, \", \".join(oldloci2)\n",
      "\ti = str(h)\n",
      "\tnew_key_matches.write(i)\n",
      "\tnew_key_matches.write('\\n') \n",
      "\n",
      "in_file_filtered.close()\n",
      "old_key_matches.close()\n",
      "new_key_matches.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above script takes the filtered output from the blast results and parses it into pairs but it produces an output that is not pretty, it has parenthesis and quotes that get in the way. \n",
      "THIS IS NOT THE FINAL OUTPUT, IT IS AN EARLIER RUN. LOCI NAMES WILL BE DIFFERENT IN CATALOG Instead of trying to fix the code, which would hopefully only be used once, I just opened the output files in notepad ++ and did a find replace to delete the (',')"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "old_key_matches: \n",
      "    \n",
      "Query\tDatabase_seq \n",
      "('OgoRAD_28015', '46789, 323195')\n",
      "('OgoRAD_54370', '62448')\n",
      "('OgoRAD_28018', '36676')\n",
      "('OgoRAD_18289', '38997')\n",
      "('OgoRAD_23429', '60265')\n",
      "('OgoRAD_47629', '1837')\n",
      "('OgoRAD_23422', '81623, 94189')\n",
      "('OgoRAD_9179', '20189')\n",
      "('OgoRAD_62711', '17933')\n",
      "('OgoRAD_6848', '52851, 84959, 51799, 43302, 58885, 27647, 2944')\n",
      "\n",
      "\n",
      "and new_key_matches: \n",
      "\n",
      "Query\tDatabase_seq \n",
      "('35236', 'OgoRAD_49962')\n",
      "('35547', 'OgoRAD_29881')\n",
      "('73393', 'OgoRAD_69053')\n",
      "\n",
      "('55298', 'OgoRAD_6015')\n",
      "('19394', 'OgoRAD_29829')\n",
      "('78424', 'OgoRAD_12954')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Step 4b: Verify the outliers have matching alignments in the new catalog and (4c) identify the best alignment for old loci that matched more than one catalog loci\n",
      "\n",
      "To make sure that the outliers were all included in the catalog, I went back to the table in Limborg's supplementary materials, where he has a sheet of all the outlier loci from both papers. I did a vlookup in excel with the outliers and their sequences and the matches from the blast results and found that 4 did not match any sequences in the new catalog. \n",
      "\n",
      "- OgoRAD_32638\n",
      "- OgoRAD_23758\n",
      "- OgoRAD_57744\n",
      "- OgoRAD_70682\n",
      "\n",
      "There are a couple of reasons this may be. Either they are legitimately not found in the catalog, or they have low complexity sequences and thus were not aligned as the blast filters ignored them, or they were eliminated from the filtered results because the alignments did not meet the filtering requirements. \n",
      "\n",
      "As a test, I made the outlier sequences into a database and blasted the missing outlier sequences against it, in the process they needed to be converted from the excel format to FASTA format. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#convert_outliers_to_FASTA.py\n",
      "\n",
      "####python plan for converting outliers to FASTA format\n",
      "## april 2015 (edited from feb)\n",
      "# carolyn tarpey (&garrett)\n",
      "\n",
      "## this needs two arguments:\n",
      "#the first is the name of the excel file with the outliers that needs converting \n",
      "#the second is the name of the output file you want (the FASTA file)\n",
      "\n",
      "import sys\n",
      "import re\n",
      "\n",
      "#open file\n",
      "excel_file = open(sys.argv[1], \"r\")\n",
      "FASTA = open(sys.argv[2],\"w\") \n",
      "\n",
      "for line in excel_file:#read one line  of the excel file at a time and \n",
      "\tcolumns = line.split(\"\\t\")#take that line and split it up by the tabs\n",
      "\tnewline =[ \">\", columns[0], \"\\n\" ] \n",
      "\tFASTA.write(''.join(newline)) # write this to the output file: > the second column tab third column tab fourth column end line\n",
      "\tseq = columns[1]\n",
      "\tpat1 = r'(\\[)'\n",
      "\tpat2 = r'(/\\w])'\n",
      "\tseq_new = \"\"\n",
      "\tx = re.sub(pat1, seq_new, seq)\n",
      "\ty = re.sub(pat2, seq_new, x)\n",
      "\t\n",
      "\tFASTA.write(''.join(y)) # write this to the output file: the edited sequence and the end of line \n",
      "\tFASTA.write(\"\\n\") #skip a line in the output\n",
      "\n",
      "excel_file.close()\n",
      "FASTA.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Make a database out of the outlier loci sequences so they may be blasted back against themselves: \n",
      "\n",
      "- makeblastdb -in outlier_seq_FASTA.txt -dbtype nucl -out outlier_seq_blastdb\n",
      " \n",
      "And used this command to actually blast the outlier sequences against themselves (their sequences are also the database): \n",
      " \n",
      "- blastn -query outlier_seq_FASTA.txt -db outlier_seq_blastdb -evalue 10 -outfmt 6 -max_target_seqs 10 -out outlier_seq_blastout\n",
      " \n",
      "So that blasts the outlier sequences against themselves, looking at four outliers specifically: \n",
      "- OgoRAD_32638\n",
      "- OgoRAD_23758\n",
      "- OgoRAD_57744\n",
      "- OgoRAD_70682\n",
      "\n",
      "two did match to their own sequences:\n",
      "- OgoRAD_57744\n",
      "- OgoRAD_70682\n",
      "\n",
      "but two didnt:\n",
      "- OgoRAD_32638\n",
      "- OgoRAD_23758\n",
      "\n",
      "I used 'find/replace' to look up the two outliers that did match themselves (ie they did not low complexity sequences, there is some other reason they have been left out) in the outliers_newcatalog_blastout results, and found that they were likely not included in the filtered results because: \n",
      "\n",
      "This outlier seems to have been filtered out because it has an alignment length of 85.\n",
      "\n",
      "- OgoRAD_57744\t822 \t100.00\t85\t0\t0\t1\t85\t7\t91\t8e-039\t  158\n",
      "\n",
      "and for this outlier there was a gap at the end of the alignment: \n",
      "\n",
      "- OgoRAD_70682\t648212\t98.86\t88\t0\t1\t1\t88\t7\t93\t3e-038\t  156\n",
      "\n",
      "- OgoRAD_70682\tTCTTTCTGCCAGTATGGCATGGATGTTGGAGACCACAGTGTCTGCAGTCAGAGCAGCTAGACTCCTCAGATCCAGCTCCTCTGTTCCA\t\n",
      "\n",
      "-       648212  TCTTTCTGCCAGTATGGCATGGATGTTGGAGACCACAGTGTCTGCAGTCAGAGCAGCTAGACTCCTCAGATCCGCTCCTCTGTTCCAC\n",
      "\n",
      "The variable site is the second site in this sequence and the gap is way down near the end of the sequence; the fact that the SNP is the same in both sequences in the alignment and the gap is so far away from the SNP, all the way at the end, means this is probably a pretty good alignment.\n",
      "\n",
      "To check if the two outlier loci that didn't match to themselves were ignored by blast because of low complexity, I blasted them against themselves with the low complexity filter turned off in the blast command and they did match themselves. I then blasted them against the catalog database with this command: \n",
      "\n",
      "- blastn -query four_outliers_FASTA.txt -db newcatalog_blastdb -evalue 10 -outfmt 6 -max_target_seqs 10 -dust no -out four_outliers_nodust\n",
      "\n",
      "made this file with the four outlier sequences: \n",
      "four_outliers_FASTA.txt\n",
      ">OgoRAD_70682\n",
      "TCTTTCTGCCAGTATGGCATGGATGTTGGAGACCACAGTGTCTGCAGTCAGAGCAGCTAGACTCCTCAGATCCAGCTCCTCTGTTCCA\n",
      "\n",
      ">OgoRAD_57744\n",
      "AAACTCCCCAGCACCTCTACAGCACGTTCTGCCAGGGGAGCCTCCCTGGTACACACCATGCCCTACAGACACAGAGAGGCAGGGGGTC\n",
      "\n",
      ">OgoRAD_32638\n",
      "AGATCAGAGTAAGAACTTTAACTGTTATGTCATGTTATAACTGTTATAACTGTTATGTGATGTTATAACTGTTATGTCATGTTATAAC\n",
      "\n",
      ">OgoRAD_23758\n",
      "GATGGGGGCCTGAGATGAACATTTTTAAAAACACAATATAAATATAGAACAAAACACAATACACAACAAGAGAGACAACATAAAGAGA\n",
      "\n",
      "Results :\n",
      "four_outliers_nodust.txt\n",
      "\n",
      "- OgoRAD_70682\t31753\t100.00\t86\t0\t0\t3\t88\t9\t94\t2e-039\t  159\n",
      "- OgoRAD_70682\t648212\t98.86\t88\t0\t1\t1\t88\t7\t93\t3e-038\t  156\n",
      "- OgoRAD_70682\t446026\t100.00\t41\t0\t0\t1\t41\t54\t94\t2e-014\t76.8\n",
      "- OgoRAD_70682\t360442\t100.00\t41\t0\t0\t3\t43\t54\t94\t2e-014\t76.8\n",
      "\n",
      "\n",
      "- OgoRAD_57744\t822\t    100.00\t85\t0\t0\t1\t85\t7\t91\t8e-039\t  158\n",
      "- OgoRAD_57744\t101420\t98.70\t77\t0\t1\t1\t77\t7\t82\t4e-032\t  135\n",
      "- OgoRAD_57744\t554124\t100.00\t55\t0\t0\t1\t55\t40\t94\t4e-022\t  102\n",
      "- OgoRAD_57744\t87692\t100.00\t55\t0\t0\t1\t55\t7\t61\t4e-022\t  102\n",
      "- OgoRAD_57744\t282815\t100.00\t49\t0\t0\t1\t49\t46\t94\t8e-019\t91.6\n",
      "\n",
      "\n",
      "- OgoRAD_32638\t28684\t100.00\t88\t0\t0\t1\t88\t7\t94\t2e-040\t  163\n",
      "- OgoRAD_32638\t294400\t88.64\t88\t0\t1\t1\t78\t7\t94\t5e-021\t99.0\n",
      "- OgoRAD_32638\t294400\t97.62\t42\t1\t0\t47\t88\t25\t66\t3e-013\t73.1\n",
      "- OgoRAD_32638\t294400\t100.00\t29\t0\t0\t19\t47\t44\t72\t1e-007\t54.7\n",
      "- OgoRAD_32638\t577376\t100.00\t47\t0\t0\t1\t47\t48\t94\t1e-017\t87.9\n",
      "- OgoRAD_32638\t371562\t100.00\t45\t0\t0\t1\t45\t50\t94\t1e-016\t84.2\n",
      "- OgoRAD_32638\t427943\t100.00\t28\t0\t0\t1\t28\t67\t94\t4e-007\t52.8\n",
      "\n",
      "\n",
      "- OgoRAD_23758\t41678\t97.73\t88\t2\t0\t1\t88\t7\t94\t4e-037\t  152\n",
      "- OgoRAD_23758\t57991\t94.32\t88\t2\t1\t1\t88\t7\t91\t5e-031\t  132\n",
      "- OgoRAD_23758\t350791\t98.21\t56\t1\t0\t1\t56\t39\t94\t5e-021\t99.0\n",
      "- OgoRAD_23758\t535928\t100.00\t38\t0\t0\t1\t38\t7\t44\t1e-012\t71.3\n",
      "- OgoRAD_23758\t535928\t100.00\t38\t0\t0\t51\t88\t34\t71\t1e-012\t71.3\n",
      "- OgoRAD_23758\t210776\t100.00\t34\t0\t0\t1\t34\t61\t94\t2e-010\t63.9\n",
      "\n",
      "\n",
      "For one outlier the result was obvious, the best alignment was this one: \n",
      "\n",
      "- OgoRAD_32638\t28684\t100.00\t88\t0\t0\t1\t88\t7\t94\t2e-040\t  163\n",
      "\n",
      "For the other there were two alignments that were good but neither was great:\n",
      "\n",
      "- OgoRAD_23758\t82\t[A/C]\tGATGGGGGCCTGAGATGAACATTTTTAAAAACACAATATAAATATAGAACAAAACACAATACACAACAAGAGAGACAACATAAAGAGA\t\n",
      "- 41678 GATGGGGGCCTGAGATTAACATTTTTAAAAACACAATATAAATATAGAACAAAACACAATACACAACAAGAGAGACAACATAGAGAGA\n",
      "- 57991 GATGGGGGCCTGAGATTAACATTTTTAAAAACACAATATAAATATAGAACAAAACACAATACACAAGAGAGACAACATAGAGAGAGAC\n",
      "\n",
      "I think the sequences show enough differences between them that this outlier will be left out of the further analysis. - "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Checks with vlookup to make sure that all the outliers are present in the one to one sheet \n",
      "and to create the one to one sheet."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Step 5a: Creating the MasterKey\n",
      "\n",
      "Looking at the results of all the matches of the old loci to the new catalog in excel, we decided to keep only those that matched one to one (if they were outliers they were treated as explained above). \n",
      "\n",
      "Of the 8,036 loci listed in the Limborg table, there is a 1:1 match for 7,286 of them, including best alignments of all but 1 of the 115 outlier loci. They are saved in a file with two columns (called one2onematches) one has the name that is in the catalog (CatalogName) that we want to change and the other has the corresponding desired name (the OgoRad_ name). \n",
      "\n",
      "That file will be used to build the key using a perl script that Garrett wrote. The key is a file with two columns, the catalog names now (to be changed) and the desired names, but the desired names don't have the ogoRAD in front, they just have the number. The master key has an entry for every loci in the catalog, so this catalog has 662345 loci and 662345 entries in the MasterKey. For the catalog loci that don't have matches in the old loci, they have an entry in the other column that may not be the same because the catalog has to be sequentially numbered with no gaps. \n",
      "\n",
      "Garrett's perl script works by taking the two columns of the one to one matches txt, the one that is the desired and the to be changed (TBC) and goes through the list putting the pairs into a dictionary with the desired names as the key. As it cycles through the lines, if it has a catalog name (to be changed)that is a key in a dictionary already, it will assign that to be changed loci a new desired name that is he next number in the list, which is generated by this one counter that he has going. This makes sure that all the loci names are unique with no gaps in the number sequence. \n",
      "\n",
      "The number of entries in the catalog is hard coded into the following script:\n",
      "\n",
      "perl generateMasterKey666351.pl one2one.txt >MASTERKEY.txt"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#generateMasterKey666351.pl\n",
      "#run on my computer april 2015\n",
      "\n",
      "#/usr/bin/perl -w\n",
      "use strict;\n",
      "\n",
      "my%locusNum;\n",
      "my%IDtaken;\n",
      "my$header=<>;\n",
      "while(my$line=<>){\n",
      "\tchomp $line;\n",
      "\tmy($desiredID,$catID)=split \"\\t\", $line, 2;\n",
      "\t$locusNum{$catID}=$desiredID;\n",
      "\t$IDtaken{$desiredID}++;\n",
      "}\n",
      "foreach my$key (sort keys %IDtaken){\n",
      "\t#print \"IDtaken: $key\\n\";\n",
      "}\n",
      "\n",
      "my$catID=0;\n",
      "my$desiredID=0;\n",
      "\n",
      "while ($desiredID<666351){\n",
      "\t$catID++;\n",
      "\tif(exists $locusNum{$catID}){\n",
      "\t\t#format is original name in catalog \\t new name in catalog (old name from previous study)\n",
      "\t\tprint \"$catID\\t$locusNum{$catID}\\n\";\n",
      "\t}else{\n",
      "\t\t$desiredID++;\n",
      "\t\tmy$nextID=0;\n",
      "\t\t#check if assigned ID already exists, if so then increment until unassigned ID is found\n",
      "\t\tuntil($nextID==1){\n",
      "\t\t\t#print \"desiredID:\\t$desiredID\\n\";\n",
      "\t\t\tif(exists $IDtaken{$desiredID}){\n",
      "\t\t\t\t$desiredID++;\n",
      "\t\t\t\t#print \"desiredID:\\t$desiredID\\r\";\n",
      "\t\t\t}else{\n",
      "\t\t\t\t$nextID=1;\n",
      "\t\t\t}\n",
      "\t\t}\n",
      "\t\tprint \"$catID\\t$desiredID\\n\";\n",
      "\t}\n",
      "}\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When this finished running, and we had a MasterKey, we tested to make sure that it had generated what we thought it had- one unique locus name per individual, with a total of 666351 loci names, as many as the size of the catalog. \n",
      "\n",
      "#Step 5b: Quick Checking the MasterKey \n",
      "Garrett did this with another perl script that takes each line of the master key and puts the name as the key in the dictionary and then the value is one to start and if it cycles through and sees that name again it adds to the count (the value) and then finally only prints the key and the value if the value is greater than 1. Because there were no outputs, the MasterKey appears to be what we expect it to be. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#testOccurances.pl\n",
      "#used on my computer april 2015\n",
      "\n",
      "#/usr/bin/perl -w\n",
      "use strict;\n",
      "\n",
      "my%catIDoccurances;\n",
      "my%desiredIDoccurances;\n",
      "while(my$line=<>){\n",
      "\tchomp $line;\n",
      "\tmy($catID,$desiredID)=split \"\\t\", $line;\n",
      "\t$catIDoccurances{$catID}++;\n",
      "\t$desiredIDoccurances{$desiredID}++;\n",
      "}\n",
      "\n",
      "foreach my$key (sort keys %catIDoccurances){\n",
      "\tif($catIDoccurances{$key}>1){\n",
      "\t\tprint \"$key\\t$catIDoccurances{$key}\\n\";\n",
      "\t}\n",
      "}\n",
      "foreach my$key (sort keys %desiredIDoccurances){\n",
      "\tif($desiredIDoccurances{$key}>1){\n",
      "\t\tprint \"$key\\t$desiredIDoccurances{$key}\\n\";\n",
      "\t}\n",
      "}\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Step 6: Renaming the catalog files\n",
      "\n",
      "The stacks catalog is in three parts, each a .tsv:\n",
      "- .catalog.alleles \n",
      "- .catalog.tags \n",
      "- .catalog.snps\n",
      "\n",
      "Each needs to be renamed; Garrett wrote a script in perl to do this. \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#this renames the catalog files using a masterkey generated at an earlier time\n",
      "#renameTags.pl\n",
      "#used this command: \n",
      "#perl renameTags.pl MASTERKEY.txt batch_3.catalog.alleles.tsv  >batch_3.catalog.allelesRenamed.tsv\n",
      "#perl renameTags.pl MASTERKEY.txt batch_3.catalog.snps.tsv  >batch_3.catalog.snpsRenamed.tsv\n",
      "#perl renameTags.pl MASTERKEY.txt batch_3.catalog.tags.tsv  >batch_3.catalog.tagsRenamed.tsv\n",
      "\n",
      "#/usr/bin/perl -w\n",
      "use strict;\n",
      "\n",
      "my$renameFile=$ARGV[0];\n",
      "chomp $renameFile;\n",
      "my$catalogFile=$ARGV[1];\n",
      "chomp $catalogFile;\n",
      "\n",
      "my%key;\n",
      "open(RENAMEFILE,\"<$renameFile\")||die \"cannot open $renameFile:$!\";\n",
      "while(my$line=<RENAMEFILE>){\n",
      "\tchomp $line;\n",
      "\tmy($original,$new)=split \"\\t\", $line, 2;\n",
      "\t$key{$original}=$new;\n",
      "}\n",
      "close RENAMEFILE;\n",
      "\n",
      "open(CATALOGFILE,\"<$catalogFile\")||die \"cannot open $catalogFile:$!\";\n",
      "while(my$line=<CATALOGFILE>){\n",
      "\tchomp $line;\n",
      "\tmy($col1,$col2,$col3,$rest)=split \"\\t\", $line,4;\n",
      "\tmy$newID=$key{$col3};\n",
      "\tprint \"$col1\\t$col2\\t$newID\\t$rest\\n\";\n",
      "}\n",
      "close CATALOGFILE;"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Step 7: Reorder the catalog entries within each file numerically by catalog id\n",
      "\n",
      "After the files are renamed, they need to be sorted, as Stacks wants the catalog entries to be in order. This script Garrett wrote in perl takes the catalog file and reorders the lines. \n",
      "\n",
      "It takes the line of the renamed file and splits it up by tabs into four parts, (we want the third column) it makes a dictionary with the key the third column (the ID that we want to sort) and then the value for that dictionary entry is the WHOLE LINE. So you go through the whole file that way line by line and then print out the sorted list of keys with their values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#reorderTags.pl\n",
      "#command: perl reorderTags.pl batch_3.catalog.tagsRenamed.tsv  >batch_3.catalog.tagsRenamedReordered.tsv\n",
      " \n",
      "\n",
      "#/usr/bin/perl -w\n",
      "use strict;\n",
      "\n",
      "my%loci;\n",
      "\n",
      "while(my$line=<>){\n",
      "\tchomp $line;\n",
      "\tmy($col1,$col2,$col3,$rest)=split \"\\t\", $line,4;\n",
      "\t$loci{$col3}=$line;\n",
      "}\n",
      "\n",
      "foreach my$key (sort{$a <=> $b} keys %loci){\n",
      "\tprint \"$loci{$key}\\n\";\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It only works for one of the catalog files, the tags file. It didn't work for the SNPs or the alleles files because there were more than one entry per tag ID (catalog id) and because we used a dictionary, it rewrote the earlier entry it had for that and just kept the last one. \n",
      "\n",
      "As a work around, we opened these files in excel and sorted based on column C then copied them back into a txt file. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These final files are the reamed catalog. For the loci that were identified in the earlier studies that align to the modern catalog, the modern catalog has been renamed to the original loci names from the miller pipeline for ease of comparison.\n",
      "\n",
      "The three catalog files themselves have to be named the same as the original output from Stacks, if they are to be used in Stacks downstream, so there can't be any indication in the name that they were renamed. For this reason there are copies of the renamed catalog that are called renamed, but they are for back-up purposes, and the renamed version is also located in a folder called NewCatalog, with all the downstream analysis in a folder called RenamedCatalog to distinguish it from what came earlier. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There is one final step, a quality control that should be completed when the final analysis is run to ensure that we are getting the results that we would expect between the catalogs. This could be done by comparing the results of the population analysis that we get using this new renamed catalog to the results that they got earlier for things like allele frequencies using the miller pipeline and stacks in paper one and two, especially for the outliers. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}