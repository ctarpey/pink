{
 "metadata": {
  "name": "",
  "signature": "sha256:c0f39f10c67ee61b2f79c1a7b018cd3777650f137e79cd4cf4d56102e34abe16"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Environmental Variables "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "worldclim.org\n",
      "\n",
      "september 10? \n",
      "\n",
      "She thinks that I should look at more variables for the environmental correlation as well, and I agree. She sent me a link to this website that has different layers for environmental data that can be extracted with ArcGIS. http://worldclim.org/  Ill take a look at it in a little bit. She said to get someone at the Olden lab to help me get the data out- and then go from there. I might ask Erica ??\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "OK this is what Carita sent me from Progeny: \n",
      "\n",
      "Pedigree name\tLatitude\tLongitude\n",
      "PSNOH96\t\t\n",
      "PNOME94\t\t64.4836\t-165.3017\n",
      "PKOPE91T\t60.7062\t-145.8987\n",
      "PHOOD11X03H\t\t\n",
      "PBIRD13X\t\t\t\n",
      "PKUSHI07\t\t\n",
      "PHAYLY10\t\t\n",
      "PTAUY12\t\t\n",
      "PSNOH03\t\t\n",
      "PNOME91\t\t64.4836\t-165.3017\n",
      "PKOPE96T\t60.7062\t-145.8987\t\t\n",
      "PKUSHI06\t\t\t\n",
      "PHAYLY09\t\t\t\t\n",
      "PTAUY09\t\t\t\n",
      "PAMUR10\t\t\n",
      "PAMUR11\t\t\t\t\n",
      "PHOOD13X\t\t\n",
      "\t\n",
      "\n",
      "And this is what i filled out thanks to Google Maps:\n",
      "\n",
      "POPNAME\tlatitude\tLongitude\n",
      "AMUR10\t53.045249\t140.9545101\n",
      "AMUR11\t53.045249\t140.9545101\n",
      "HAYLY09\t57.769904\t162.485706\n",
      "HAYLY10\t57.769904\t162.485706\n",
      "KOPPE91\t60.7062\t-145.8987\n",
      "KOPPE96\t60.7062\t-145.8987\n",
      "KUSHI06\t42.980725\t144.378062\n",
      "KUSHI07\t42.980725\t144.378062\n",
      "NOME91\t64.4836\t-165.3017\n",
      "NOME94\t64.4836\t-165.3017\n",
      "SNOH03\t48.014461\t-122.181078\n",
      "SNOH96\t48.014461\t-122.181078\n",
      "TAUY09\t59.714924\t148.929489\n",
      "TAUY12\t59.714924\t148.929489\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "but he also said that the longitude should be all together, so that they are on the same scale, dont treat them as seperate coast lines with different scales. And I brought up that i had thought he mean to put them on a circle, where the center of the space between them was a zero, and they were around that spot in a circle, he said that is a good idea, i should do that too, that I can do more than one way. But i dont really know how to get that value, he said i could plot the PCA of them, which is pretty interesting. He says that the order matters, you need one line for the environmental gradient and one column for each of the populations. he said another good one is temperature, to look up in some papers that do this sort of thing and pick their variable, then go to some site that has all the data we need. I can even just pick the latitude off the map and then have temperature for some close by site, it doesn't have to be that exact place. it could be something like the temperature in August or something general like that. \n",
      "\n",
      "\n",
      "Talked to Morten and Ryan about the process, they say that the two coasts should be standardized together so that they are on the same scale, you wouldnt want nome to be the same as hayly and tauy, which is what standardizing them independently would do. Also they said that there maybe some sort of value in longitude, (ryan says no, there really isnt and an environmental variable, taken from some past pink work would be more worth the time) but Morten was like, yeah they could be independent variables that split up the coasts, but they wouldnt be signs of parallel adaptation, they would be some sort of neutral signal. \n",
      "\n",
      "we also talked about the pros and cons of doing something like distance from a PCA (would just be the longitude) or using some measure of coastal distance from a point to get at the geography of the kamchatka penninsula but they decided that would just be giving you the signals of drift. So the best plan now is to just use R to standardize the latitudes and have that as the main variable. \n",
      "\n",
      "\n",
      "Im using this as the starting point for standardizing, its POPINFO.txt in the Populations_b2_may file: \n",
      "\n",
      "CLUSTER\tSILLI\tPOPNAME\tYEAR\tLINEAGE\tCONTINENT\tlatitude\tLongitude\n",
      "pop_1\tPAMUR10\tAMUR10\t2010\tEven\tAsia\t53.045249\t140.9545101\n",
      "pop_2\tPAMUR11\tAMUR11\t2011\tOdd\tAsia\t53.045249\t140.9545101\n",
      "pop_3\tPHAYLY09\tHAYLY09\t2009\tOdd\tAsia\t57.769904\t162.485706\n",
      "pop_4\tPHAYLY10\tHAYLY10\t2010\tEven\tAsia\t57.769904\t162.485706\n",
      "pop_5\tPKOPPE91\tKOPPE91\t1991\tOdd\tNorthAmerica\t60.7062\t-145.8987\n",
      "pop_6\tPKOPPE96\tKOPPE96\t1996\tEven\tNorthAmerica\t60.7062\t-145.8987\n",
      "pop_7\tPKUSHI06\tKUSHI06\t2006\tEven\tAsia\t42.980725\t144.378062\n",
      "pop_8\tPKUSHI07\tKUSHI07\t2007\tOdd\tAsia\t42.980725\t144.378062\n",
      "pop_9\tPNOME91\tNOME91\t1991\tOdd\tNorthAmerica\t64.4836\t-165.3017\n",
      "pop_10\tPNOME94\tNOME94\t1994\tEven\tNorthAmerica\t64.4836\t-165.3017\n",
      "pop_11\tPSNOH03\tSNOH03\t2003\tOdd\tNorthAmerica\t48.014461\t-122.181078\n",
      "pop_12\tPSNOH96\tSNOH96\t1996\tEven\tNorthAmerica\t48.014461\t-122.181078\n",
      "pop_13\tPTAUY09\tTAUY09\t2009\tOdd\tAsia\t59.714924\t148.929489\n",
      "pop_14\tPTAUY12\tTAUY12\t2012\tEven\tAsia\t59.714924\t148.929489\n",
      "\n",
      "\n",
      "and im doing the standardization in R using some code I found on Stack Overflow. \n",
      "\n",
      "\n",
      "envDat <- read.table (\"G:/Analysis/Pop_analysis/Populations_b3_may/POPINFO.txt\", header = TRUE, sep = \"\\t\")\n",
      "\n",
      "envDat <- as.data.frame(envDat)  \n",
      "\n",
      "Std_envDat<- scale(envDat$latitude)\n",
      "\n",
      "envDat$latitude\n",
      "\n",
      "Std_envDat\n",
      "\n",
      "> envDat$latitude\n",
      " [1] 53.04525 53.04525 57.76990 57.76990 60.70620 60.70620 42.98072 42.98072 64.48360 64.48360 48.01446 48.01446\n",
      "[13] 59.71492 59.71492\n",
      "\n",
      "> Std_envDat\n",
      "            [,1]\n",
      " [1,] -0.3001785\n",
      " [2,] -0.3001785\n",
      " [3,]  0.3445463\n",
      " [4,]  0.3445463\n",
      " [5,]  0.7452322\n",
      " [6,]  0.7452322\n",
      " [7,] -1.6735799\n",
      " [8,] -1.6735799\n",
      " [9,]  1.2606949\n",
      "[10,]  1.2606949\n",
      "[11,] -0.9866780\n",
      "[12,] -0.9866780\n",
      "[13,]  0.6099630\n",
      "[14,]  0.6099630\n",
      "attr(,\"scaled:center\")\n",
      "[1] 55.24501\n",
      "attr(,\"scaled:scale\")\n",
      "[1] 7.328174\n",
      "\n",
      "\n",
      "cool, so i feel good about the numbers, I also did the calculation by hand in excel to make sure I understood what was going on. \n",
      "\n",
      "\n",
      "> envDat$Longitude\n",
      " [1]  140.9545  140.9545  162.4857  162.4857 -145.8987 -145.8987  144.3781\n",
      " [8]  144.3781 -165.3017 -165.3017 -122.1811 -122.1811  148.9295  148.9295\n",
      "> Std_long\n",
      "            [,1]\n",
      " [1,]  0.7767843\n",
      " [2,]  0.7767843\n",
      " [3,]  0.9189846\n",
      " [4,]  0.9189846\n",
      " [5,] -1.1177044\n",
      " [6,] -1.1177044\n",
      " [7,]  0.7993947\n",
      " [8,]  0.7993947\n",
      " [9,] -1.2458492\n",
      "[10,] -1.2458492\n",
      "[11,] -0.9610641\n",
      "[12,] -0.9610641\n",
      "[13,]  0.8294541\n",
      "[14,]  0.8294541\n",
      "attr(,\"scaled:center\")\n",
      "[1] 23.33804\n",
      "attr(,\"scaled:scale\")\n",
      "[1] 151.4146\n",
      "\n",
      "\n",
      "heres the newest version! \n",
      "\n",
      "CLUSTER\tSILLI\tPOPNAME\tYEAR\tLINEAGE\tCONTINENT\tLatitude\tLongitude\tStand_Lat\tStand_Long\n",
      "pop_1\tPAMUR10\tAMUR10\t2010\tEven\tAsia\t53.045249\t140.9545101\t-0.3001785\t0.7767843\n",
      "pop_2\tPAMUR11\tAMUR11\t2011\tOdd\tAsia\t53.045249\t140.9545101\t-0.3001785\t0.7767843\n",
      "pop_3\tPHAYLY09\tHAYLY09\t2009\tOdd\tAsia\t57.769904\t162.485706\t0.3445463\t0.9189846\n",
      "pop_4\tPHAYLY10\tHAYLY10\t2010\tEven\tAsia\t57.769904\t162.485706\t0.3445463\t0.9189846\n",
      "pop_5\tPKOPPE91\tKOPPE91\t1991\tOdd\tNorthAmerica\t60.7062\t-145.8987\t0.7452322\t-1.1177044\n",
      "pop_6\tPKOPPE96\tKOPPE96\t1996\tEven\tNorthAmerica\t60.7062\t-145.8987\t0.7452322\t-1.1177044\n",
      "pop_7\tPKUSHI06\tKUSHI06\t2006\tEven\tAsia\t42.980725\t144.378062\t-1.6735799\t0.7993947\n",
      "pop_8\tPKUSHI07\tKUSHI07\t2007\tOdd\tAsia\t42.980725\t144.378062\t-1.6735799\t0.7993947\n",
      "pop_9\tPNOME91\tNOME91\t1991\tOdd\tNorthAmerica\t64.4836\t-165.3017\t1.2606949\t-1.2458492\n",
      "pop_10\tPNOME94\tNOME94\t1994\tEven\tNorthAmerica\t64.4836\t-165.3017\t1.2606949\t-1.2458492\n",
      "pop_11\tPSNOH03\tSNOH03\t2003\tOdd\tNorthAmerica\t48.014461\t-122.181078\t-0.986678\t-0.9610641\n",
      "pop_12\tPSNOH96\tSNOH96\t1996\tEven\tNorthAmerica\t48.014461\t-122.181078\t-0.986678\t-0.9610641\n",
      "pop_13\tPTAUY09\tTAUY09\t2009\tOdd\tAsia\t59.714924\t148.929489\t0.609963\t0.8294541\n",
      "pop_14\tPTAUY12\tTAUY12\t2012\tEven\tAsia\t59.714924\t148.929489\t0.609963\t0.8294541\n",
      "\n",
      "#stand_lat.txt\n",
      "\n",
      "-0.3001785 -0.3001785 0.3445463 0.3445463 0.7452322 0.7452322 -1.6735799 -1.6735799 1.2606949 1.2606949 -0.986678 -0.986678 0.609963 0.609963\n",
      "\n",
      "#stand_long.txt\n",
      "\n",
      "0.7767843 0.7767843 0.9189846 0.9189846 -1.1177044 -1.1177044 0.7993947 0.7993947 -1.2458492 -1.2458492 -0.9610641 -0.9610641 0.8294541 0.8294541\n",
      "\n",
      "\n",
      "sep 14\n",
      "\n",
      "Environmental variables WORLDCLIM:\n",
      "\n",
      "Data Format\n",
      "\n",
      "These layers (grid data) cover the global land areas except Antarctica. They are in the latitude / longitude coordinate reference system (not projected) and the datum is WGS84. There are four monthly variables: average minimum, mean, and maximum temperature and precipitations. There are also 18 bioclimatic variables.\n",
      "\n",
      "Please note that the temperature data are in \u00b0C * 10. This means that a value of 231 represents 23.1 \u00b0C. This does lead to some confusion, but it allows for much reduced file sizes which is important as for many downloading large files remains difficult. The unit used for the precipitation data is mm (millimeter).\n",
      "\n",
      "The data are available at 4 different spatial resolutions; from 30 seconds (0.93 x 0.93 = 0.86 km2 at the equator) to 2.5, 5 and 10 minutes (18.6 x 18.6 = 344 km2 at the equator). The original data were at a 30 second resolution, the other data have been derived through aggregation, by calculating the mean of groups of cells. Cells with 'no data' were ignored. In other words, if some of the original cells were on land, and some cells were on sea, the aggregate cells have data. Only if all original cells have 'no data' then the aggregate cell has 'no data'. Aggregation was done for monthly precipitation, minimum, mean and maximum temperature. The Bioclimatic variables were calculated from these aggregated data. \n",
      "\n",
      " Bioclim\n",
      "\n",
      "BIOCLIM\n",
      "\n",
      "Bioclimatic variables are derived from the monthly temperature and rainfall values in order to generate more biologically meaningful variables. These are often used in ecological niche modeling (e.g., BIOCLIM, GARP). The bioclimatic variables represent annual trends (e.g., mean annual temperature, annual precipitation) seasonality (e.g., annual range in temperature and precipitation) and extreme or limiting environmental factors (e.g., temperature of the coldest and warmest month, and precipitation of the wet and dry quarters). A quarter is a period of three months (1/4 of the year).\n",
      "\n",
      "They are coded as follows:\n",
      "\n",
      "BIO1 = Annual Mean Temperature\n",
      "BIO2 = Mean Diurnal Range (Mean of monthly (max temp - min temp))\n",
      "BIO3 = Isothermality (BIO2/BIO7) (* 100)\n",
      "BIO4 = Temperature Seasonality (standard deviation *100)\n",
      "BIO5 = Max Temperature of Warmest Month\n",
      "BIO6 = Min Temperature of Coldest Month\n",
      "BIO7 = Temperature Annual Range (BIO5-BIO6)\n",
      "BIO8 = Mean Temperature of Wettest Quarter\n",
      "BIO9 = Mean Temperature of Driest Quarter\n",
      "BIO10 = Mean Temperature of Warmest Quarter\n",
      "BIO11 = Mean Temperature of Coldest Quarter\n",
      "BIO12 = Annual Precipitation\n",
      "BIO13 = Precipitation of Wettest Month\n",
      "BIO14 = Precipitation of Driest Month\n",
      "BIO15 = Precipitation Seasonality (Coefficient of Variation)\n",
      "BIO16 = Precipitation of Wettest Quarter\n",
      "BIO17 = Precipitation of Driest Quarter\n",
      "BIO18 = Precipitation of Warmest Quarter\n",
      "BIO19 = Precipitation of Coldest Quarter\n",
      "\n",
      "This scheme follows that of ANUCLIM, except that for temperature seasonality the standard deviation was used because a coefficient of variation does not make sense with temperatures between -1 and 1).\n",
      "\n",
      "This AML (Arc-Info workstation script) was used to generate these layers.\n",
      "You can also use the 'biovars' method in the R package dismo\n",
      "\n",
      "\n",
      "The website explains how to import the data yourself in R, but I cant get the packages it needs to install (specifically rgdal)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I also went and talked to Erika sutherland, and Mathis from the olden lab. Erika showed me what the basics were in GIS and how i would go about getting the numbers out, and introduced me to Mathis who is their GIS wiz, then I talked to Eica Escajeda who is TAing the GIS class and loves it. she told me to go to the grad student lab and to just use the version they have there and get the numbers out that I need. I think with what they showed me I should be able to get it done really easily. \n",
      "\n",
      "Basically, you use a map, a basic continents map or whatever, something that is already made in the program that you load the layers onto. they are premade layers that come in a format that you cant extract the data anyother way, they have to be opened by GIS. then from there you go to a thing called the catalog and you right click on a layer and here the layers would be the different variables, and you say get attributes table. That will be the numbers. The other way you can do it, which might be easier depending on how complete these data sets are is to click on a point on the map , so make a layer that has my populations and then get attributes for the place, and if all the layers are loaded in it will give me all the details that i need about that one place, so all the data together. \n",
      "\n",
      "\n",
      "\n",
      "NOTES FROM QUINNS BOOK ON SALMON: \n",
      "\n",
      "\n",
      "pg 13 \n",
      "\" some rivers have only odd year or even year runs and this has puzzled scientists for many years. the odd year runs seem better adapted to warmer water and the differences between the lines may have resulted from their colonization from northern (even) and southern (odd) glacial reguges. \n",
      "\n",
      "pink salmon produce small egges, generally in the lower reaches of rivers. the fry emerge from the gravel about 29-33mm long without parr marks, and are fully adapted for seawater. they are slim bodied and their color and shape are adaptations for countershading adn locomotion in open marine water. they migrate diferlty to sea and make essentially no use of freshwater for readring. they can therfore wpawn at very high densities because food, rather than space for incubating enbryos, tends to limit the density of juvenile salmon in streams. at sea pinkl salmon migrate to offshore waters and the asian populaitons tend to migrate fartehr toward North america than the asian. Males and females are similar in average size 9about 2kg) but males vary much more than females (about .5 to 6 kg) the species displays extreme sebual dimorphism notably in the dorsal hump and the elongaged jaws of the males. \n",
      "\n",
      "from page 11\n",
      "\n",
      "The amur river flows 4510km from Mongolia to the sea of Okhotsk with a discharge of about 12500 m3/s. or roughly twice the discharge of the Yukon. \n",
      "\n",
      "\n",
      "I didnt find anything in there that tells me the return months. I know there was one plot about the retun in SE alaska that was basically July through Sept, with the majority in end of July?  IDK how representative that was of all the pops and all the range. \n",
      "\n",
      "\n",
      "         $$ September 29 2015\n",
      "==================================================================\n",
      "\n",
      "finished downloading the data that I need to go to GIS and i am extracting it all using the program 7 zip. \n",
      "\n",
      "I went over to the graduate student computer lab to try to figure out what is going on with ArcGIS, and i talked to a couple people about it. I was able to get the files to load into the program, and the map made sense and seemed normal, then I put dots down where my coordinates are to try to use those as a way to get the data out that I want. but right clicking on the layers didnt let me choose the get attributes table, where it would have the data in a table format and I could just take the cells that are the easiest for me. \n",
      "\n",
      "Mike Vlah came up with this solution: \n",
      "\n",
      "Just a shart in the dark, but I googled what to do when your attribute table is greyed out.  That happens when the layer you're dealing with isn't a raster.  That is, it doesn't have geospatial data linked to every pixel.  You might be able to convert what you do have into a raster and then extract values from it.  Chez this out:\n",
      "\n",
      "http://support.esri.com/es/knowledgebase/techarticles/detail/33239\n",
      "\n",
      "particularly:\n",
      "\n",
      "\u2022 To add a raster attribute table in ArcGIS 10.x:\n",
      "1. In ArcMap or ArcCatalog, click ArcToolbox > Data Management Tools > Raster > Raster Dataset > Copy Raster.\n",
      "2. In the Pixel Type drop-down, select the 32_Bit_Signed integer. This converts the 32-bit floating point raster to a 32-bit signed integer raster.\n",
      "3. Click ArcToolbox > Data Management Tools > Raster > Raster Properties > Build Raster Attribute Table.\n",
      "4. Input the 32-bit signed integer output raster from step 1. This output raster will have an attribute table now.\n",
      "\n",
      "That totally does work, but there are over a hundred files and it takes a really long time to do them all. So I went back to my computer and worked at trying to get them to open in R. I can, but its also one file at a time, and they dont really seem to have an easy way to tell what the coordinates are. \n",
      "\n",
      "So im looking back through the documentation and the different file formats presented on the website to see if i need something that might be more appropriate? Im downloading the 30s ESRI files, and maybe they will work better in ArcGIS. Ill give them a shot tomorrow \n",
      "\n",
      "\n",
      "         $$ September 29 2015\n",
      "==================================================================\n",
      "\n",
      "worked to get the environemental data for the seven sites, and finally got it to work following the instructions of people on this discusison forum https://groups.google.com/forum/#!topic/maxent/045l4NYb4JI and then finally this youtube video  https://www.youtube.com/watch?v=KRZ3GTRyEE0. \n",
      "\n",
      "The key was to use all the data, not the individual tiles that are offered on the map. I saved them in the Udrive so that i could have them wherever I needed, since I was using a computer in the graduate student computer lab for the GIS portion\n",
      "\n",
      "I used the lat and long that I picked and used for the outlier tets, but I was not able to get the environmental data for Amur for some of the Temperature variables and I think its because the lats and longs that i picked for amur are too close to the ocean for there to be complete data in all the categories. \n",
      "\n",
      "\n",
      "         $$ October 1 2015\n",
      "==================================================================\n",
      "I made some plots of the data that I did get out of Arc Gis and it makes sense to me, so I think I did get what I needed. I went to lisat to ask about what I should do about the lat long and she said get a new lat and long based as much as you can on where they are actually spawning since we dont know where the samples themselves were taken. \n",
      "\n",
      "i looked in a couple things and there was one reference in this book to a paper in 1966 that said that the majority of the heaviest spawning happened in a river called Amgun that is a tributary to the Amur and that spawning could happen up to 700km up the river. \n",
      "\n",
      "So I picked some points that are before that, that river mouth and then after that. I picked four points in total on that stretch of the beginning of the Amur, they arent that different from the mouth of the river, theyre just more inland so that there is probably a greater chance of there being data about them. \n",
      "\n",
      "here are the spots that I chose: it didnt really matter to me, its just that if i were going to go through the process of the whole extraction and making a layer thing I wanted to get some data out so I chose a lot of points. \n",
      "\n",
      "\n",
      "Name\tX\tY\n",
      "Tyr\t139.727554\t52.938669\n",
      "Kray\t139.286728\t52.970108\n",
      "Susanino\t140.094223\t52.752122\n",
      "Takhta\t139.834198\t53.146954\n",
      "\n",
      "\n",
      "So they all worked. I plotted them the same way i did with the others and the data were really nearly identical. I thought about chosing the point that was the most close to the mouth of the river so that the outlier test wouldnt change so much but then i realized that it would all have to be redone anyway, so I chose the one that was at the mouth of the amgun river, sufficiently close to the mouth and also recognizing that a good deal of the fish could have come from the amgun river. \n",
      "\n",
      "thats this point: \n",
      "\n",
      "Tyr\t139.727554\t52.938669\n",
      "\n",
      "I added the environmental data to the others and called it AMUR instead of the original AMUR point. \n",
      "\n",
      "I still have to standardize them all in R. \n",
      "\n",
      "\n",
      "UGH when I was looking at them again I realized that I didnt export the 11th month of the tmin variable for the rest of the 6 sites. \n",
      "\n",
      "So I made a new sheet that has all 7 locations (the new amur point)\n",
      "\n",
      "        $$ October 2 2015\n",
      "==================================================================\n",
      "\n",
      "\n",
      "Pop\tX\tY\n",
      "Nome\t-165.301700\t64.483600\n",
      "Koppen\t-145.898700\t60.706200\n",
      "Tauy\t148.929489\t59.714924\n",
      "Haylyluya\t162.485706\t57.769904\n",
      "Snohomish\t-122.181078\t48.014461\n",
      "Kushiro\t144.378062\t42.980725\n",
      "Amur\t139.727554\t52.938669\n",
      "\n",
      "\n",
      "I got all the data out of GIS that I needed, and here are the points that I have. I need to standardize the latitude and longitude as well as the environmental points. \n",
      "\n",
      "I need to add the new points to the all data sheet too. \n",
      "\n",
      "\n",
      "I used this before to do it: \n",
      "### Standardizing Environmental Data\n",
      "###   latitudes and longitudes of populations\n",
      "### Carolyn Tarpey | September 2015 \n",
      "### ---------------------------------------\n",
      "\n",
      "envDat <- read.table (\"G:/Analysis/Pop_analysis/Populations_b3_may/POPINFO.txt\", header = TRUE, sep = \"\\t\")\n",
      "envDat <- as.data.frame(envDat)  \n",
      "Std_lat<- scale(envDat$latitude)\n",
      "Std_long<- scale(envDat$Longitude)\n",
      "\n",
      "envDat$latitude\n",
      "Std_lat\n",
      "envDat$Longitude\n",
      "Std_long\n",
      "\n",
      "# check that we get mean of 0 and sd of 1\n",
      "colMeans(Std_envDat)  # faster version of apply(scaled.dat, 2, mean)\n",
      "apply(Std_envDat, 2, sd)\n",
      "\n",
      "I edited it and after a lot of messing around got it to do all the variables and write it to a txt file: \n",
      "Heres the gist of it: (i had to cut out the names of the columns and the names of the rows because it wasnt working with them in)\n",
      " \n",
      "POP_ENV_noheader <- read.table (\"G:/Analysis/Pop_analysis/Populations_b3_may/POP_ENV_noheader.txt\", header = FALSE, sep = \"\\t\")\n",
      "POP_ENV_noheader <- as.data.frame(POP_ENV_noheader) \n",
      "colsPOP_ENV_noheader <- c(names(POP_ENV_noheader))\n",
      "POP_ENV_noheader.dat <- scale(POP_ENV_noheader)\n",
      "POP_ENV_noheader.dat\n",
      "write.table(POP_ENV_noheader.dat, file = \"POP_ENV_noheader.txt\", sep =\"\\t\", row.names = TRUE, col.names = TRUE)\n",
      "\n",
      "\n",
      "\"V1\"\t\"V2\"\t\"V3\"\t\"V4\"\t\"V5\"\t\"V6\"\t\"V7\"\t\"V8\"\t\"V9\"\t\"V10\"\t\"V11\"\t\"V12\"\t\"V13\"\t\"V14\"\t\"V15\"\t\"V16\"\t\"V17\"\t\"V18\"\t\"V19\"\t\"V20\"\t\"V21\"\t\"V22\"\t\"V23\"\t\"V24\"\t\"V25\"\t\"V26\"\t\"V27\"\t\"V28\"\t\"V29\"\t\"V30\"\t\"V31\"\t\"V32\"\t\"V33\"\t\"V34\"\t\"V35\"\t\"V36\"\t\"V37\"\t\"V38\"\t\"V39\"\t\"V40\"\t\"V41\"\t\"V42\"\t\"V43\"\t\"V44\"\t\"V45\"\t\"V46\"\t\"V47\"\t\"V48\"\t\"V49\"\t\"V50\"\t\"V51\"\n",
      "\"1\"\t0.769888387774808\t-1.6048274669907\t-0.314377855470597\t-0.194044921817816\t-0.21802413518684\t0.352619752686746\t0.949635205500196\t1.38606187031657\t1.51661266828426\t1.26823083465226\t0.951091888616683\t0.831349655118686\t0.302715542366678\t-0.0740554424885844\t-0.228184243755172\t0.708231577022063\t0.544577090176628\t0.577258375811174\t0.695785490371705\t0.639077220271321\t0.0225925393498808\t0.379524226911525\t0.891522917470157\t1.16350233402239\t1.28546653874045\t1.21564299979485\t0.944968108493837\t0.565601024754835\t0.450829859846519\t0.659199453063274\t0.796982502107292\t0.785310567297698\t0.444737752577604\t0.838666315535728\t1.28874053206742\t1.4160878830228\t1.28872916483004\t1.13051959107512\t0.843879158906201\t0.353283151777825\t0.307314467016176\t0.656271828838433\t0.832895579749489\t0.88544770469448\t0.88846006251916\t1.28018379065272\t1.65358551707728\t1.69753844424366\t1.25900067614452\t0.970123019096399\t0.673306957355745\n",
      "\"2\"\t0.740351092546943\t-0.300173535790688\t-0.446416554768248\t-0.74971901611429\t-0.644453693714041\t-0.639555433794588\t-0.495461846347929\t-0.177912419174962\t-0.404740190615738\t0.0991620817217816\t-0.203554721548958\t-0.370652113239314\t-0.357555314534554\t-0.672195554896383\t-0.551040673749191\t-1.11767795748794\t-0.839486529779972\t-0.439910693290585\t-0.0364922459985159\t0.311173725053575\t1.22451563276354\t1.21359491163569\t0.944861895438457\t0.469367418838576\t-0.0218286770729511\t-0.588214354739444\t-1.01199871826589\t-1.18791970101712\t-0.946576347795827\t-0.565028102625663\t-0.0693028262701993\t0.171548424895128\t1.03074514420927\t1.23484610876426\t0.878686726409606\t0.372654706058632\t-0.0976721051239613\t-0.690438435379785\t-1.07537130171899\t-1.24184380624932\t-1.05548843121786\t-0.71050917006475\t-0.150315474451831\t-0.10745724571535\t0.607592042755038\t1.1165073242679\t0.728853120474834\t0.212192305530457\t-0.250531620693746\t-0.834143432737698\t-1.15332005135577\n",
      "\"3\"\t0.79879637868829\t0.587626970474841\t-0.22635205593883\t-0.955524236224095\t-0.891333964440316\t-1.07514454005469\t-0.957892902939329\t-0.699237182338805\t-0.502436098695399\t-0.266171903568994\t-0.621192857140786\t-0.575871927349216\t-0.633947766260651\t-0.831699584871796\t-0.873897103743211\t-1.08447960231503\t-1.03888552587541\t-0.969395962138076\t-0.836888841565967\t-0.906753542898052\t-0.863035003165448\t-0.701678512545728\t-0.548629487673943\t-0.733799767480027\t-1.07445599370192\t-1.28100015032145\t-1.11499697230588\t-0.934265759553252\t-0.899996140874416\t-0.914807404251074\t-0.814308208674841\t-0.78912275451759\t-0.654026106731771\t-0.457922098666748\t-0.351474690563843\t-0.566435153209121\t-1.04726201605136\t-1.1970207434913\t-0.97193864216434\t-0.781505153932763\t-0.762654750605588\t-0.862373725498437\t-0.823038827326418\t-0.618953735320413\t-0.435632030654556\t-0.111066173618273\t-0.132104628086064\t-0.411653072729087\t-1.04969577784342\t-1.10407305939004\t-0.815055790483271\n",
      "\"4\"\t0.884897506500994\t0.332797460561683\t-0.424410104885306\t-0.111722833773894\t-0.173136813236608\t-0.324963301495629\t-0.639971551532741\t-0.757162156023677\t-0.697827914854721\t-0.741106084447001\t-0.473791162226023\t-0.590530485499923\t-0.434330995569581\t-0.233559472463997\t-0.296154018490755\t-0.287719078165213\t-0.417229832166094\t-0.551381276205846\t-0.683621408372199\t-0.859910186438374\t-0.609998562446783\t-0.516329471495913\t-0.468621020721493\t-0.433007970900377\t-0.327430156094265\t-0.326785752633024\t-0.268122439088217\t-0.294616689774803\t-0.387613864738889\t-0.524668952438116\t-0.675702556134443\t-0.895863996674558\t-0.873778878593646\t-0.674020167700494\t-0.497922478298777\t-0.462091835512704\t-0.287590087309442\t-0.320770264595708\t-0.282387578466666\t-0.331872051670077\t-0.390981232905396\t-0.545989235011589\t-0.719542926884174\t-1.04018613852458\t-1.23811208712347\t-0.929448505542386\t-0.642301812418447\t-0.589894609374671\t-0.317128633789552\t-0.379525114165327\t-0.341485825261766\n",
      "\"5\"\t-1.19701556256545\t1.21240171213026\t-0.402403655002365\t-0.770299538125271\t-0.779115659564737\t-0.881549381716864\t-0.986794843976291\t-1.16263697181778\t-1.05604624448014\t-0.448838896214381\t-0.0807199757866559\t-0.531896252897094\t-0.879629945572738\t-0.931389603606429\t-0.839912216375419\t-0.254520722992304\t-0.511064653858067\t-0.830057733493999\t-1.10936427835489\t-0.906753542898052\t-0.926294113345114\t-1.10326810148699\t-1.29537517923014\t-1.17341854709644\t-0.98956669397378\t-0.496714344002197\t-0.416897694923751\t-0.283588257537243\t-0.527354485503123\t-0.820636053813463\t-1.12617092689074\t-0.869178686135316\t-0.910404340570625\t-1.03418361609007\t-1.17158230187947\t-1.11423757111531\t-0.990286621395719\t-0.498758643121374\t-0.477760379847674\t-0.363988701831698\t-0.571186574820641\t-0.837062966259489\t-1.1680251621339\t-0.919834023323392\t-0.957244067359353\t-1.052205855331\t-1.08872434870928\t-1.15432614208569\t-1.07189478220869\t-0.564213806085353\t-0.578270807872518\n",
      "\"6\"\t-1.07377910338588\t0.717500379112631\t2.2603767808336\t1.47297736107161\t1.8243490135487\t1.5383900975059\t1.44096820312856\t1.27021192294682\t1.28865554943172\t1.37783103023949\t1.6881003631905\t1.93074151642174\t2.09926647858631\t1.66055088349403\t1.70895433620895\t0.365181906902001\t0.450742268484656\t0.36825103284506\t0.218953475991096\t-0.0401514483940095\t-0.388591676817951\t-0.670787005704092\t-0.761985399547143\t-0.664386275961646\t-0.293474436203008\t-2.90244053354115e-17\t0.281201582458374\t0.378117676716325\t0.450829859846519\t0.336326251562895\t0.173257065675498\t-0.17536061211502\t-0.580775182777813\t-0.962150926412155\t-1.02513451414454\t-0.827293447450163\t-0.30658188552799\t0.00782366499013918\t0.292238307948062\t0.599510803016914\t0.656462316976962\t0.567684171502116\t0.436161294720886\t0.103158955886736\t-0.315260022184218\t-0.88852938894618\t-0.961175052626187\t-0.6493084549232\t0.0158564316894776\t0.302402363693229\t0.538001253006744\n",
      "\"7\"\t-0.923138699559698\t-0.945325519498025\t-0.446416554768248\t1.30833318498376\t0.881715252593838\t1.03020280686912\t0.689517736167534\t0.140674936091831\t-0.144217769069976\t-1.28910706238316\t-1.25993353510476\t-0.693140392554875\t-0.0965179990154626\t1.08234877483316\t1.0802339199048\t1.67098387703643\t1.81134718301826\t1.84523625647227\t1.75162780792877\t1.76331777530359\t1.54081118366187\t1.39894395268551\t1.23822627426411\t1.37174280857753\t1.42128941830548\t1.47707160190127\t1.58584613363153\t1.75667170641126\t1.85988111921922\t1.82961480850215\t1.71524495018743\t1.77266705724966\t1.54350161188698\t1.05476438456947\t0.878686726409606\t1.18131541820586\t1.44066355057843\t1.56864483052291\t1.67134043534341\t1.76641575888912\t1.81653420555635\t1.73197909649372\t1.59186551632595\t1.69782448230252\t1.4501961020474\t0.584558808517224\t0.441867204287868\t0.89545152933853\t1.4143937067014\t1.6094300295888\t1.67682426461084\n",
      "\n",
      "\n",
      "I did a spot check in excel to make sure that it was not scrambling the population order or anything like that. \n",
      "\n",
      "The only weird one is this tmax for november: \n",
      "here are the raw numbers: \n",
      "\n",
      "tmax_11\n",
      "8.0\n",
      "-5.8\n",
      "-11.1\n",
      "-3.8\n",
      "-5.1\n",
      "-1.3\n",
      "10.0\n",
      "\n",
      "The average and the standard deviation, done in excel, respectively: \n",
      "-1.300000\n",
      "7.650272\n",
      "\n",
      "This is what R gave me as the standardized values, see pop #6: \n",
      "V26\n",
      "1.215643\n",
      "-0.588214355\n",
      "-1.28100015\n",
      "-0.326785753\n",
      "-0.496714344\n",
      "-2.90E-17\n",
      "1.477071602\n",
      "\n",
      "Then the standardize() function in excel with the calculated mean and stdev, again look at pop #6:\n",
      "\n",
      "1.215643\n",
      "-0.588214355\n",
      "-1.28100015\n",
      "-0.326785753\n",
      "-0.496714344\n",
      "-5.80488E-17\n",
      "1.477071602\n",
      "\n",
      "\n",
      "Garrett says its OK to replace that with 0, as it is essentially 0. the raw data is exactly the mean, so it doesnt really need to be standardized. Plus the programs that I want probably dont handle scientific notation well, or dont handle an input of 17 decimals. \n",
      "\n",
      "So I replaced it with 0. \n",
      "\n",
      "I added the data to the POPINFO and environmental sheet and wondered how to pick the months to include in the tests. I looked through quinns book for some indication and found on pg 147 this: \n",
      "\n",
      "sheridan (1962) provided an example of the importance of matching spawinig data to the thermal regime of the site. pink salmon spawn in the kadashan and klawock rivers in southeasat alaska, the kadashan is coler and the salmon spawn primarily in mid-august, wheras they spawn at the end of september in teh warmer klawock; but in both sites the juveniles emerge and migrate to the sea in April and MAy. Using observed temperature regimes and the temperature specific rate of development, Sheridan estimated if kadashan (early) fish were to spawn in the warmer klawock, their progeny wourld emerge in early november, and if (late) klawock fish spawined in the cold kadashan, their progeny would emerge at the end of June. Marine survival tends todepend heavily on date of entry into the ocean , so we may infer that natural selection has adapted these populations to spawn at the appropoiate dates, given the thermal regimes in their respeoctive rivers. \n",
      "\n",
      "pg 150 talks about the river temperature and the lethal temperatures for spawning and hatching etc. \n",
      "\n",
      "http://www.kamchatka.org.ru/fishing.html\n",
      "talks about going fishing for pink from july to august in the opala and bistraya rivers in Kamchatka\n",
      "\n",
      "HMMM. So I feel like I should just make the files for all the months. That is going to be 50 tests. lat, long and 12 months for the tmax, tmin and tmean and precipt. for four groupings (Asia odd, even, NA odd, even)  for each test. thats two hundred tests per program. Thats going to take a LONG ASS TIME. not to mention be really really hard to pare down into something that is significant or understandable. or maybe i wont even be able to keep track of the results. \n",
      "\n",
      "i went looking for ben hecht's paper about how he used the data but I cant find it. I emailed lisa for it. \n",
      " \n",
      "    \n",
      "Moving on. \n",
      "\n",
      "programs that use environmental data: \n",
      "\n",
      "bayenv2\n",
      "\n",
      "bayeScEnv\n",
      "\n",
      "LFMM\n",
      "\n",
      "and which of them can take more than one variable at a time? \n",
      "\n",
      "Luckily the BayeScEnv2 program has the population files split out already, so I think once they are formatted for the differnt programs that should be easy. \n",
      "\n",
      "Or is there some way to group the variables together, so that its individual tests for all the months of precip together etc. \n",
      "\n",
      "Here is from the manual of BayesEnv2: \n",
      "\n",
      "ENVIRONFILE is  the  file  of  environmental  variables  that  you  wish to estimate bayes factors for.  Each environmental variable should be standardized, i.e.  subtract the mean and then divided through by the standard deviation of the variable across populations.  Each line of the file is an environmental variable across populations, values should be tab separated and each line ended with a tab followed by the end of line character.  The variables should be listed in the same population order as they appear in the allele count files.\n",
      "\n",
      "NUMENVIRON is the number of environmental variables in the ENVIRONFILE.\n",
      "\n",
      "\n",
      "So yeah you can use a couple different variables at a time. \n",
      "\n",
      "But for the Bayenv2, it really seems like the program can only take one environmental variable at a time. So i would have to do all the months individually, and the results would be hard to directly compare to the other program because they would be asking differnt questions. \n",
      "\n",
      "Same with LFMM, it can only do one covariable at a time: \n",
      "\n",
      "D variable_number is the number of covariable in your variable file.  Warning:  If you set several covari-\n",
      "ables, the program will be launched for each covariable sequentially and independently\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#choosing the environmental variables to use"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "      $$ October 5 2015\n",
      "==================================================================\n",
      "\n",
      "I read the Hecht paper where he uses the worldclim data as the environmental variables. and its not what i had initially thought, I was under the impresion that he used the raw temp and precip but he used the bioclim data. \n",
      "\n",
      "Which might make sense because its quicker- it already reduces the number of tests because it is a single variable. \n",
      "\n",
      " Ryan recommends using a dimmension reduction method to reduce the number of environmental variables\n",
      " \n",
      " and hecht says that he used a pairwise correlation between climate variables using the hmisc package in R. \n",
      "\n",
      "Lisa said that I need to look at a way to reduce the number of variables and that some kindof multivariate analysis would be a good way to look at it. So I'll have a meeting with Ryan and her and Wes to talk about it. \n",
      " \n",
      " So I need to get the rest of the variables out of ArcGIS \n",
      "Standardize them\n",
      "Do some sort of PCA on them.. (are they standardized first? ) YES\n",
      " There are a couple different options following these links:  \n",
      "\n",
      "\thttp://gastonsanchez.com/blog/how-to/2012/06/17/PCA-in-R.html\n",
      "\thttp://www.r-bloggers.com/computing-and-visualizing-pca-in-r/\n",
      "\n",
      "\n",
      "The ARCGis has crashed twice trying to open the bio data. \n",
      "I got it to work and did the same thing as before to export the data. I checked to make sure that I had all 7 pops (Though I just trusted that the lat and long for the amur river is the correct one) and I checked that I had all the variables. \n",
      "\n",
      "Used the same R code from last week and changed the name of the file. I used the just the data, not the names of the pops or the names of the variables, thats the only thing that worked last time and theres no reason to change it now. \n",
      "\n",
      "POPs <- read.table (\"G:/Analysis/Pop_analysis/Populations_b3_may/POPs_data.txt\", header = FALSE, sep = \"\\t\")\n",
      "POPs <- as.data.frame(POPs) \n",
      "colsPOPs <- c(names(POPs))\n",
      "POPs.dat <- scale(POPs)\n",
      "POPs.dat\n",
      "write.table(POPs.dat, file = \"POPs.txt\", sep =\"\\t\", row.names = TRUE, col.names = TRUE)\n",
      "\n",
      "But looking at the numbers that came out i can tell that i did it wrong, theyre all being standardized by population not by category, so the one variable that has the numbers in the thousands is the only positive value in the whole sheet, the rest are negatives. \n",
      "\n",
      "I have to turn the spreasheet around (or figure out how to scale by row, which seems harder)\n",
      "\n",
      "Redid it and then changed the Tmax_11 cell that has the scintific notation again to be zero, see October 2nd \n",
      "\n",
      "Obviously and thankfully the lat and long are the same standardized here as they were before, and I think its a good sign that the Tmax_11 was the same wierd scientific notation as it was the first time i did this. At least the results are reproduceable \n",
      "\n",
      "\n",
      "I copied over the results from the standardization to a txt file and called it Env_data_4_pca.txt and Ill try to read it back into R as table to do the PCA on .\n",
      "It keeps telling me that the first column, the pops names are not qualitative, and Im not sure how to fix that. I mean, I could delete them, sure but then how would I know what populations I was dealing with? \n",
      "I want to know how to tell it that I have row names in addition to having column names. \n",
      "\n",
      "Ok, by the time that I got to here, I could tell that I was turned around again. The rows and the columns need to be switched, its doing the PCA on the wrong set of values. \n",
      "\n",
      " > head(pca_env$rotation)\n",
      "                   PC1         PC2         PC3         PC4         PC5          PC6        PC7\n",
      "stand_long  0.05019818 -0.14139690 -0.16087152 -0.05006583 -0.44509727 -0.340636744 -0.2732967\n",
      "stand_lat   0.10967796  0.15318591  0.10007213  0.10599085  0.13300582  0.014892905 -0.1424337\n",
      "alt        -0.03575949  0.21838255 -0.12193735  0.14334422  0.09905242 -0.149498902 -0.7746927\n",
      "bio_1      -0.15350523 -0.02654621  0.03226201 -0.03721175  0.01772139 -0.010145125 -0.1583146\n",
      "bio_10     -0.10465489 -0.17211230 -0.09764745  0.05669732  0.04390925 -0.017648578  0.1888004\n",
      "bio_11     -0.14467178  0.04520389  0.09533950 -0.09840885  0.01186615 -0.002086625 -0.2113691\n",
      "\n",
      "I also realized that I had been standardizing the longitude incorrectly, because of the fact that longitude goes to 180 then starts over. \n",
      "So I need to put them all on the same scale (0 to 180) then standardize them. Obviously the positive would be the easiest\n",
      "\n",
      "Kushiro\t144.378062\n",
      "Amur\t139.727554\n",
      "Tauy\t148.929489\n",
      "Haylyluya\t162.485706\n",
      "Nome\t-165.3017\n",
      "Koppen\t-145.8987\n",
      "Snohomish\t-122.181078\n",
      "\n",
      "#putting the longitude on the same scale\t\t\t\n",
      "\t\t\t\n",
      "the midpoint is 180, so its going to be like taking the negative values and putting them on the positive scale. \t\t\t\n",
      "subtract the absolute value of the negative ones from 180 and then add that to 180 and then keep it positive\t\t\t\t\n",
      "\t\t\t\n",
      "Haylyluya\t162.485706\t\t(the most east of the asian samples, for comparison)\n",
      "\n",
      "name\tlong\t180-abs value long \t180+previous\n",
      "Nome\t-165.3017 \t14.6983\t   194.6983\n",
      "Koppen\t-145.8987\t34.1013\t   214.1013\n",
      "Snohomish\t-122.181078\t57.8189\t   237.8189\n",
      "\n",
      "SO Ill put this new scale for the longitude into R to be standardized: \n",
      "\n",
      "pops\tX\tY\n",
      "Kushiro\t144.378062\t42.980725\n",
      "Amur\t139.727554\t52.938669\n",
      "Tauy\t148.929489\t59.714924\n",
      "Haylyluya\t162.485706\t57.769904\n",
      "Nome\t194.6983\t64.4836\n",
      "Koppen\t214.1013\t60.7062\n",
      "Snohomish\t237.8189\t48.014461\n",
      "\n",
      "OK REDID the standardization. I need to update all the files that go along with it.\n",
      "\n",
      "UPDATED The popINFO excel sheet and the three txt files in the BayeSCENV2 file, luckily that batchfile hasnt gotten to the longitude yet and I hope it just goes a head and uses the new and updated longitudes without a problem.\n",
      "\n",
      "The next thing im trying to figure out is how to do these PCAS, like im not even sure how the files are supposed to be oriented, with the names of the populations on the top, as the column names or on the side as the row names. \n",
      "I made both sets of files one called _VERT and one called _HORIZ and im messing with those and the code that Ive been using. They basically all get resutls, but i havent got a clue whether theyre right, or like which one of them is the one that I want\n",
      "\n",
      "I think the key lies in the way that the file is made that they use in the tutorial that Im following. This is the link with the code that im following http://gastonsanchez.com/blog/how-to/2012/06/17/PCA-in-R.html and this is the data set that they use: USArrests https://stat.ethz.ch/R-manual/R-patched/library/datasets/html/USArrests.html\n",
      "\n",
      "Format:\n",
      "\n",
      "A data frame with 50 observations on 4 variables.\n",
      "[,1] \tMurder \tnumeric \tMurder arrests (per 100,000)\n",
      "[,2] \tAssault \tnumeric \tAssault arrests (per 100,000)\n",
      "[,3] \tUrbanPop \tnumeric \tPercent urban population\n",
      "[,4] \tRape \tnumeric \tRape arrests (per 100,000) \n",
      "\n",
      "\n",
      "\n",
      "### PCA of Environmental Data\n",
      "###   choosing uncorrelated environmental variables to continue analysis\n",
      "### Carolyn Tarpey | October 2015 \n",
      "### ---------------------------------------\n",
      "\n",
      "\n",
      "install.packages(\"FactoMineR\")\n",
      "library(FactoMineR)\n",
      "\n",
      "env_data <- read.table (\"G:/Analysis/Pop_analysis/Populations_b3_may/Env_data_4_pca.txt\", header = TRUE, sep = \"\\t\")\n",
      "env_data  <- as.data.frame(env_data )  \n",
      "\n",
      "names(env_data)\n",
      "View(env_data)\n",
      "\n",
      "\n",
      "pca_env <- prcomp(env_data, scale. = TRUE)\n",
      "\n",
      "pca_env$sdev\n",
      "head(pca_env$rotation)\n",
      "head(pca_env$x)\n",
      "\n",
      "write.table(pca_env$rotation, file =\"G:/Analysis/Pop_analysis/Populations_b3_may/Env_data_pca_rotation.txt\", sep =\"\\t\", row.names = TRUE, col.names = TRUE )\n",
      "write.table(pca_env$x, file =\"G:/Analysis/Pop_analysis/Populations_b3_may/Env_data_pca_scores.txt\", sep =\"\\t\", row.names = TRUE, col.names = TRUE )\n",
      "\n",
      "###USING ANOTHER METHOD\n",
      "pcaFM <- PCA(env_data, graph = FALSE)\n",
      "\n",
      "#matrix with eigenvalues\n",
      "pcaFM$eig\n",
      "\n",
      "\n",
      "\n",
      "#############################################\n",
      "env_data_H <- read.table (\"G:/Analysis/Pop_analysis/Populations_b3_may/Env_data_4_pca_HORZ.txt\", header = TRUE, sep = \"\\t\")\n",
      "env_data_H <- as.data.frame(env_data_H )  \n",
      "\n",
      "names(env_data_H)\n",
      "\n",
      "pca_env_H <- prcomp(env_data_H, scale. = TRUE)\n",
      "\n",
      "pca_env_H$sdev\n",
      "head(pca_env_H$rotation)\n",
      "head(pca_env_H$x)\n",
      "\n",
      "write.table(pca_env_H$rotation, file =\"G:/Analysis/Pop_analysis/Populations_b3_may/Env_data_pca_rotation_H.txt\", sep =\"\\t\", row.names = TRUE, col.names = TRUE )\n",
      "write.table(pca_env_H$x, file =\"G:/Analysis/Pop_analysis/Populations_b3_may/Env_data_pca_scores_H.txt\", sep =\"\\t\", row.names = TRUE, col.names = TRUE )\n",
      "\n",
      "###USING ANOTHER METHOD\n",
      "pcaFM <- PCA(env_data, graph = FALSE)\n",
      "\n",
      "#matrix with eigenvalues\n",
      "pcaFM$eig\n",
      "env_data_V <- read.table (\"G:/Analysis/Pop_analysis/Populations_b3_may/Env_data_4_pca_VERT.txt\", header = TRUE, sep = \"\\t\")\n",
      "env_data_V  <- as.data.frame(env_data_V )  \n",
      "\n",
      "names(env_data_V)\n",
      "View(env_data_V)\n",
      "pca_env_V <- prcomp(env_data_V, scale. = TRUE)\n",
      "\n",
      "pca_env_V$sdev\n",
      "head(pca_env_V$rotation)\n",
      "head(pca_env_V$x)\n",
      "\n",
      "write.table(pca_env$rotation, file =\"G:/Analysis/Pop_analysis/Populations_b3_may/Env_data_pca_rotation_V.txt\", sep =\"\\t\", row.names = TRUE, col.names = TRUE )\n",
      "write.table(pca_env$x, file =\"G:/Analysis/Pop_analysis/Populations_b3_may/Env_data_pca_scores_V.txt\", sep =\"\\t\", row.names = TRUE, col.names = TRUE )\n",
      "\n",
      "###USING ANOTHER METHOD\n",
      "pcaFM_V <- PCA(env_data_V, graph = FALSE)\n",
      "\n",
      "#matrix with eigenvalues\n",
      "pcaFM_V$eig\n",
      "\n",
      "###USING ANOTHER METHOD\n",
      "pcaFM_H <- PCA(env_data_H, graph = FALSE)\n",
      "\n",
      "#matrix with eigenvalues\n",
      "pcaFM_H$eig\n",
      "\n",
      "\n",
      "\n",
      "         $$ October 6 2015\n",
      "==================================================================\n",
      "\n",
      "Looked more at the PCA for the environmental variables. \n",
      "\n",
      "The data need to be in horizontal fashion to work, so figured that out. Plus i plotted the PCAs and got the correlations.  Its pretty much what we would expect, it looks like they are in a continuous variable state of correlationwith one another, the latitude is the onlyt thing off by itself in one quadrant and there are a couple bioclim variables off on their own in another quadrant then half the plot is the spread of precip, temp and the other bioclim one after the otheer. \n",
      "\n",
      "Ryan said to plot them witout the bioclims because theyre double weighting some of the categories, like if the max summer temp was used in a lot of the bioclim variables then it is gettinga lot of weight in the pca ifyou include those bioclims. \n",
      "\n",
      "And wes had me plot the pca and the correlations one on top of the other so you could see where the populations are in environmental space. and he said that he thought i should use the pca axis amounts as the variables. So like a PCA axis for temp, one for precip and then the longitude and latitude. \n",
      "\n",
      "Im not sure what the pca axis are for the long and the lat, but I already ran the BAYESCENV runs with them just standardized. so. also im not sure if they should be on the PCA or if they are just going to confuse things. \n",
      "\n",
      "I think they will confuse things Ill take them out. \n",
      "\n",
      "I did the temp and it looks like i thought it would, or actually i thought that it would spread out more around the circle, but it didnt but the summer and the winter months are correlated. \n",
      "\n",
      "Ok, I finished PRecip, Bioclim, nobioclim, temp and all the variables. and the no bioclim also has the stand lat and long, it is just no bioclim and one that is just the mean temp, called Tmean\n",
      "\n",
      "i talked to ryan about what it was that i was actually doing because I was really confused. I plotted the scree plot for the full set of all the variables and i didnt understand why there were seven boxes. I was worried that i was doing my PCA on the wrong variables, like they had been switched. But he said that that was right. that there are actually as many axis as there are variables but they are so small they arent reported. The axis plots shows the weighted linear conbination of the variables the percent of the difference (?) explained by the PC axises. so the scree plot and the rotations table are showing different but similar things. the rotations table has the PC axises at the top and the percent contribution of each of the variables to it. so if you sort by one of the PC axises, you can see which of the variables contributes to it the most. And they should all contribute to all the axese. So one way to do it is to make a PCA with the variables that you are interested in, making sure not to overweight one or the other of the categories too much like for instance whether to keep all three of the categories of temperature but only one set of percipitation means that the temp gets a lot of contribution to the PC axis, then do a plot that has the PC axis 1 plotted against PC axis 2 and those are the coordinates that you put into the program. \n",
      "\n",
      "So you kindof hope that it comes out in some biologically significant way, where pc1 is basically all explained by temperature and PC2 is explained by percipitation etc. the bummer is if it isnt and they are all mixed up and its just like \"theyre different places and there isnt one variable that explains the differences\" in that case, they become like coordinates for the locations in some environmental space and latitude might just be the best indication of how the are differnt. \n",
      "\n",
      "\n",
      "Made two new sheets, one is mix no bio (has precip and temp mean data no biol data) and the other is mix (has mean temp, precip and biol data)\n",
      "\n",
      "they are pretty interesting \n",
      "\n",
      "         $$ October 7 2015\n",
      "==================================================================\n",
      "\n",
      "the saga continues\n",
      "\n",
      "\n",
      "moving on to picking environmental variables and the best way of doing that, what are the assumptions when you chose the PC scores intead of the raw data? \n",
      "its helpful because they are on the same scale\n",
      "but the results are more difficult to enterpret, as the biological significance is not as apparent\n",
      "it is helpful to uncorrelate the factors for the analysis, and pare down the number of factors\n",
      "\n",
      "look at another source of environmental variables: the climate in the past especially modeled from the LGM so that it can be investigated how the populations were impacted by the past environment\n",
      "\n",
      "why do different outlier tests yeild different outliers? That is common. theyre different because they are sensitive to different things and there are parts of the demographics etc that might be influencing the different tests. \n",
      "\n",
      "Bayenv looks at 3 different models, a neutral one  a locus specific (this is outliers with the population structure, not the environment, just individual loci) and outliers from the envirnment. \n",
      "\n",
      "Garret thinks its important to use the min or max temp, not the mean because the range of species is dictated by the min and the max, so that is something that would be biologically significant. \n",
      "\n",
      "precipitation seems to be grouping the populations in a way that is different than the latitude and longitude so taking PC score from that pca seems like a good idea\n",
      "the same with the temperature. \n",
      "People seem to say look at the percent variation explained by the axes (probably in that summary() function) and use the first and second PC scores from that. \n",
      "\n",
      "So what to do the vector lengths mean? if theyre all the same length it seems to be saying that they are contributing primarily to the first and second axises. \n",
      "\n",
      "MEETING WITH DANIEL SCHINDLER\n",
      "he said to try to get something like river integrated temperature or total discharge at the mouth of the rivers for some flow data. then doa regresssion agaisnt them, but keep the PCSs clean dont include any of the derivatives in them, like the biocl data. \n",
      "\n",
      "try to do some of the calculations on the cloud, its way faster and really cheap if you know what you need it for\n",
      "\n",
      "only use axis that are significant and keep the lat and long in there. \n",
      "\n",
      "Some of the bioclims are interesting and seem important or useful. but dont mix them up in the PCA of the raw data. and a lot of them are not useful\n",
      "\n",
      "so taking a step back and looking at the project, thing about what you are asking. is geography influencing the differnces between the populations or is environment or is it a little bit of both. that is three hypotheses. so does the environment covary with one another (environmental factors) or with the location? \n",
      "\n",
      "you want to be able to identify the differences in environment independent of location. \n",
      "\n",
      "do a PCA of latand long / temperature and Precip all together on the same one \n",
      "\n",
      "look again at the plots with the percent of the variation explained by each axis. and try to find RIVER FLOW \n",
      "\n",
      "do 2 pcas. one that is just the temperature, one of the precipitation and one for both of them together. and look at that. and hopefully what you want is that plot that has them both on it and if the temperature and the precipitation are being pulled out on PC1 and PC2 then go with those, and if they are covarying, then use PC1 on the plot that has them both together. \n",
      "\n",
      "look at the plot with the lat and long on it and look at one without it just to make sure that you have what you think or that the patterns are OK. \n",
      "\n",
      "\n",
      "OK its hard to find, but i got some source for the annual discharge in the AMur and the Tauy river and the USGS has one on the Snohomish river: http://waterdata.usgs.gov/wa/nwis/uv?site_no=12150800\n",
      "\n",
      "I emailed tyler to see if he knows anyone or a good way to get the koppen creek and the Nome river and Lisa is looking into the kushiro and the Haylyluya \n",
      "\n",
      "I made the 6 txt files to make new PCAs out of: \n",
      "temp with lat and long\n",
      "temp without lat long\n",
      "precip with lat long \n",
      "precip without lat long\n",
      "temp and precip with lat long\n",
      "temp and precip without lat long\n",
      "\n",
      "Here are some of the results of the plots: \n",
      "\n",
      "> summary(pca_Precip)\n",
      "Importance of components:\n",
      "                          PC1    PC2     PC3     PC4     PC5     PC6       PC7\n",
      "Standard deviation     3.0198 1.5250 0.69229 0.23612 0.12805 0.06093 7.467e-17\n",
      "Proportion of Variance 0.7599 0.1938 0.03994 0.00465 0.00137 0.00031 0.000e+00\n",
      "Cumulative Proportion  0.7599 0.9537 0.99368 0.99832 0.99969 1.00000 1.000e+00\n",
      "\n",
      "> summary(pca_Precip_lat)\n",
      "Importance of components:\n",
      "                          PC1    PC2    PC3     PC4    PC5     PC6       PC7\n",
      "Standard deviation     3.0682 1.7081 1.1947 0.42531 0.1981 0.14576 1.513e-16\n",
      "Proportion of Variance 0.6724 0.2084 0.1019 0.01292 0.0028 0.00152 0.000e+00\n",
      "Cumulative Proportion  0.6724 0.8808 0.9828 0.99568 0.9985 1.00000 1.000e+00\n",
      "\n",
      "> summary(pca_Temp)\n",
      "Importance of components:\n",
      "                          PC1    PC2     PC3     PC4     PC5     PC6       PC7\n",
      "Standard deviation     3.0786 1.4582 0.55662 0.22668 0.15682 0.09934 1.056e-16\n",
      "Proportion of Variance 0.7898 0.1772 0.02582 0.00428 0.00205 0.00082 0.000e+00\n",
      "Cumulative Proportion  0.7898 0.9670 0.99285 0.99713 0.99918 1.00000 1.000e+00\n",
      "\n",
      "> summary(pca_Temp_lat)\n",
      "Importance of components:\n",
      "                          PC1    PC2     PC3     PC4     PC5     PC6       PC7\n",
      "Standard deviation     3.2189 1.7210 0.75862 0.25733 0.15682 0.10230 1.326e-16\n",
      "Proportion of Variance 0.7401 0.2115 0.04111 0.00473 0.00176 0.00075 0.000e+00\n",
      "Cumulative Proportion  0.7401 0.9517 0.99277 0.99750 0.99925 1.00000 1.000e+00\n",
      "\n",
      "> summary(pca_Precip_temp)\n",
      "Importance of components:\n",
      "                          PC1    PC2    PC3     PC4     PC5    PC6      PC7\n",
      "Standard deviation     3.7575 2.4481 1.6895 0.90505 0.36476 0.2855 3.63e-16\n",
      "Proportion of Variance 0.5883 0.2497 0.1189 0.03413 0.00554 0.0034 0.00e+00\n",
      "Cumulative Proportion  0.5883 0.8380 0.9569 0.99106 0.99660 1.0000 1.00e+00\n",
      "\n",
      "> summary(pca_Precip_temp_lat)\n",
      "Importance of components:\n",
      "                          PC1    PC2    PC3     PC4     PC5    PC6       PC7\n",
      "Standard deviation     3.8512 2.5215 1.9106 0.91263 0.49393 0.2886 1.853e-16\n",
      "Proportion of Variance 0.5705 0.2445 0.1404 0.03203 0.00938 0.0032 0.000e+00\n",
      "Cumulative Proportion  0.5705 0.8150 0.9554 0.98741 0.99680 1.0000 1.000e+00\n",
      "\n",
      "The thing that i dont know is whether the temperature explains one axis in the precip temp lat and the the precip explains the other? So i think i need to figure out a better way to plot them that isnt the biplot, that one doesnt really make any sense to me. \n",
      "\n",
      "\n",
      "         $$ October 8 2015\n",
      "==================================================================\n",
      "\n",
      "What is the difference between loadings and scores? \n",
      "\n",
      "im having a hard time gettting the components, the actual environmental variables to plot in a way that makes sense to me. I remember someone telling me that the loadings are how much each of the variables contributes to the pc axis. \n",
      "\n",
      "\n",
      "From here: \n",
      "\n",
      "http://cosmic.mse.iastate.edu/library/pdf/pcainterpretation.pdf \n",
      "\n",
      "the loadings plot will show the relationship between the categories, for me the locations (it is explained by the environmental variables) [column vectors, a summary fo the variable (properties) a means to interpret the patterns seen in the score plot]\n",
      "\n",
      "and on the score plot you can identify the relationship between the samples, or the environmental variables in my case, it is explained by the differences in the locations [row vectors, a summary of the relationship among the obervations or samples]\n",
      "\n",
      "i just dont understand why the plot doestn have the PC axis 1 with the most spread? like if PC axis 1 explains the greatest amount of variation in the data, why isnt it the one that actually shows the spread the most on the plot?\n",
      "\n",
      "anyway, i have plotted these so many times now that I assume that they are correct if they never change? \n",
      "\n",
      "ok so practically speaking, what are the numbers that Im going to use? \n",
      "\n",
      "The coordinates of the cites in space for just the locations, so that would be the plot that has the names of the populations. I would either do PC1 of precip and PC1 of temp or I would do PC 1 and PC2 of temp and precip together, if it seems like temp pulls them apart on one and precip on the other. \n",
      "\n",
      "\n",
      "\n",
      "Still looking for that other environmental variables. \n",
      "\n",
      "went and talked a bunch of people about the PCA, and from what I can tell, there were some consistent ideas about what the best way to progress is. \n",
      "\n",
      "I talked to vincent and xingli in the olden lab. Vincent thought that I should continue using the temperature and precipitation together because it looks to him like the temperature scale hot/cold and the precipitation scale high/low were spread out on PC1 and the temperature and precipitation were separated out on PC2. \n",
      "Then there was something else some sort of seasonality that is pc3\n",
      "\n",
      "Xingli was concerned with the fact that I was using temporal data, he thought that I should find something that was not temporal, instead i should use the mean average or something like that. At the time i understood what he was saying. but then I went and talked to Adrienne and Thomas in Daniels lab and Im not sure that I remember it all correctly. \n",
      "\n",
      "Adrinne and Thomas thought bascially the same thing, that I need to use something that isnt correlated to itself the way the monthly data is. They thought I should do something taht was a summary of the ranges of the temperatures or the precipitation. So something like summer and fall averages. They thought I should pick the means in a way that I plotted them out against the latitude to make sure that they were the gradient that I wanted before using them, because the latitudinal cline seems to be so important. \n",
      "\n",
      "They were in favor of derivatives, so they would have been all over those bioclims intstead of the actual data, whereas Daniel was the one that was like no dont use them both, use the raw data. he didnt have any problem with me using the monthly data. \n",
      "\n",
      "So having talked to so many people about which of the environmental variables to use, Im still at a loss. I thought I was so close! I thought that I could use the \n",
      "\n",
      "They also said that I should look up other people who do this, So i did some googling. \n",
      "\n",
      "This was a post that helped me, \n",
      "http://stats.stackexchange.com/questions/50537/should-one-remove-highly-correlated-variables-before-doing-pca\n",
      "\n",
      "It makes me think that though I do have correlated data, (the months plot like right on eachother in a spectrum) They are balanced because they are in a spectrum and there are the same number for the temperature and the precipitation. \n",
      "\n",
      "Ryan sent me a link to a blog post where the guy uses temporal data in a PCA and he doesnt say anything about how it might not be appropriate to use correlated data in that method. \n",
      "\n",
      "The blog is super cool: \n",
      "\n",
      "https://jakevdp.github.io/blog/2015/07/23/learning-seattles-work-habits-from-bicycle-counts/\n",
      "\n",
      "\n",
      "I talked to lisa about what ive found so far, and it seems like i have two clear options. \n",
      "\n",
      "One is to continue with LFMM with the PC axises, the other is to test a couple of the bioclim variables. \n",
      "\n",
      "lisa recommends that I explore the data by running LFMM on a bunch of the options at a lower rate, like fewer iterations etc. and no repetition and see what gives the same kind of signals. \n",
      "\n",
      "The bioclims that she thought would be biologically significant would be: \n",
      "\n",
      "bio1\tannual mean temperature\n",
      "bio3\tisothermality\n",
      "bio5\tmax temp of warmest month\n",
      "bio6\tmax temp of coldest month\n",
      "bio7\tannual temperature range\n",
      "bio12\tannual precipitation\n",
      "\n",
      "They seem good to me too. \n",
      "\n",
      "and so I think Im going to build the input files to run this with \n",
      "\n",
      "A. The temp and precip pca axies\n",
      "B. The temp pca axies\n",
      "C. The precip pca axies\n",
      "\n",
      "D. the bioclim pca axies\n",
      "E-J. the individual bioclim data sets. \n",
      "\n",
      "So to do that I need to get the bioclim data and standardize it in R. \n",
      "Then I have to make PCAs out of the bioclim variables that I pick, after that all the data has to go in one file, space separated. \n",
      "\n",
      "It really shouldnt be that hard to have them running overnight over at the student computer lab. The thing that is hard is the number of axes to use. so for the temp and precip, do I use both PC1 and PC2 or even PC3? And I think that I can use more than one variable for the correlation, so for those I could do individual files and have it run all the PC axes at a time. \n",
      "\n",
      "@@@@@@@@@@@@@@@@\n",
      "Standardizing the BIoclim data\n",
      "\n",
      "looking at the code in R, I can tell that I didn tdo it before, but I have a good Idea of how to do it now. I need a file that has all the raw data.\n",
      "\n",
      "I panicked that I was using the wrong lat long in this file, but i went back and they are the right ones, the new adjusted ones.\n",
      "\n",
      "pops\tX\tY\n",
      "Kushiro\t144.378062\t42.980725\n",
      "Amur\t139.727554\t52.938669\n",
      "Tauy\t148.929489\t59.714924\n",
      "Haylyluya\t162.485706\t57.769904\n",
      "Nome\t194.6983\t64.4836\n",
      "Koppen\t214.1013\t60.7062\n",
      "Snohomish\t237.8189\t48.014461\n",
      "\n",
      "ill make a sheet that has the bios on it and the lat longs and put that into R. They shouldnt have a header or any info at the beginning, like no pop name. The bioclims are in a really strange order to me, it doenst make a lot of sense: \n",
      "\n",
      "adjusted_X\tY\tbio_17\tbio_8\tbio_16\tbio_7\tbio_15\tbio_6\tbio_14\tbio_5\tbio_13\tbio_4\tbio_12\tbio_3\tbio_11\tbio_2\tbio_10\tbio_19\tbio_1\tbio_18\tbio_9\n",
      "\n",
      "\n",
      "Added this little bit: \n",
      "\n",
      "bioclim <- read.table (\"G:/Analysis/Pop_analysis/Populations_b3_may/EnvData/bioclim.txt\", header = FALSE, sep = \"\\t\")\n",
      "bioclim <- as.data.frame(bioclim) \n",
      "colsbioclim<- c(names(bioclim))\n",
      "bioclim_scaled.dat <- scale(bioclim)\n",
      "bioclim_scaled.dat\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This is what I got out after: \n",
      "\n",
      "attr(,\"scaled:center\")\n",
      "        V1         V2         V3         V4         V5         V6         V7         V8         V9        V10        V11 \n",
      " 177.44847   55.22978  119.00000   92.71429  318.85714  339.14286   42.14286 -154.85714   35.57143  184.28571  118.28571 \n",
      "       V12        V13        V14        V15        V16        V17        V18        V19        V20        V21 \n",
      "9326.42857  831.57143   25.28571 -104.57143   81.28571  131.14286  182.57143   13.00000  227.00000  -39.14286 \n",
      "attr(,\"scaled:scale\")\n",
      "         V1          V2          V3          V4          V5          V6          V7          V8          V9         V10 \n",
      "  38.382529    7.632633   90.358545   30.793475  161.489348   86.643770   12.902565   93.800193   30.060257   36.012564 \n",
      "        V11         V12         V13         V14         V15         V16         V17         V18         V19         V20 \n",
      "  61.921225 3183.774639  478.312609    8.712334   89.177458   10.435744   31.082839  151.187364   52.971691  104.067286 \n",
      "        V21 \n",
      " 127.131728 \n",
      "\n",
      "Finding all the environmental data that I need was way harder than it should have been. I need to standardize the files, get them all together, run through R at the same time, put into a definative excel sheet. They are like a mess right now. i need to delete the files that amount to tests that all have the same confusing names. i have no idea what they have in them. \n",
      "\n",
      "\n",
      "\n",
      "Ok so i made a new file with the env variables but Im not going to make the adjustments that are like multiplying the temperature by 10 because it doesnt matter if they are all going to be standardized and centered anyway. \n",
      "\n",
      "this is the header for the raw_env data sheet, and it has the populations in this order: \n",
      "\n",
      "Nome\tKoppen\tTauy\tHaylyluya\tAmur\tSnohomish\tKushiro\n",
      "\n",
      "\n",
      "\n",
      "Y\n",
      "adjusted_X\n",
      "bio_1\n",
      "bio_2\n",
      "bio_3\n",
      "bio_4\n",
      "bio_5\n",
      "bio_6\n",
      "bio_7\n",
      "bio_8\n",
      "bio_9\n",
      "bio_10\n",
      "bio_11\n",
      "bio_12\n",
      "bio_13\n",
      "bio_14\n",
      "bio_15\n",
      "bio_16\n",
      "bio_17\n",
      "bio_18\n",
      "bio_19\n",
      "prec_1\n",
      "prec_2\n",
      "prec_3\n",
      "prec_4\n",
      "prec_5\n",
      "prec_6\n",
      "prec_7\n",
      "prec_8\n",
      "prec_9\n",
      "prec_10\n",
      "prec_11\n",
      "prec_12\n",
      "tmax_1\n",
      "tmax_2\n",
      "tmax_3\n",
      "tmax_4\n",
      "tmax_5\n",
      "tmax_6\n",
      "tmax_7\n",
      "tmax_8\n",
      "tmax_9\n",
      "tmax_10\n",
      "tmax_11\n",
      "tmax_12\n",
      "tmean_1\n",
      "tmean_2\n",
      "tmean_3\n",
      "tmean_4\n",
      "tmean_5\n",
      "tmean_6\n",
      "tmean_7\n",
      "tmean_8\n",
      "tmean_9\n",
      "tmean_10\n",
      "tmean_11\n",
      "tmean_12\n",
      "tmin_1\n",
      "tmin_2\n",
      "tmin_3\n",
      "tmin_4\n",
      "tmin_5\n",
      "tmin_6\n",
      "tmin_7\n",
      "tmin_8\n",
      "tmin_9\n",
      "tmin_10\n",
      "tmin_11\n",
      "tmin_12\n",
      "\n",
      "\n",
      "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "#  O C T O B E R 1 4 \n",
      "\n",
      "all_env <- read.table (\"G:/Analysis/Pop_analysis/Populations_b3_may/EnvData/raw_env.txt\", header = FALSE, sep = \"\\t\")\n",
      "all_env <- as.data.frame(all_env) \n",
      "colsall_env <- c(names(all_env))\n",
      "all_env.dat <- scale(all_env)\n",
      "all_env.dat\n",
      "write.table(all_env.dat, file = \"G:/Analysis/Pop_analysis/Populations_b3_may/EnvData/all_env_STAND.txt\", sep =\"\\t\", row.names = TRUE, col.names = TRUE)\n",
      "\n",
      "\n",
      "^ that seems to work great. \n",
      "\n",
      "Heres a test, the lat and long were done again. \n",
      "\n",
      "\t\t\tY (lat)\t\t\tadjusted_X (long)\n",
      "Nome\t\t1.212401712\t\t0.449418714\n",
      "Koppen\t\t0.717500379\t\t0.954935164\n",
      "Tauy\t\t0.58762697\t\t-0.743019922\n",
      "Haylyluya\t0.332797461\t\t-0.38983275\n",
      "Amur\t\t-0.300173536\t-0.98276272\n",
      "Snohomish\t-0.945325519\t1.572862132\n",
      "Kushiro\t\t-1.604827467\t-0.861600617\n",
      "\n",
      "\n",
      "and that Tmin_11 was 0 instead of being some strange number in scientific notation that basically means 0\n",
      "\n",
      "\n",
      "I dont have to redo the PCAs for the variables because Im not doing a PCA of the bioclim ones, right? OR should I just add a line for the bioclim? \n",
      "\n",
      "I think thats what Ill do. A single line to make a bioclim PCA in the R code for ENV_data_PCA_INDEPTH.r\n",
      "\n",
      "The order that the other files have been made is like this: \n",
      "Kushiro\n",
      "Amur\n",
      "Tauy\n",
      "Haylyluya\n",
      "Nome\n",
      "Koppen\n",
      "Snohomish\n",
      "\n",
      "That comes from teh file called POPNAMES.txt and i'll organize the bioclim data in the same way so that the code is seamless. \n",
      "\n",
      "> summary(pca_Bio)\n",
      "Importance of components:\n",
      "                          PC1    PC2    PC3     PC4     PC5     PC6       PC7\n",
      "Standard deviation     3.3186 1.8214 1.6350 1.11988 0.74651 0.42954 2.679e-16\n",
      "Proportion of Variance 0.5796 0.1746 0.1407 0.06601 0.02933 0.00971 0.000e+00\n",
      "Cumulative Proportion  0.5796 0.7543 0.8950 0.96096 0.99029 1.00000 1.000e+00\n",
      "\n",
      "\n",
      "> summary(pca_Bio_lat)\n",
      "Importance of components:\n",
      "                          PC1    PC2    PC3     PC4     PC5     PC6       PC7\n",
      "Standard deviation     3.4173 1.9320 1.8469 1.13763 0.83523 0.43156 2.708e-16\n",
      "Proportion of Variance 0.5561 0.1777 0.1624 0.06163 0.03322 0.00887 0.000e+00\n",
      "Cumulative Proportion  0.5561 0.7339 0.8963 0.95791 0.99113 1.00000 1.000e+00\n",
      "\n",
      "\n",
      "It looks like there are several axies that describe the variation in the data the best, and that isnt really helpful to this analysis, like what would i say about the biological significance of the first and second axies? \n",
      "\n",
      "so I will continue with making the files to be run in LFMM. \n",
      "\n",
      "I need the scores, those are the values for each of the PC axis for each populatin. They dont seem standardized which means I have to standardize them. which is crazy. but whatever. \n",
      "\n",
      "\n",
      "this is what it loooks like. I added the code to the environmental standardizing r script\n",
      "\n",
      "> PCAs.dat\n",
      "             V1         V2          V3           V4         V5          V6         V7\n",
      "[1,] -0.9837965 -0.3907642 -1.72092492  0.209268628 -0.0372709  1.00774002 -1.3492169\n",
      "[2,]  0.1224710 -1.8663546  0.86087707 -0.203790586 -0.9281111  0.02341817 -1.0545395\n",
      "[3,]  0.9126787 -0.3938146  0.16012564  1.010281838  1.7630088  0.01658309  1.4745987\n",
      "[4,]  0.5766173  0.3293527 -0.96992873  0.008707602 -0.6106990 -1.84068076  0.3870481\n",
      "[5,]  0.9164467  0.6472152  0.05120317 -1.803956308  0.2058651  0.76474465  0.4167278\n",
      "[6,]  0.1815180  1.2112305  0.71373609  1.207017638 -1.0729076  0.72483617 -0.4673045\n",
      "[7,] -1.7259351  0.4631349  0.90491168 -0.427528812  0.6801147 -0.69664134  0.5926863\n",
      "             V8         V9           V10        V11         V12        V13        V14\n",
      "[1,] -0.6266320 -1.2999124  1.5452061070 -0.7132634  0.38365653 -0.1296852 -0.3389996\n",
      "[2,]  0.5091641 -0.4303154  0.1001546933  0.8297936 -1.91554818 -0.5749126  0.6882464\n",
      "[3,]  0.8614745 -0.3297328  0.0002979379  1.4896404  1.12686291  0.8961843  0.3690828\n",
      "[4,]  0.5145107  0.4734663 -0.4670435519 -0.3048648  0.88731242 -1.8856219 -0.8647969\n",
      "[5,]  0.9146394 -0.2003974 -0.9098621839 -1.5384070 -0.27769123  0.9972337 -0.8647969\n",
      "[6,] -1.8426717 -0.1447044 -1.2569213944  0.3375508  0.01665342  0.1800267  1.7839316\n",
      "[7,] -0.3304852  1.9315962  0.9881683919 -0.1004497 -0.22124587  0.5167749 -0.7726672\n",
      "            V15         V16         V17        V18        V19         V20          V21\n",
      "[1,] -0.8565349 -0.21094173 -1.55746998 -1.3802389  0.1823970 -0.02584441  1.094645627\n",
      "[2,]  0.4495426 -0.60194710 -1.09954382  1.6206427 -0.5082030 -0.69618867  0.295737049\n",
      "[3,]  1.0350400 -0.02033010 -0.09226546  0.1506385  0.7537834  1.86326106  0.245715940\n",
      "[4,]  0.5868730  0.04122948  0.69325091 -0.3019248  1.5474831 -1.35287764  0.531550851\n",
      "[5,]  1.0276598  0.15290479  0.62538535 -0.9576566 -1.6397862 -0.25745082 -0.006819204\n",
      "[6,] -0.9239790  1.95714052  0.23485071  0.6079376 -0.1549892  0.09978162 -2.089555286\n",
      "[7,] -1.3186016 -1.31805585  1.19579229  0.2606014 -0.1806852  0.36931886 -0.071274977\n",
      "            V22         V23        V24        V25        V26         V27         V28\n",
      "[1,] -0.3800799 -0.40549154 -1.4171237 -1.2768700  0.8599436  0.67515438 -0.09419389\n",
      "[2,]  0.7935279  0.25871674 -1.4188080  1.2542616 -0.9220985 -0.09890505 -0.24040887\n",
      "[3,]  1.1209222 -0.23818402  0.4416887 -0.8794097  0.1324238 -1.68631924  1.47636619\n",
      "[4,]  0.2910801  0.08145417  0.6729609  1.1886672  1.7431542  0.38353869  0.72027355\n",
      "[5,]  0.7496010  0.03487419  1.0419980 -0.5733570 -0.9594656  1.49822123 -1.78150357\n",
      "[6,] -1.3479923 -1.54913042  0.4242161  0.4899810 -0.6126029 -0.36144954  0.12985415\n",
      "[7,] -1.2270589  1.81776088  0.2550680 -0.2032731 -0.2413546 -0.41024046 -0.21038754\n",
      "\n",
      "attr(,\"scaled:center\")\n",
      "           V1            V2            V3            V4            V5            V6 \n",
      " 8.723181e-17  7.930164e-18 -7.434529e-18 -1.548860e-18 -1.428571e-10  1.548860e-18 \n",
      "           V7            V8            V9           V10           V11           V12 \n",
      " 5.342857e-17  1.428571e-10 -1.428571e-10  1.428572e-10 -1.428571e-10 -8.673617e-19 \n",
      "          V13           V14           V15           V16           V17           V18 \n",
      "-1.428571e-10  1.282857e-17 -1.428570e-10 -1.428572e-10  1.428571e-10  0.000000e+00 \n",
      "          V19           V20           V21           V22           V23           V24 \n",
      "-2.857143e-10  1.428571e-10  2.207143e-17  1.428571e-10  3.568574e-17 -7.930164e-18 \n",
      "          V25           V26           V27           V28 \n",
      "-1.428572e-10 -2.379049e-17 -9.912706e-18  2.221429e-17 \n",
      "\n",
      "attr(,\"scaled:scale\")\n",
      "          V1           V2           V3           V4           V5           V6           V7 \n",
      "3.078642e+00 1.458180e+00 5.566191e-01 2.266817e-01 1.568198e-01 9.934434e-02 4.717022e-16 \n",
      "          V8           V9          V10          V11          V12          V13          V14 \n",
      "3.019788e+00 1.525042e+00 6.922880e-01 2.361249e-01 1.280521e-01 6.093302e-02 3.039194e-16 \n",
      "         V15          V16          V17          V18          V19          V20          V21 \n",
      "3.757525e+00 2.448061e+00 1.689483e+00 9.050458e-01 3.647603e-01 2.854717e-01 6.997046e-16 \n",
      "         V22          V23          V24          V25          V26          V27          V28 \n",
      "3.318648e+00 1.821416e+00 1.634967e+00 1.119875e+00 7.465085e-01 4.295413e-01 8.993607e-16 \n",
      "\n",
      "\n",
      "this is what the header is: \n",
      "\tPC1_temp\tPC2_temp\tPC3_temp\tPC4_temp\tPC5_temp\tPC6_temp\tPC7_temp\tPC1_precip\tPC2_precip\tPC3_precip\tPC4_precip\tPC5_precip\tPC6_precip\tPC7_precip\tPC1_temp_precip\tPC2_temp_precip\tPC3_temp_precip\tPC4_temp_precip\tPC5_temp_precip\tPC6_temp_precip\tPC7_temp_precip\tPC1_bio\tPC2_bio\tPC3_bio\tPC4_bio\tPC5_bio\tPC6_bio\tPC7_bio\n",
      "\n",
      "\t\n",
      "to the excel sheet that has all this compiled im adding the amount of variation that each of the PC axies explains. Thats the excel file PCA summary\n",
      "\n",
      "\n",
      "trying to set this up for bioclim, im very confused about whether im running all the populations together at the same time. Like is this comparing all the lineages and all the coasts and all the variables, or is it like there should be a more specific run that just looks within the even lineage in asia sort of thing? \n",
      "\n",
      "I guess in figuring out if the program works I dont have to do that right now. But there are going to be a lot of runs on here. \n",
      "\n",
      "I also dont understand how it saves the projects. Am I coming up with a \n",
      "\n",
      "\n",
      "UGH just realized i included all the bioclims in the PCA!!! \n",
      "\n",
      "\n",
      "I have to go back and retrace my steps tomorrow to remake the PCA for the bioclim with just the bioclims that Im interested in. \n",
      "\n",
      "\n",
      "October 15\n",
      " \n",
      " notes on meeting with Julian Olden\n",
      "\n",
      "the first axis shows the temperature precipitation gradient positive\n",
      "the second is the residuals that are differnce between the low and high and the high and low, and that is really cool. make sure that you test the significance of the pc axies, so that you know that the second is actually significant, though it really should be\n",
      "\n",
      "the angles of the arrows (or actually the cosine of the angles of the arrows) shows the correlation of the variables to eachother so things that are 90 degrees apart from one another are not correlated. these variables look good because they have a spread that shows that they really run the range of correlation. \n",
      "\n",
      "so overall, use the one that has teh precipitation and temperature together because if you do the ones seperately and use PC axis 1 for each then you are going to get the same results for each of the runs and then you wont be able to tell what is going on. but being able to incorporate pc axis two into it on the pca of both the variables together will allow you to capture that variation of the other things that its showing. \n",
      "\n",
      "and the populations on pca are not spread in a weird shape so thats a good sign. if they are in a wedge or a line or somethign that can be a bad sign, but they arent. \n",
      "\n",
      "\n",
      "\t\n",
      "The issue now is that the results are really hard to understand. I cant really figure out if the \"all \" varibles worked, or if something got overwritten? \n",
      "\n",
      "Ok, so it will make a new file/program if the environmental variable file has a different name, so im going to rename the environmental variable file to _all at the end and then restart both of them the ones that look at each variable independently and the one that looks at all of them together, with repetition of 5, like the manual recommends. \n",
      "\n",
      "I typed up the post production code from the manual so that i can run that on monday after they hopefully finish without errors/ \n",
      "\n",
      " Now that I'm looking at the bayescan files I realize that I should be thinking about the actual tests that Im doing in LFMM. Like I should be running the individual odd/even seperately, and each country seperately. The same way that the tests have been set up in the other runs. \n",
      " \n",
      " \n",
      " SO I think im going to make those files for LFMM conversion and have the program convert them and then Ill add them to the things that need to be run. I can also run the bayesenv on another computer over there If need be, so that I have several running and am not limited in the computing power. \n",
      " \n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}