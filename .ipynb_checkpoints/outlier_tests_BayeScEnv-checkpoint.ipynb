{
 "metadata": {
  "name": "",
  "signature": "sha256:f141bfa7f547e2b054d1103feea11354701a1a2dd21fabdb829e2805da975ea7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Outlier tests\n",
      "\n",
      "## Bayescan "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Bayescan"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "July 27\n",
      "\n",
      "I started looking into bayescan, downloaded it and am trying to compile it now. after flailing about, i realized it probably needs to be installed on the linux side... \n",
      "\n",
      "ima try that. Ok, I got it to work when i redownloaded it over there and then did make\n",
      "now I have to get on the input files, figure out what they need to be and how to build them. I used pgd spider and converted the genepop file that i found in the 16681 and 80 folder in the filtered folder, and then used the GESTE format which apparently is also the bayescan format. But the good news is that it seems like Bayenv2 also takes the Geste format. \n",
      "\n",
      "INFO  13:11:23 - Saved SPID file as: genepop_bayescan.spid\n",
      "INFO  13:11:23 - loading SPID file: genepop_bayescan.spid\n",
      "INFO  13:12:41 - Convert...\n",
      "INFO  13:12:41 - convert GENEPOP:16681_80_genepop_one_per_line.txt to GESTE / BayeScan:16681_80_bayescan.xml\n",
      "INFO  14:20:22 - translation completed\n",
      "\n",
      "\n",
      "heres what im going to try: \n",
      "\n",
      "./bayescan_2.1 ./16681_80_bayescan.txt -od ./ -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 5000 -pr_odds  >bayescan_log.txt 2>&1\n",
      "\n",
      "\n",
      "that didnt work yesterday, it says it cant find the program. I tried basically every single iteration of the command, so that sucks. Ill keep trying today, i might download a different version. \n",
      "\n",
      "Garrett solved the problem, he put the program into some environment so that it can get run from wherever. \n",
      "\n",
      "carolyn@ubuntu:~/Desktop/BayeScan2.1/source$ ls\n",
      "16681_80_bayescan.sel  beta.o         likelihood.cpp     read_write.o\n",
      "anyoption.cpp          COPYING        likelihood.o       RJupdates.cpp\n",
      "anyoption.h            dirichlet.cpp  Makefile           RJupdates.o\n",
      "anyoption.o            dirichlet.o    MersenneTwister.h  start.cpp\n",
      "bayescan_2.1           errors.cpp     MHupdates.cpp      start.o\n",
      "bayescan_log.txt       errors.o       MHupdates.o        structure.h\n",
      "beta.cpp               global_defs.h  read_write.cpp\n",
      "carolyn@ubuntu:~/Desktop/BayeScan2.1/source$ sudo cp bayescan_2.1 /usr/local/bin/\n",
      "[sudo] password for carolyn: \n",
      "carolyn@ubuntu:~/Desktop/BayeScan2.1/source$ bayescan_2.1\n",
      "\n",
      "bayescan_2.1 16681_80_bayescan.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 5000 -pr_odds >bayescan_log.txt 2>&1\n",
      "\n",
      "It started running. I have no idea how long it will take or how i tell it that i have different populations that i want to test- like i guess right now its running all of them to get that baseline, the putatively neutral set. \n",
      "\n",
      "\n",
      "\n",
      "The bayescan run finished, there are several files that look useful. The one that has the _fst at the end has the q values, I know that is my metric here. The one that I think i have to check to make sure that it got the right convergence is the _AccRte but im not sure what any of those numbers in it mean. \n",
      "\n",
      "There is some code in the bayescan file to plot the results in R, I did that and got a strange looking plot. It also lists and counts the outliers, there were 1397. From what ryan was saying the other day, it doesnt seem like we can trust that the outliers that it chooses here are actually outliers, there are a fair number of false positives, but the ones that are not listed as outliers are not either. \n",
      "\n",
      "Ill take that list and cut them out of the genotype file, then make a poptree map with them, just the neutral loci. I called the file outliers.txt. and it is the outliers, its the ones that didnt pass the threshold of a q-value greater than 0.05. \n",
      "\n",
      "From: http://cbsuapps.tc.cornell.edu/bayescan.help.htm\n",
      "The file output_prop.txt gives the results of the pilot runs.\n",
      "\n",
      "The file output_AccRte.txt gives the evolution of the acceptance rate for all parameters during the process.\n",
      "\n",
      "I didnt get a _prop.txt file out, so I hope that it means that there were useful or positive outcomes from the pilot runs? \n",
      "\n",
      "I havent a clue what the \"the evolution of the acceptance rate for all parameters during the process\" means\n",
      "\n",
      "\n",
      "July 29\n",
      "\n",
      "\n",
      "i made the bayescan files for the different river comparisons, one for each pair of rivers. i just took the input for all the rivers and cut out the ones that i didnt want to test. Then changed the number of populations and saved it as the river name. \n",
      "\n",
      "The GUI on the windows side said i didnt have permission to run the program, and I changed the permissions but that wasnt enough, so Im going over to the other side and trying the command line. i hope that works better.\n",
      "\n",
      "no, no luck. It runs the file i have for all the populations but it gives a strange error when i try the amur_bayescan.txt, it says that there is either: \n",
      "*** Error in `bayescan_2.1': free(): invalid next size (fast): 0x09c2f4f8 ***\n",
      "Aborted (core dumped)\n",
      "\n",
      "or: \n",
      "Segmentation fault (core dumped)\n",
      "At garretts recommendation i tried to make a file that had more than 2 populations in it, i made one with 4, just by copy and pasting, and it ran fine. i made one with 3 and it gave the same segmentation error. \n",
      "\n",
      "\n",
      "I was able to get bayescan to run with just two populations in the file, the secret is that the populations have to be numbered consecutively. So i just have to make sure that theyre right before I renumber them. \n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "I want to make a shell script that runs all the combinations through over the weekend. It will be all the pairwise comparisons, as well as the odd and even lineage comparisons. \n",
      "\n",
      "Id like to start arlequin again today too, but that will have to be more hands on since it is with a GUI\n",
      "\n",
      "I had not used the default values before to run this. I think i meant to, but i accidentally typed the burn in an order of magnitude off: it should be 50,000 the rest of the values, though i typed them out are the defaults. \n",
      "\n",
      "bayescan_2.1 amur_bayescan.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 5000 -pr_odds >amur_bayescan_log.txt 2>&1\n",
      "\n",
      "This time i want to correct that. \n",
      "\n",
      "bayescan_2.1 amur_bayescan.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >amur_bayescan_log.txt 2>&1\n",
      "\n",
      "This is what i came up with and im going to run it on Ryans computer over the weekend: \n",
      "\n",
      "#baysecan_july30.sh\n",
      "\n",
      "bayescan_2.1 amur_bayescan.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >amur_bayescan_log.txt 2>&1\n",
      "bayescan_2.1 tauy_bayescan.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >tauy_bayescan_log.txt 2>&1\n",
      "bayescan_2.1 snohomish_bayescan.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >snohomish_bayescan_log.txt 2>&1\n",
      "bayescan_2.1 nome_bayescan.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >nome_bayescan_log.txt 2>&1\n",
      "bayescan_2.1 kushiro_bayescan.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >kushiro_bayescan_log.txt 2>&1\n",
      "bayescan_2.1 koppen_bayescan.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >koppen_bayescan_log.txt 2>&1\n",
      "bayescan_2.1 hayly_bayescan.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >hayly_bayescan_log.txt 2>&1\n",
      "bayescan_2.1 asia_odd_bayescan.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >asia_odd_bayescan_log.txt 2>&1\n",
      "bayescan_2.1 asia_even_bayescan.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >asia_even_bayescan_log.txt 2>&1\n",
      "bayescan_2.1 NA_odd_bayescan.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NA_odd_bayescan_log.txt 2>&1\n",
      "bayescan_2.1 NA_even_bayescan.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NA_even_bayescan_log.txt 2>&1\n",
      "bayescan_2.1 16681_80_bayescan.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >16681_80_bayescan_log.txt 2>&1\n",
      "\n",
      "\n",
      "all the populations are in this order, within their category: \n",
      "AMUR10\n",
      "AMUR11\n",
      "HAYLY09\n",
      "HAYLY10\n",
      "KOPPE91\n",
      "KOPPE96\n",
      "KUSHI06\n",
      "KUSHI07\n",
      "NOME91\n",
      "NOME94\n",
      "SNOH03\n",
      "SNOH96\n",
      "TAUY09\n",
      "TAUY12\n",
      "\n",
      "so even though they are all labelled as 1 through 2 or 4, that refers to where they are in relation to each other on this list. \n",
      "\n",
      "Aug 3\n",
      "\n",
      "\n",
      "Bayescan finished running over the weekend, the only one that failed was haylyluya, I'll try to get that one started again with a better file. \n",
      "\n",
      "\n",
      "\n",
      "BAYESCAN\n",
      "\n",
      "the very last part of the hayly input file is missing, which is why it wasnt running. i have to find the original file, figure out which population is the second hayly one and fill that in, then restart bayescan. it shouldnt take that long since its one comparison. \n",
      "I copied the _fst and _sel results to the new folder I made in pink called bayescan to plot the results. I had rerun 16681_80_bayescan over the weekend so this version that finished 8/1/2015 i renamed with a bayescan2 so that the file wouldnt overwrite the last one that used. \n",
      "\n",
      "#baysecan_hayly.sh\n",
      "./bayescan_2.1 hayly_bayescan.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >hayly_bayescan_log.txt 2>&1\n",
      "\n",
      "the pop 4 (second hayly pop) was missing the very last number in the file, I went to the 16681_80_bayescan.txt file and pulled it out from the pop 4 there. saved the hayly_basecan.txt and the shell script. moved them both to the ubuntu_shared file and will run them in linux over night. OK there were some scrambled lines, must have been an end of line conversion error or something so I remade the whole file from the 16681_80 file and called it hayly_bayescan2.txt and ran it without a shell, just like this: \n",
      "carolyn@ubuntu:~/Desktop/BayeScan2.1$ bayescan_2.1 hayly_bayescan2.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >hayly_bayescan2_log.txt 2>&1\n",
      "\n",
      "Lisa used an FDR of 0.05 in the pink one paper for the threshold in the bayescan program\n",
      "\n",
      "name of the test\t\t\t\tFDR 0.01   FDR 0.05\n",
      "all_outlier_count\t\t\t\t935\t\t\t1397\n",
      "all_outlier2_count\t\t\t\t934\t\t\t1396\n",
      "amur_outlier_count\t\t\t\t2\t\t\t3\n",
      "tauy_outlier_count\t\t\t\t0\t\t\t0\n",
      "snohomish_outlier_count\t\t\t0\t\t\t0\n",
      "nome_outlier_count\t\t\t\t0\t\t\t0\n",
      "hayly_outlier_count\t\t\t\t0\t\t\t1\n",
      "kushiro_outlier_count\t\t\t2\t\t\t4\n",
      "koppen_outlier_count\t\t\t0\t\t\t0\n",
      "asia_odd_outlier_count\t\t\t7\t\t\t20\n",
      "asia_even_outlier_count \t\t3\t\t\t11\n",
      "NA_odd_outlier_count\t\t\t9\t\t\t20\n",
      "NA_even_outlier_count\t\t\t5\t\t\t9\n",
      "\n",
      "August 4\n",
      "\n",
      "\n",
      "The Haylyl bayescan results are in R now, the files are in the pink directory. Tat an FDR of 0.05 there was one outlier, at 0.01 there were none. \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Bayenv2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "I downloaded Bayenv2 from their github and talked to Ryan about it, he had some good ideas: \n",
      "\n",
      "\n",
      "there are three files that you need:\n",
      "one is the genotypes file, (got it, same format as the Bayescan file)\n",
      "\n",
      "the second is a file with the environmental variable. the variable should be standardized, so that the mean is zero and they have a standard deviation of one. his instructions for this were average, divide and subtract. I think ill google it. but he also said that the longitude should be all together, so that they are on the same scale, dont treat them as seperate coast lines with different scales. And I brought up that i had thought he mean to put them on a circle, where the center of the space between them was a zero, and they were around that spot in a circle, he said that is a good idea, i should do that too, that I can do more than one way. But i dont really know how to get that value, he said i could plot the PCA of them, which is pretty interesting. He says that the order matters, you need one line for the environmental gradient and one column for each of the populations. he said another good one is temperature, to look up in some papers that do this sort of thing and pick their variable, then go to some site that has all the data we need. I can even just pick the latitude off the map and then have temperature for some close by site, it doesn't have to be that exact place. it could be something like the temperature in August or something general like that. \n",
      "\n",
      "the third file you need is the covariance matrix. this one the program will help you make. it just makes a ton of them (or you can ask it to) and they should all be pretty much identical. From there you can take the average of a couple or you can just pick one of them. Just look through them and make sure that they are all pretty much the same. \n",
      "\n",
      "the output of bayenv2 is more raw than you would expect, and the key is that you make sure that the outputs converged, so assess it to make sure that the MCMC chain actually reached a convergence point. this is to show at the end that it stabilized. and to assess that it did. Ryan said there are a couple ways to do this, and that i can read up on it. \n",
      "\n",
      "I read the manual and the imput file is not the geste format. its just allele counts, and its two lines per count. So i googled it and found some code that takes structure files and gives the allele counts for this very reason. its the same guy who has a blog on popgen coding: \n",
      "\n",
      "oK, ITS a bit of a mess and is taking forever so i decided that using the geste format, it wouldnt be that hard to edit myself into the right format. Im doing that right now. in excel. nope. that doesnt work either. \n",
      "\n",
      "i think i have to write something in python that will do it. I got ryans help and have been looking through some of the code I already have. Ryan sent me this, its nested dictionaries. \n",
      "\n",
      "\n",
      "I am going to finish the conversion of the bayesenv file today. Its hard to get back into it and figure out where and why i had given up last time. Luckily, i had basically gotten the dictionary in a dictionary format to work, but was running into some minor errors and then couldnt get the output started. \n",
      "\n",
      "i asked for ryans help and he sat with me for about 40 minutes and looked at the code and then just kindof ended up doing it for me which is what i needed. I was at my wits end having worked on it for so long without actually ever finishing it. \n",
      "\n",
      "This is the finished product that he came up with: \n",
      "\n",
      "#convert_bayescan_to_bayenv2.py\n",
      "\n",
      "###Python code to convert a bayescan input file to Bayesenv2\n",
      "###Carolyn Tarpey and Ryan Waples | September 2015\n",
      "\n",
      "###takes two arguments: 1) bayescan input file, 2) output file name for Bayesenv2\n",
      "# ie: convert_bayescan_to_bayenv2.py 16681_80_bayescan.txt 16681_80_bayenv2_in.txt\n",
      "\n",
      "#!/bin/bash\n",
      "import sys\n",
      "import re\n",
      "\n",
      "pat1 = r'[pop]='\n",
      "AC_of_pop = dict()\n",
      "\n",
      "with open(sys.argv[1], 'r') as INFILE:\n",
      "\tfor line in INFILE:\n",
      "\t\tif pat1 in line:\n",
      "\t\t\tpopsplit = line.strip().split('=')\n",
      "\t\t\tcurrent_pop = int(popsplit[1])\n",
      "\t\t\t#print current_pop\n",
      "\t\t\tAC_of_locus = dict()\n",
      "\t\t\t\n",
      "\t\telif line == '\\n':\n",
      "\t\t\tcontinue\n",
      "\t\telse: \n",
      "\t\t\tlinesplit = line.strip().split(\"\\t\")\n",
      "\t\t\tcurrent_locus = int(linesplit[0])\n",
      "\t\t\tallele_count_A = linesplit[3]\n",
      "\t\t\tallele_count_B = linesplit[4]\n",
      "\t\t\t#put the allele counts as the value in the dict with the locus as the key\n",
      "\t\t\tAC_list = [allele_count_A, allele_count_B]\n",
      "\t\t\tAC_of_locus[current_locus] = (AC_list)\n",
      "\t\t\t#put that dict in the overall dict with the pop as the key\n",
      "\t\t\tAC_of_pop[current_pop] = AC_of_locus\n",
      "\n",
      "keys = AC_of_pop.keys()\n",
      "print keys\n",
      "\n",
      "with open(sys.argv[2], 'w') as OUTFILE:\n",
      "\tfor locus in sorted(AC_of_locus.keys()):\n",
      "\t\ttop_line = [] \n",
      "\t\tbottom_line = [] \n",
      "\t\tfor pop in sorted(AC_of_pop.keys()):\n",
      "\t\t\ttop_line.append(AC_of_pop[pop][locus][0])\n",
      "\t\t\tbottom_line.append(AC_of_pop[pop][locus][1])\n",
      "\t\t\n",
      "\t\tOUTFILE.write('\\t'.join(top_line))\n",
      "\t\tOUTFILE.write('\\t\\n')\n",
      "\t\tOUTFILE.write('\\t'.join(bottom_line))\n",
      "\t\tOUTFILE.write('\\t\\n')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "############# test the order of the output data\n",
      "\n",
      "# with open(sys.argv[2], 'w') as OUTFILE:\n",
      "\t# for locus in sorted(AC_of_locus.keys()):\n",
      "\t\t# top_line = [] \n",
      "\t\t# bottom_line = [] \n",
      "\t\t# for pop in sorted(AC_of_pop.keys()): \n",
      "\t\t\t# top_line.append(str(pop))\n",
      "\t\t\t# top_line.append(str(locus))\n",
      "\t\t\t# bottom_line.append(str(pop))\n",
      "\t\t\t# bottom_line.append(str(locus))\n",
      "\t\t\n",
      "\t\t# OUTFILE.write('\\t'.join(top_line))\n",
      "\t\t# OUTFILE.write('\\t\\n')\n",
      "\t\t# OUTFILE.write('\\t'.join(bottom_line))\n",
      "\t\t# OUTFILE.write('\\t\\n')\n",
      "\n",
      "# with open(sys.argv[2], 'w') as OUTFILE:\n",
      "\t# for locus in sorted(AC_of_locus.keys()):\n",
      "\t\t# top_line = [] \n",
      "\t\t# bottom_line = [] \n",
      "\t\t# for pop in sorted(AC_of_pop.keys()):\n",
      "\t\t\t# top_line.append(AC_of_pop[pop][locus][0])\n",
      "\t\t\t# top_line.append(str(locus))\n",
      "\t\t\t# bottom_line.append(AC_of_pop[pop][locus][1])\n",
      "\t\t\t# bottom_line.append(str(locus))\n",
      "\t\t\n",
      "\t\t# OUTFILE.write('\\t'.join(top_line))\n",
      "\t\t# OUTFILE.write('\\t\\n')\n",
      "\t\t# OUTFILE.write('\\t'.join(bottom_line))\n",
      "\t\t# OUTFILE.write('\\t\\n')\t\t\t\t\t\n",
      "\t\t\n",
      "#for pop, ac_dict in AC_of_pop.items():\n",
      "#\tprint pop\n",
      "#\tprint ac_dict.items()\n",
      "#\tprint '\\n'\n",
      "\n",
      "\n",
      "\n",
      "Gettign ready to run the program\n",
      "\n",
      "runs on linux side, only polymorphic loci in the dataset (arlequin has told us which are polymorphic for each of the subsets) \n",
      "environmental data has to in order of the populations\n",
      "\n",
      "\n",
      "the manual says that we have to have one file for each of the snps, but it seems like he has changed that in the newest version: added a bash script which loops through all SNPs in a file. \n",
      "\n",
      "So im not going to try to figure out how to make that until i realize that i have to. I may have to run it on Ryans or Garretts computer though, I dont think i have the capability to run the newest version on my computer because i have the linux 32 bit system. :/\n",
      "\n",
      "\n",
      "\n",
      "it wants me to come up with a matrix file as the input where  you also get one as an output file. so maybe i use the one that they provide in the example bundle? Im not really sure whats up with that. \n",
      "\n",
      "but ill try to get it all running tomorrow, and I have to think about the tests that i want to run with this and make the input files for them. \n",
      "\n",
      "\n",
      "         $$ September 9 2015\n",
      "==================================================================\n",
      "\n",
      "I want to move forward with the Bayenv2 run today- but it might make more sense to set it up on RYans or garretts computer, considering how fast these runs of Bayescenv finished compared to when i had them running on here. \n",
      "\n",
      "I need the Matrix file and im still not really sure what to do- do you start it with the example and then get your own? or do you estimate your own using a separate process? \n",
      "\n",
      "./bayenv2 -i SNPSFILE -p NUMPOPS -k 100000 -r 14637 >matrix.out\n",
      "\n",
      "./bayenv2 -i 16681_80_bayenv2_in.txt -p 14 -k 100000 -r 14637 >matrix.out\n",
      "(draws the covariance matrix into matrix.out every 500 iterations)\n",
      "\n",
      "Im wondering if I need a separate one for each of the runs? My guess is yes? because theyre all different? Then what about the polymorphic loci thing? I wonder if I need to adjust the other runs of the programs to only be the polymorphic loci? \n",
      "\n",
      "anyway, i tried it on the linux side and the core dumped\n",
      "./bayenv2_32bit -i 16681_80_bayenv2_in.txt -p 14 -k 100000 -r 14637 >matrix.out\n",
      "\n",
      "i tried this on Garretts and so far its working. He has the 64 bit version of Linux so i was able to use that program version that is the one that is maintained.\n",
      "\n",
      "./bayenv2 -i 16681_80_bayenv2_in.txt -p 14 -k 100000 -r 14637 >matrix.out 2>&1 matrix_log.txt\n",
      "\n",
      "In the mean time, Ill get the files ready to run the individual covariance matrixes on.\n",
      "\n",
      "\n",
      "Ok, back to converting the files. I need pgdspider to convert from the genepop file to the bayescan file. \n",
      " \n",
      "i have the genepop files from arlequin, each is all the populations in it, all 14 even though they are the data sets for just some of the populations. i opened them up and took out the other populations and named them with the NA or ASIAN. i get them into bayescan format (geste) using the pgdspider and then manually edited them, converting the spaces to tabs (i guess i could totally code this in but whatever) then use the script to get them into the bayesenv2 format.\n",
      "  \n",
      "INFO  13:34:36 - loading SPID file: genepop_bayescan.spid\n",
      "INFO  13:34:37 - Convert...\n",
      "INFO  13:34:37 - convert GENEPOP:15945_ASIAN_polymorphic.txt to GESTE / BayeScan:15945_polymorphic_ASIAN_bayescan.txt\n",
      "INFO  13:37:47 - translation completed\n",
      "INFO  13:38:47 - Convert...\n",
      "INFO  13:38:47 - convert GENEPOP:16409_NA_polymorphic.txt to GESTE / BayeScan:16409_polymorphic_NA_bayescan.txt\n",
      "INFO  13:41:07 - translation completed\n",
      "\n",
      " convert_bayescan_to_bayenv2.py 15945_polymorphic_ASIAN_bayescan.txt 15945_polymorphic_ASIAN_bayenv2_in.txt\n",
      " \n",
      " convert_bayescan_to_bayenv2.py 16409_polymorphic_NA_bayescan.txt 16409_polymorphic_NA_bayenv2_in.txt\n",
      " \n",
      "sweet! done. \n",
      "\n",
      "theres something wrong with the 16409, there are some of the loci that only have one allele, so the conversion script wont work on them, and also they are wrong. So I went back to the original Arlequin file and got the original file, renamed it 16409_North_polymorphic.txt and restarted it in PGD spider ot get it converted to the bayescan format\n",
      "\n",
      "INFO  13:54:24 - Convert...\n",
      "INFO  13:54:24 - convert GENEPOP:16409_North_polymorphic.txt to GESTE / BayeScan:16409_North_polymorphic_bayescan.txt\n",
      "INFO  13:56:50 - translation completed\n",
      "\n",
      " convert_bayescan_to_bayenv2.py 16409_North_polymorphic_bayescan.txt 16409_polymorphic_North_bayenv2_in.txt\n",
      "\n",
      " same- still missing a single locus. \n",
      " \n",
      " I went back to the original genepop file again and deleted the title line, renamed it 16409_genepop.txt and tried pgdspider again\n",
      "It did not solve the problem. still missing a locus\n",
      "\n",
      "I tried it on the file that has all the individuals, north and asian. still not right. \n",
      "\n",
      "going to the source, to remake the file from the 16681 genepop. \n",
      "\n",
      "perl subset_genepop_by_SNPs.pl NApolymorphic.txt 16681_80_genepop_one_per_line.txt > 16409_revamp.txt\n",
      "\n",
      "made a new spid file to convert it in into a bayescan, but I didnt take the populations out, i feel like at this point if i can just get the right number of samples, i can do that in excel. \n",
      "\n",
      "it still failed to get the number of samples right. SO I went online and googled other ways of converting from genepop to bayescan format, and got this guys code from github: \n",
      "\n",
      "\n",
      "it still failed to get the number of samples right. SO I went online and googled other ways of converting from genepop to bayescan format, and got this guys code from github: \n",
      "\n",
      "#markravinet/bayescan_convert\n",
      "\n",
      "#bayescan_convert/big_convert.R\n",
      "#@markravinet markravinet on Jun 6, 2014 Script upload\n",
      "\n",
      "#### Convert LARGE genepop SNP file to BayesScan input file ####\n",
      "# To be used on cluster\n",
      "# Mark Ravinet September 2013, University of Gothenburg\n",
      "## Modified heavily from code provided by Kevin Keenan (cheers Kevin!)\n",
      "\n",
      "I opened it up in R and tried it on the 16409_revamp.txt file as the genepop, and tried to convert it to 16409_revamp_bayescan.txt, and its running right now. \n",
      "\n",
      "it gives 16409 loci, but the last one in each population is empty, like 0 0 for the count. So i opened the file that it takes in, the 16409_revamp and at the end of each line there are 2 tabs, and at the end of a line of a regular genepop file there are no tabs. so it has something to do with the additional tabs? \n",
      "\n",
      "i tried a couple things with the 16681_80_genepop_one_per_line_txt file and the subset script to see if I could get the right number, like change the line endings, remove the actual name of the population after POP and making sure that the last locus is actully in the file, but none of them are working. each time there are one too few loci and the end of the lines end in two tabs, where there should be no tabs. \n",
      "\n",
      "i tried to subset the genotyppes with the NA monomorphic list, and there were no problems. It came out perfectly fine. garrett looked at it with me and we decided that there is somethign wrong with this locus 100029_14 if you take it out, the file looks fine. the input file does have a genotype for it. so that is weird. the script to subset the genotypes doesnt have any digit limit on the name of the loci, so thats not an issue. \n",
      "\n",
      "Im worried that this locus has been missing or left out of all the other analyiss that was was done using a genepop file, but luckily since it was the last of the loci, it wouldnt have screwed up the indexing at all, since there is nothing that comes after it. \n",
      "\n",
      "I checked in batch_3_16681_pop_80.genepop the original file that doesnt even have one per line, and there were 16681 genotypes in the first sample the one that Ive been using to check every time. \n",
      "\n",
      "        $$ September 10 2015\n",
      "==================================================================\n",
      "lepMap at chr 4 and 5\n",
      "\n",
      "I talked to lisa about the problems I had had in Arlequin, and she said to forget that one locus 100029_14, let it go. Just make sure its the only  one that is missing from the files and just move on. (she also said to figure out if its the name or the fact that it is the last thing in the file) \n",
      "\n",
      "Then she looked at the results of the Arlequin and pulled up some of the projects that she had on her computer. It didnt look like she had used the hierarchical model, so that might be what was the difference? Anyway, she asked for my .arp file and is checking it out on her computer to see if she can get it to run.\n",
      "\n",
      "I went over to Garretts, and she is messing with the Asian arp so I started the ALL 16681 and the North American 15945- I think the secret is that they shouldnt be run with the hierarchical model because they were totally working. \n",
      "\n",
      "She said to plot the null distribution and then plot the outliers over top of it. \n",
      "\n",
      "I want to continue to put together the analysisMetaData sheet. \n",
      "\n",
      "Ok, So im looking for the results for NA to put in the sheet and in the only output that has the individual based F statistics, I find this: \n",
      "********************\n",
      "Locus by locus AMOVA:\n",
      "********************\n",
      "\n",
      "Distance method for locus-by-locus analysis: Pairwise differences\n",
      "\n",
      "List of loci with only one allele:\n",
      "----------------------------------\n",
      "\n",
      "94 172 180 200 206 228 249 266 294 307 377 515 528 576 642 692 700 743 746 747 812 817 818 829 889 916 925 962 996 1003 1023 1051 1057 1084 1095 1096 1121 1163 1213 1218 1248 1250 1274 1285 1358 1365 1402 1425 1450 1453 1457 1472 1490 1499 1574 1610 1662 1695 1704 1717 1719 1744 1750 1792 1807 1809 1813 1848 1899 1959 1983 1987 2000 2041 2075 2097 2111 2119 2124 2127 2144 2212 2216 2230 2242 2339 2373 2469 2478 2534 2537 2560 2566 2592 2613 2626 2644 2694 2724 2740 2751 2761 2792 2813 2833 2852 2865 2876 2884 2897 3060 3107 3128 3174 3189 3193 3203 3223 3237 3251 3356 3363 3414 3457 3472 3530 3541 3598 3654 3657 3671 3676 3687 3696 3772 3822 3843 3932 3936 3938 3940 4012 4034 4071 4086 4102 4125 4203 4247 4250 4266 4294 4302 4333 4362 4369 4371 4373 4478 4536 4554 4603 4620 4772 4851 4905 4909 4930 5043 5052 5066 5118 5161 5196 5282 5348 5354 5362 5385 5404 5412 5453 5466 5501 5549 5554 5559 5634 5635 5638 5794 5815 5860 5877 5879 5901 5902 5940 5947 5970 5975 5994 6028 6033 6059 6082 6093 6099 6108 6123 6131 6137 6212 6250 6329 6337 6368 6391 6486 6527 6546 6550 6556 6591 6612 6632 6636 6641 6649 6673 6680 6688 6709 6737 6755 6807 6814 6842 6845 6873 6889 6989 7041 7076 7086 7087 7108 7155 7185 7254 7277 7283 7284 7305 7334 7341 7527 7555 7619 7629 7630 7641 7663 7787 7809 7828 7839 7930 7950 7955 7992 8000 8022 8044 8049 8092 8168 8174 8194 8204 8235 8238 8246 8253 8270 8290 8302 8336 8372 8450 8458 8460 8515 8524 8569 8575 8576 8601 8606 8607 8632 8664 8684 8828 8836 8945 8972 8994 9052 9082 9152 9156 9197 9201 9216 9382 9395 9446 9475 9535 9584 9613 9615 9636 9648 9658 9669 9715 9724 9805 9852 9876 9990 10023 10083 10153 10161 10285 10287 10307 10343 10366 10406 10407 10410 10549 10561 10593 10646 10669 10705 10730 10748 10777 10780 10815 10818 10824 10855 10857 10883 10886 10890 10915 10979 11023 11034 11051 11074 11084 11153 11211 11224 11232 11301 11353 11365 11422 11444 11453 11462 11504 11517 11528 11596 11611 11674 11693 11858 11871 11906 11961 11985 12023 12028 12073 12078 12136 12156 12162 12166 12168 12174 12200 12219 12258 12281 12341 12410 12433 12446 12453 12474 12475 12494 12499 12546 12590 12615 12653 12711 12721 12728 12778 12800 12812 12828 12856 12861 12877 13018 13038 13045 13051 13086 13088 13109 13118 13207 13223 13246 13264 13290 13315 13332 13360 13388 13395 13396 13397 13461 13576 13598 13645 13746 13806 13858 13863 13874 13921 13953 14021 14059 14094 14103 14107 14130 14157 14209 14223 14227 14246 14259 14293 14300 14315 14342 14352 14353 14378 14379 14389 14400 14401 14403 14408 14455 14525 14531 14533 14551 14570 14592 14599 14626 14679 14725 14733 14737 14752 14754 14832 14860 14955 14965 14974 14975 15042 15056 15075 15081 15095 15111 15116 15123 15158 15178 15187 15202 15218 15232 15260 15265 15279 15283 15288 15293 15311 15315 15350 15354 15397 15408 15412 15416 15423 15457 15477 15488 15537 15545 15551 15574 15589 15595 15602 15607 15618 15665 15682 15704 15733 15744 15757 15798 15804 15811 15824 15835 15877 15900 15907 15917 15919 15923 15924 15926 15937 15997 16007 16016 16030 16047 16061 16096 16115 16124 16127 16146 16160 16189 16210 16236 16271 16312 16334 16361 16366 16370 16373 16389 16391 16402 \n",
      "\n",
      "which i totally thought I had already solved that problem when i made the sheets that cut out the monomorphic loci. what has changed since i cut it down from 16681 to 16409 that there are  now all these new monomorphic loci? could it be possible that I messed up and deleted the wrong populations? \n",
      "\n",
      "im sure that the original list came from the 16681_80_named_NA run and was using the NA pops:  \n",
      "\t 1   KOPPE96  -0.01734            0.615836\n",
      "     2    NOME94   0.01221            0.407625\n",
      "     3    SNOH96  -0.02381            0.770283\n",
      "     4   KOPPE91  -0.00009            0.517107\n",
      "     5    NOME91  -0.00913            0.621701\n",
      "     6    SNOH03  -0.01152            0.641251\n",
      "\t \n",
      "\t \n",
      "But it still doesnt solve the problem of like what is going on? \n",
      "\n",
      "\n",
      "From 15945_Asian2.res this is the results when you do  GENETIC STRUCTURE ANALYSIS\n",
      "\n",
      "\n",
      "Number of usable loci for distance computation : 15259\n",
      "Allowed level of missing data                  : 0.05\n",
      "\n",
      "\n",
      "List of loci with too much missing data :\n",
      "-----------------------------------------\n",
      "    4    41   142   150   156   172   195   241   286   287   292   307   330   344   353    432   434   436   453   497   504   512   514   519   546   558   577   584   602   618    625   628   662   666   706   745   746   772   785   790   796   802   808   809   813    820   821   831   865   879   900   917   944   965   975   999  1021  1033  1064  1066   1082  1105  1108  1133  1183  1239  1242  1289  1293  1308  1318  1331  1355  1361  1371   1377  1395  1411  1415  1425  1459  1480  1534  1566  1567  1577  1594  1604  1679  1736   1765  1796  1814  1835  1905  1917  1943  1961  2001  2011  2034  2040  2044  2060  2066   2087  2091  2171  2179  2180  2225  2256  2308  2357  2365  2366  2371  2402  2415  2511   2549  2593  2605  2610  2622  2624  2649  2655  2660  2674  2713  2752  2761  2783  2786   2801  2863  2887  2930  2936  2967  3038  3075  3111  3150  3159  3243  3252  3306  3312   3338  3380  3431  3437  3442  3462  3468  3473  3527  3563  3571  3599  3719  3744  3765   3766  3822  3835  3845  3849  3851  3878  3882  3884  3886  3956  3996  4045  4048  4079   4118  4145  4148  4172  4205  4242  4256  4270  4274  4333  4365  4368  4435  4452  4470   4477  4481  4489  4501  4518  4520  4538  4558  4566  4578  4586  4602  4609  4613  4636   4656  4679  4698  4719  4763  4788  4798  4835  4853  4858  4866  4876  4898  4918  4947   4957  4968  4993  4994  5001  5019  5050  5100  5117  5124  5140  5166  5198  5224  5257   5258  5264  5322  5331  5345  5346  5355  5360  5389  5423  5465  5475  5491  5512  5523   5525  5584  5594  5602  5624  5628  5659  5664  5666  5669  5672  5691  5719  5728  5748   5762  5800  5809  5854  5864  5868  5926  5933  5937  5940  5978  5980  5991  5998  6005   6034  6102  6114  6123  6152  6164  6175  6179  6181  6200  6238  6244  6269  6284  6289   6291  6295  6310  6321  6334  6366  6405  6456  6460  6486  6495  6504  6521  6580  6586   6632  6659  6666  6694  6719  6726  6745  6750  6771  6811  6836  6846  6860  6940  6996   7012  7013  7014  7041  7057  7113  7174  7187  7188  7205  7213  7214  7223  7228  7241   7273  7400  7429  7433  7439  7471  7474  7484  7503  7514  7515  7533  7561  7577  7583   7584  7627  7631  7634  7694  7696  7711  7769  7795  7918  7927  7939  7941  7943  7950   7951  7966  7973  7993  8011  8079  8084  8113  8147  8156  8181  8248  8264  8285  8294   8308  8319  8344  8351  8358  8376  8408  8441  8445  8448  8480  8551  8552  8662  8673   8721  8746  8757  8826  8892  8926  8979  8988  8990  9024  9037  9060  9062  9083  9103   9105  9165  9172  9186  9192  9195  9200  9228  9236  9245  9264  9270  9271  9309  9318   9353  9373  9379  9444  9467  9483  9500  9510  9556  9588  9597  9614  9684  9698  9729   9743  9782  9790  9841  9853  9871  9877  9935  9938  9952  9973  10009  10033  10078  10080   10099  10135  10136  10144  10192  10203  10219  10305  10308  10315  10337  10352  10435  10460  10491   10494  10515  10530  10538  10557  10574  10608  10610  10674  10740  10745  10780  10782  10798  10836   10858  10860  10870  10876  10901  10905  10917  10965  10996  11038  11047  11056  11066  11074  11086   11092  11115  11116  11179  11197  11239  11266  11274  11281  11321  11368  11385  11432  11433  11451   11465  11491  11559  11613  11690  11732  11782  11795  11842  11865  11916  11980  12162  12191  12204   12216  12217  12262  12326  12338  12415  12482  12540  12574  12613  12620  12656  12668  12715  12716   12749  12759  12763  12773  12878  12903  12957  12976  12977  13036  13043  13051  13082  13115  13206   13227  13241  13276  13285  13300  13401  13409  13416  13476  13502  13506  13519  13526  13529  13589   13591  13597  13606  13631  13634  13688  13697  13705  13707  13769  13806  13833  13880  13884  13890   13895  13957  14025  14026  14042  14101  14122  14135  14149  14159  14188  14208  14246  14263  14269   14282  14296  14321  14340  14444  14447  14454  14472  14483  14505  14506  14570  14623  14626  14631   14653  14708  14758  14772  14779  14810  14820  14823  14951  14966  14986  14993  14994  15020  15039   15052  15094  15118  15144  15145  15195  15196  15198  15204  15212  15232  15245  15330  15358  15391   15422  15434  15464  15470  15472  15474  15532  15545  15615  15654  15701  15703  15756  15757  15803   15808  15858  15876  15934  15935  15936  15937  15939  15941  15943  15944  \n",
      "\t\n",
      "And the output for the Asian 15045 original has the same list of loci with too much missing data. OOOOOugh what is going on?\n",
      " \n",
      "Ok, so the levels for the missing data are different for those. And the arlequin manual says that you can set that level in the general settings tab. \n",
      "\n",
      "Lisa talked with me a long time about the programs and what I should be looking for and doing, she couldnt get the hierarchical version of arlequin to run; she thought it was because she was using an newer version, I am running a version from 2010, she just downloaded the 2015 and it was giving the same errors that I was having all along. \n",
      "\n",
      "WE talked about using the program galaxy to do some summary stats on the files to explore what the data looks like. \n",
      "\n",
      "So I think i have to figure out what the MAF and missing data are in those new data sets, throw out that one locus that was giving me trouble in the conversion and go from there. I have the perl scripts that garrett wrote to count the missing and look at the hets, and I can do basic genepop stats with the files and throw shit out from that too. \n",
      "\n",
      "         $$ October 19  2015\n",
      "==================================================================\n",
      "\n",
      "So the next thing that i want to do is build the bayenv2 env files. so that i can get those started. I have the genotype file made, im pretty positive. Especially since I have already estimated the covariance matrix. I dont know if its one of those things where you have to estimate the covariance matrix each time you do it, or with each separate grouping of genotypes? my guess is yes probably. \n",
      "\n",
      " so today Im going to build inport files. \n",
      " \n",
      " \n",
      "asia_odd_lat\t\t\t2\t3\t8\t13\n",
      "asia_even_lat\t\t\t1\t4\t7\t14\n",
      "NA_odd_lat\t\t\t\t5\t9\t11\t\n",
      "NA_even_lat\t\t\t\t6\t10\t12\t\n",
      "\n",
      "\n",
      "this is what i ran for bayenv2 back in september: ./bayenv2_32bit -i 16681_80_bayenv2_in.txt -p 14 -k 100000 -r 14637 >matrix.out\n",
      "  \n",
      "and I have the input file! So I just need to edit it into the 4 groupings ASE ASO NAO NAE. \n",
      "\n",
      "i made the environmental files that would be formatted right for the LFMM, and I can use the NAO and NAE and compare the ASE and ASO with what I made and used earlier. \n",
      "The input files for the 4 lineage groupings are made for the bayenv2 file now. the environmental files are the same as is used for the LFMM. \n",
      "Im going to make the matrix file for each of the groups of inputs.. and im going to try the all file with the matrix that ive already made. \n",
      "\n",
      "Im looking at the instructions in the manual about the matrix and this is what it says: The command line for estimating the matrix is:\n",
      "\n",
      "./bayenv2 -i SNPSFILE -p NUMPOPS -k 100000 -r 63479 > matrix.out\n",
      "\n",
      "This outputs the current draw of the covariance matrix into matrix.out every 500 iterations. The rows and columns in the covariance matrix appear in the same population order as they appeared in the allele count file. The covariance matrix can be visualized by the image command in R, the cov2cor function can be used to convert a covariance matrix into a correlation matrix. The correlation matrix computed from the estimated covariance matrix should be very correlated with the matrix of pairwise FST (see the paper for discussion). The matrix should be inspected for unexpected low or high correlations, as judged from pairwise FST , as these may indicate problems in the labeling of populations. Note that the entries of the covariance matrix are not forced to be positive, thus small negative entries in the covariance matrix are possible. Matrices should be compared within an across independent runs of the program to ensure that the matrix is well estimated.  \n",
      "\n",
      "I am going to run the matrix for these four and then try to figure out what to do with the visualizing the matrix at the same time for all 5 of them. \n",
      "\n",
      "\n",
      "./bayenv2_32bit -i NAE_bayesenv_in.txt -p 3 -k 100000 -r 14637 >NAE_matrix.out\n",
      "./bayenv2_32bit -i NAO_bayesenv_in.txt -p 3 -k 100000 -r 14637 >NAO_matrix.out\n",
      "./bayenv2_32bit -i ASE_bayesenv_in.txt -p 4 -k 100000 -r 14637 >ASE_matrix.out\n",
      "./bayenv2_32bit -i ASO_bayesenv_in.txt -p 4 -k 100000 -r 14637 >ASO_matrix.out\n",
      "\n",
      " Ill run the LFMM for the North America set today too, and see what else is going on with the runs that i started on friday. \n",
      " \n",
      " the environmental files for Bayenv2 are the same as for the bayescenv. \n",
      " \n",
      " \n",
      " \n",
      "          $$ October 23  2015\n",
      "==================================================================\n",
      "\n",
      "\n",
      "Garrett figured out why the bayenv2 program was failing for so long, even though it was running (and crashing) on my computer. \n",
      "\n",
      "It turns out it cant handle cases where there are no alternate alleles between the populations. So if there are 3 populations, it can only take the polymorphic loci. Makes sense. \n",
      "\n",
      "So he wrote a perl script that filters out the loci that are monomorpic from the input files: \n",
      "\n",
      "#/usr/bin/perl -w\n",
      "use strict;\n",
      "\n",
      "my$inFile=$ARGV[0];\n",
      "my($file,$ext)=split '\\.', $inFile, 2;\n",
      "my$filteredOut=$file.\"_filtered.txt\";\n",
      "my$failedOut=$file.\"_failedLoci.txt\";\n",
      "\n",
      "\n",
      "open(INFILE,\"<$inFile\")||die \"cannot open $inFile:$!\";\n",
      "open(FILTERED,\">$filteredOut\")||die \"cannot open $filteredOut:$!\";\n",
      "open(FAILED,\">$failedOut\")||die \"cannot open $failedOut:$!\";\n",
      "my$locusCount=0;\n",
      "while (my$line=<INFILE>){\n",
      "\t$locusCount++;\n",
      "\tchomp $line;\n",
      "\tmy$line2=<INFILE>;\n",
      "\tchomp $line2;\n",
      "\tmy@col1=split \"\\t\", $line;\n",
      "\tmy@col2=split \"\\t\", $line2;\n",
      "\tmy$keep1=0;\n",
      "\tmy$keep2=0;\n",
      "\tforeach my$i (0..$#col1){\n",
      "\t\tif($col1[$i]>0){\n",
      "\t\t\t$keep1=1;\n",
      "\t\t}\n",
      "\t\tif($col2[$i]>0){\n",
      "\t\t\t$keep2=1;\n",
      "\t\t}\n",
      "\t}\n",
      "\tif(($keep1==1)&&($keep2==1)){\n",
      "\t\tprint FILTERED \"$line\\n$line2\\n\";\n",
      "\t}else{\n",
      "\t\tprint FAILED \"$locusCount\\n\";\n",
      "\t}\n",
      "}\n",
      "close INFILE;\n",
      "close FILTERED;\n",
      "close FAILED;\n",
      "\n",
      "\n",
      "So this is what I did: \n",
      "perl removeZeroCounts.pl NAE_bayesenv_in.txt\n",
      "perl removeZeroCounts.pl NAO_bayesenv_in.txt\n",
      "perl removeZeroCounts.pl ASE_bayesenv_in.txt\n",
      "perl removeZeroCounts.pl ASO_bayesenv_in.txt\n",
      "\n",
      "it creates a file that is the loci that passed the test, in the same format as the program needs and a list of the index of the loci that failed. It is starting at 1. \n",
      "\n",
      "\n",
      "This is the new shell script to run the program: \n",
      "\n",
      "./bayenv2 -i NAE_bayesenv_in_filtered.txt -p 3 -k 100000 >NAE_matrix.out\n",
      "./bayenv2 -i NAO_bayesenv_in_filtered.txt -p 3 -k 100000 >NAO_matrix.out\n",
      "./bayenv2 -i ASE_bayesenv_in_filtered.txt -p 4 -k 100000 >ASE_matrix.out\n",
      "./bayenv2 -i ASO_bayesenv_in_filtered.txt -p 4 -k 100000 >ASO_matrix.out\n",
      "\n",
      "I started it and its working! \n",
      "\n",
      "\n",
      "\n",
      "          $$ October 26  2015\n",
      "==================================================================\n",
      "\n",
      "talked to lisa, we talked about Bayenv2 and how the program distance transforms the variables (so it just takes the variables that I have already standardized and does the absolute value of them) which i think is a bad idea and im not sure why the program does it. it means that things that are really far on the edge of the scale have the same values after you take the absolute value of them. a -1.6 will be the same as a 1.6 when in fact theyre really really different. \n",
      "\n",
      "The nuthatch people used the standardized numbers and let the program take the absolute values, Lisa says that that doesnt seem right to her either and so I have to look into that a little more\n",
      "This is where I got their info: http://datadryad.org/resource/doi:10.5061/dryad.8hm4p\n",
      "\n",
      "        $$ October 27  2015\n",
      "==================================================================\n",
      "\n",
      "\n",
      "Talked to ryan about a host of issues. \n",
      "\n",
      "BAYENV2\t\n",
      "estimating the covariance matrix: \n",
      "\t1. what is the pop_spec.out file and is it OK that mine is all zeros? \n",
      "\t2. how do I know if the covariance matrices are a small set of very similar covariance matrices? \n",
      "\n",
      "in the environmental file, what is up with it taking the absolute value of the environmental variables? (distance transforming the varibles) Should I rescale or make all mine positive? \n",
      "\n",
      "\n",
      "\n",
      "interpreting the covariance matrix: \n",
      "\tthe diagonals are the variance within a population, and the others are the covariance between the populations, pairwise. So the closer the pops are to eachother, related wise, or the more similar they are, the bigger the number should be. if its zero than they are not related at all and knowing something about one population will tell you absolutely nothing about another population.  So the covariance matrices that i have are not right. \n",
      "\t\n",
      "\tthey should not be such tiny numbers. the covariance matrix for all the populations are all like this: \n",
      "\t\n",
      "\tVAR-COVAR MATRIX: ITER = 99500\n",
      "2.607952e-01\t2.408774e-01\t2.412271e-01\t2.611193e-01\t2.435891e-01\t2.611281e-01\t2.624981e-01\t2.411676e-01\t2.412103e-01\t2.600593e-01\t2.492190e-01\t2.627749e-01\t2.413656e-01\t2.603849e-01\t\n",
      "2.408774e-01\t2.439801e-01\t2.421166e-01\t2.420893e-01\t2.455869e-01\t2.411968e-01\t2.423487e-01\t2.445997e-01\t2.420919e-01\t2.405774e-01\t2.536834e-01\t2.427992e-01\t2.425350e-01\t2.413946e-01\t\n",
      "2.412271e-01\t2.421166e-01\t2.421246e-01\t2.424411e-01\t2.447295e-01\t2.416624e-01\t2.427418e-01\t2.435000e-01\t2.412529e-01\t2.409669e-01\t2.527309e-01\t2.433016e-01\t2.416363e-01\t2.417605e-01\t\n",
      "2.611193e-01\t2.420893e-01\t2.424411e-01\t2.632416e-01\t2.450296e-01\t2.624906e-01\t2.636209e-01\t2.423635e-01\t2.424429e-01\t2.613356e-01\t2.508730e-01\t2.642203e-01\t2.425988e-01\t2.616443e-01\t\n",
      "2.435891e-01\t2.455869e-01\t2.447295e-01\t2.450296e-01\t2.525655e-01\t2.456028e-01\t2.448108e-01\t2.469368e-01\t2.448334e-01\t2.435311e-01\t2.624722e-01\t2.482239e-01\t2.451158e-01\t2.442712e-01\t\n",
      "2.611281e-01\t2.411968e-01\t2.416624e-01\t2.624906e-01\t2.456028e-01\t2.643888e-01\t2.635775e-01\t2.413674e-01\t2.417208e-01\t2.615159e-01\t2.524568e-01\t2.656636e-01\t2.417774e-01\t2.617215e-01\t\n",
      "2.624981e-01\t2.423487e-01\t2.427418e-01\t2.636209e-01\t2.448108e-01\t2.635775e-01\t2.661666e-01\t2.426014e-01\t2.427120e-01\t2.626095e-01\t2.501021e-01\t2.651486e-01\t2.428876e-01\t2.629219e-01\t\n",
      "2.411676e-01\t2.445997e-01\t2.435000e-01\t2.423635e-01\t2.469368e-01\t2.413674e-01\t2.426014e-01\t2.471917e-01\t2.434992e-01\t2.408211e-01\t2.551210e-01\t2.429389e-01\t2.439459e-01\t2.416743e-01\t\n",
      "2.412103e-01\t2.420919e-01\t2.412529e-01\t2.424429e-01\t2.448334e-01\t2.417208e-01\t2.427120e-01\t2.434992e-01\t2.421314e-01\t2.409748e-01\t2.529524e-01\t2.433953e-01\t2.416149e-01\t2.417464e-01\t\n",
      "2.600593e-01\t2.405774e-01\t2.409669e-01\t2.613356e-01\t2.435311e-01\t2.615159e-01\t2.626095e-01\t2.408211e-01\t2.409748e-01\t2.612199e-01\t2.492890e-01\t2.632430e-01\t2.411145e-01\t2.606044e-01\t\n",
      "2.492190e-01\t2.536834e-01\t2.527309e-01\t2.508730e-01\t2.624722e-01\t2.524568e-01\t2.501021e-01\t2.551210e-01\t2.529524e-01\t2.492890e-01\t2.772195e-01\t2.559598e-01\t2.531670e-01\t2.500269e-01\t\n",
      "2.627749e-01\t2.427992e-01\t2.433016e-01\t2.642203e-01\t2.482239e-01\t2.656636e-01\t2.651486e-01\t2.429389e-01\t2.433953e-01\t2.632430e-01\t2.559598e-01\t2.692204e-01\t2.434116e-01\t2.634254e-01\t\n",
      "2.413656e-01\t2.425350e-01\t2.416363e-01\t2.425988e-01\t2.451158e-01\t2.417774e-01\t2.428876e-01\t2.439459e-01\t2.416149e-01\t2.411145e-01\t2.531670e-01\t2.434116e-01\t2.429301e-01\t2.418841e-01\t\n",
      "2.603849e-01\t2.413946e-01\t2.417605e-01\t2.616443e-01\t2.442712e-01\t2.617215e-01\t2.629219e-01\t2.416743e-01\t2.417464e-01\t2.606044e-01\t2.500269e-01\t2.634254e-01\t2.418841e-01\t2.617930e-01\t\n",
      "\n",
      "\n",
      "\n",
      "so that tells you a little bit about what we are going for. \n",
      "\n",
      "the covariance can be negative because, overall it should be positive, but what is happening is that its calculated on standardized data, so the data has a mean and it is telling us the deviation of them from the mean. so naturally some will fall above the mean and some will be below the mean. \n",
      "\n",
      "looking at the diagonals between the covariance matrices, they should be the ones that vary the least because they are just representing the variation within the populatins and that should be less variable between the runs, that should not be as dependent on the other things going on. \n",
      "\n",
      "the issue that I might be running into is that I may have hit the lower bound for the program. It may not be designed to work well on so few populations. it gets a lot of its power from the number of populations it has, so the greater the structure it can draw from. \n",
      "\n",
      "Recomputing the covariance each time, we should also be recomputing or restandardizing the environmental variables for each of the sets of populations that Im running. \n",
      "\n",
      "\n",
      "and remake the input files for the bayenv2 because that is not right.  do the coda in R to test the convergence. \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Arlequin"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Lositan"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "August 24\n",
      "\n",
      "wanted to start on the outlier tests while that was running- wend to the lositan website and tried downloading the program, i had to change the java settings to allow the website to run programs. \n",
      "\n",
      "i made a new folder called lositan and put the file that i opened into it, the one from the filtering/16681and80 called \n",
      "16681_80_genepop_one_per_line.txt but i added a title line to it. \n",
      "\n",
      "and \n",
      "\n",
      "in the lositan program it identified that there were 14 populations i checked the neutral mean fst box and got the warning: \"using a neutral mean fst means doing a first simulation run to remove potential selected loci for computin gthe initial mean fst. this effectively doubles thte computation time but it is the recommeneded option if you are not sure what you want, please check this option \n",
      "\n",
      "and for the precise mean fst: simulating a precise mean fst is not possible outside ideal theoretical conditions (infinite populatins, infinite alleles model). SELWB can try to approximate a desired fst by running a biseciton algorithm over repeated simulations. checking this option will increase the computation time by a certain amoutn fo time (in most cases it will double the computation time), this is still the recommended option. If you are not sure what you want, please check this option. \n",
      "\n",
      "so i checked both of the boxes\n",
      "\n",
      "and i left the rest of the default options filled in. the attempted FST is 0.062134\n",
      "confidence interval: 0.95\n",
      "false discovery rate: 0.1\n",
      "expected total pops: 14\n",
      "mutation model: infinite alleles\n",
      "subsample size: 50 \n",
      "\n",
      "i loaded in the population names: \n",
      "AMUR10\n",
      "AMUR11\n",
      "HAYLY09\n",
      "HAYLY10\n",
      "KOPPE91\n",
      "KOPPE96\n",
      "KUSHI06\n",
      "KUSHI07\n",
      "NOME91\n",
      "NOME94\n",
      "SNOH03\n",
      "SNOH96\n",
      "TAUY09\n",
      "TAUY12\n",
      "\n",
      "it finished running and had a popup that said that the simulated fst is 0.057786. i saved the outputs. the colors on the chart are the yellow is balancing selection color, the red is the positive selection color, teh blue is the marker color and the grey is the neutral color. \n",
      "\n",
      "i changed the false discovery rate to 0.05 and ran it again, to see what, if anything changes \n",
      "In the Lositan program, the FDR run with a limit of 0.01 finished running, and the simulated Fst is 0.051289. I saved the output loci list. \n",
      "\n",
      "\n",
      "\n",
      "In the Lositan program, the FDR run with a limit of 0.01 finished running, and the simulated Fst is 0.051289. I saved the output loci list. \n",
      "\n",
      "I want to do more runs, but Im not sure what the best plan is, I want to use the right FDR, so maybe Ill compare them, or look at some papers to see what other people used and I also want to look at the comparisons that im doing, and make some that arent just all the populations together (though i think this is a good way to get putatively neutral snps. \n",
      "\n",
      "I want to run more comparisons in Lositan, I am going to do 4 new ones now: asian odd, asian even, NA odd and NA even. \n",
      "\n",
      "I didnt change the expected number of populations? im not sure what that refers to, because it summarises the data correctly on the side, saying that there are 14 total populations and 4 selected populations. SO thats very confusing to me. I might run a test where I do this next version with that changed and see if it makes a difference. \n",
      "\n",
      "I clicked on it and changed it to 4 and it warned me that a changed parameter would mean that i would have to restart the simulations. Im not sure what that means exactly, but i didnt change anything else and it started running right away on its own. \n",
      "\n",
      "\n",
      "I pulled the results of all the runs on Lositan into an excel workbook, each with their own pages and Im going to compare the three different combinations of populaitons. \n",
      "\n",
      "\n",
      "It looks like there is a pattern, there are two variables, and four factors: the FDR and the # of expected populations. \n",
      "\n",
      "The effect is on the pvalue (siluated Fst< sample Fst) \n",
      "the biggest pvalue comes from the most stringent FDR, the 0.01 and the smallest # of pops (4- also the correct #) and then the 0.01 with 14 pops, the 0.05 with 4 pops and finally the 0.05 with the 14 pops is the smallest value. \n",
      "\n",
      "the effect of the change in these variables is differnt in magnitude depending on the individual locus. in some it seems to be a lot and in others its really not that different. \n",
      "\n",
      "I am rerunning the Asia odd with the expected populations set to 4 \n",
      "\n",
      "And I looked on their site and found this basic R code for plotting the files: \n",
      "\n",
      "#\n",
      "#Bare bones script to read LOSITAN data into R\n",
      "#\n",
      "# USAGE:\n",
      "# The script expects to find:\n",
      "#    a file called loci with locus He/Fst and\n",
      "#    a file called ci with the confidence intervals\n",
      "#\n",
      "# A PNG graphic will be generated called output.png\n",
      "#\n",
      "# The graphic and the tables generated are 'bare bones' in\n",
      "# the sense that they are provided as a STARTING POINT on\n",
      "# how to read the data into R.\n",
      "# User customization of the script is expected.\n",
      "# Feel free to change and use it at your discretion\n",
      "#\n",
      "#(C) 2008 Tiago Antao\n",
      "#This script is free software under the GPL v3\n",
      "plot_ci <- function(f_name, bcolor, mcolor, tcolor) {\n",
      "  cpl <- read.table(f_name, header=TRUE)\n",
      "  lines(cpl[,1],cpl[,2], type='l', col=bcolor)\n",
      "  lines(cpl[,1],cpl[,3], type='l', col=mcolor)\n",
      "  lines(cpl[,1],cpl[,4], type='l', col=tcolor)\n",
      "}\n",
      "\n",
      "plot_loci <- function(f_name, color) {\n",
      "  cpl <- read.table(f_name, header=TRUE, sep='\\t')\n",
      "  points(cpl[,2],cpl[,3], col=color)\n",
      "}\n",
      "\n",
      "\n",
      "png('output.png')\n",
      "plot(-10,ylim=c(0,0.4),xlim=c(0,1), xlab='He', ylab='Fst')\n",
      "plot_ci('ci', 'green', 'black', 'red')\n",
      "plot_loci('loci', 'blue')\n",
      "dev.off()\n",
      "print('The purpose of this script is to provide a Bare bones example on how to read LOSITAN data')\n",
      "print('You should customize it to your needs')\n",
      "\n",
      "I put it in R as a new script called Lositan_plots. \n",
      "\n",
      "I continued to run Lositan on the North American samples. now they are all run I want to compare the list of outliers there to the ones from Arlequin and the ones from Bayescan. \n",
      "  \n",
      "  $$ October 5 2015\n",
      "==================================================================\n",
      "\n",
      "nd hecht says that he used a pairwise correlation between climate variables using the hmisc package in R. \n",
      " \n",
      " I also saw in his paper that I wasnt running the program LOSITAN the way that he had, and I probably needed to account for the smallest of the samplesizes by using it as the subsample size. \n",
      " \n",
      " \n",
      " Lisa said not to worry about that, that because im using a lot of different programs im only looking at where they overlap and so defaults are probably fine because its just wide scale, broad. \n",
      " \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#BayeScEnv"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "September 3 \n",
      "\n",
      "Ok, moved the Bayescan files over to the BayeSCenv file, along with the env data. The individual files are all in order of how thepopulations are in the popinfo file, so I have to do the same thing with the environmental data to match them up if I want to run them, but I may not have to run them all, I need to think about exactly which tests I want. \n",
      "\n",
      "First ill run the whole dataset, to see if I can get the program to work. \n",
      "\n",
      "\n",
      "OK, it looks like its starting to work, I have the 16681_80_bayescan.txt as the input file and stand_lat.txt as the environmental input file and the results are output to the BayeSCenv file. So far Im just using the defaults, there arent any details about the parameters that they used for the Bayenv2, the closest program to this that they ran in pink paper one, but for bayescan they used 50,000 iterations and the default settings, so I just left the default settings for this. its beginning the pilot runs. Its at 100% or cpu- using half and lepmap is splitting the other half between the two ordering commands, and together they are using 80% of thememory, but holding steady. Its probably just going to take a long time. \n",
      "\n",
      "There is a command line version that if this works i might set up to run the rest of the comparisons in the background of someone elses computer. I think ill make the files for it, and maybe plan on running it on Ryans computer over the weekend? \n",
      "\n",
      "Also the manual for the BayeScEnv is really helpful: https://github.com/devillemereuil/bayescenv/wiki/5.-How-to-calibrate-the-MCMC\n",
      "it has a lot of good information regarding the model and what is actually happening. \n",
      "\n",
      "Here are two points: \n",
      "\n",
      "However, you should always check that the acceptance rates displayed in \"out_AccRte.txt\" (where \"out\" is the output prefix of your choice) are between 0.2 and 0.4.\n",
      "\n",
      "The \"trace\" of the successive iterations are stocked in the \"out.sel\" file (where \"out\" is the output prefix of your choice), which can be read in R and transformed into a MCMC object using the \"coda\" package. (I put the code that he gives out into a script in R)\n",
      "\n",
      "\n",
      "We recommend effective sizes of at least the Fst's to be reported in Supplementary Material when using the method.\n",
      "\n",
      "\n",
      "here is the command to run it through the command line with the default parameters at the back. \n",
      "./bayescenv data_codominantSNP.txt -env env.txt -o test -nbp 20 -pilot 5000 -thin 10 -n 5000 -burn 50000 \n",
      "\n",
      "Here are the environmental data that I built the files for each of the runs out of. the single numbers in between the names and the numbers with all the digits are the index numbers of the populations that were used. \n",
      "\n",
      "asia_odd_lat\t\t\t2\t3\t8\t13\t\t-0.3001785\t0.3445463\t-1.6735799\t0.609963\n",
      "asia_even_lat\t\t\t1\t4\t7\t14\t\t-0.3001785\t0.3445463\t-1.6735799\t0.609963\n",
      "NA_odd_lat\t\t\t5\t9\t11\t\t\t0.7452322\t1.2606949\t-0.986678\t\n",
      "NA_even_lat\t\t\t6\t10\t12\t\t\t0.7452322\t1.2606949\t-0.986678\t\n",
      "\n",
      "\n",
      "\n",
      "asia_odd_long\t\t\t2\t3\t8\t13\t\t0.7767843\t0.9189846\t0.7993947\t0.8294541\n",
      "asia_even_long\t\t\t1\t4\t7\t14\t\t0.7767843\t0.9189846\t0.7993947\t0.8294541\n",
      "NA_odd_long\t\t\t5\t9\t11\t\t\t-1.1177044\t-1.2458492\t-0.9610641\t\n",
      "NA_even_long\t\t\t6\t10\t12\t\t\t-1.1177044\t-1.2458492\t-0.9610641\t\n",
      "\n",
      "This is what Im going to run on Ryans computer over the long weekend: \n",
      "\n",
      "./bayescenv 16681_80_bayescan.txt -env stand_lat.txt -o out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >16681_80_bayescan_log.txt 2>&1\n",
      "./bayescenv asia_odd_bayescan.txt -env asia_odd_lat.txt -o out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >asia_odd_bayescan_log.txt 2>&1\n",
      "./bayescenv asia_even_bayescan.txt -env asia_even_lat.txt -o out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >asia_even_bayescan_log.txt 2>&1\n",
      "./bayescenv NA_odd_bayescan.txt -env NA_odd_lat.txt\t-o out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NA_odd_bayescan_log.txt 2>&1\n",
      "./bayescenv NA_even_bayescan.txt -env NA_even_lat.txt -o out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NA_even_bayescan_log.txt 2>&1\n",
      "\n",
      "./bayescenv 16681_80_bayescan.txt -env stand_long.txt -o out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >16681_80_bayescan_log.txt 2>&1\n",
      "./bayescenv asia_odd_bayescan.txt -env asia_odd_long.txt -o out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >asia_odd_bayescan_log.txt 2>&1\n",
      "./bayescenv asia_even_bayescan.txt -env asia_even_long.txt -o out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >asia_even_bayescan_log.txt 2>&1\n",
      "./bayescenv NA_odd_bayescan.txt -env NA_odd_long.txt -o out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NA_odd_bayescan_log.txt 2>&1\n",
      "./bayescenv NA_even_bayescan.txt -env NA_even_long.txt -o out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NA_even_bayescan_log.txt 2>&1\n",
      "\n",
      "\n",
      "Sep 8 \n",
      "\n",
      "I came in this morning and realized that I had not actually specified the output files for the bayesenv runs, though they had all run on ryans computer because the logs were there. So the last one to run, and the only one that i actually got any results for was: NA_even_bayescan_log.txt \n",
      "\n",
      "So I remade the batch file to have the proper output file names and restarted it on Garretts computer. I also got rid of this one: Y:\\WORK\\TARPEY\\BayeScEnv\\bin\\win32\\bayescenv.exe  16681_80_bayescan.txt -env stand_lat.txt -o out_16681_80_bayescan.txt -threads 5 -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >16681_80_bayescan_log.txt 2>&1\n",
      "because it had already finished running on my computer the week before. \n",
      "\n",
      "\n",
      "So here is the new batch file: \n",
      "\n",
      "Y:\\WORK\\TARPEY\\BayeScEnv\\bin\\win32\\bayescenv.exe asia_odd_bayescan.txt -env asia_odd_lat.txt -threads 5 -o out_asia_odd_bayescan.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >asia_odd_bayescan_log.txt 2>&1\n",
      "Y:\\WORK\\TARPEY\\BayeScEnv\\bin\\win32\\bayescenv.exe  asia_even_bayescan.txt -env asia_even_lat.txt -threads 5 -o out_asia_even_bayescan.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >asia_even_bayescan_log.txt 2>&1\n",
      "Y:\\WORK\\TARPEY\\BayeScEnv\\bin\\win32\\bayescenv.exe  NA_odd_bayescan.txt -env NA_odd_lat.txt -threads 5 -o out_NA_odd_bayescan.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NA_odd_bayescan_log.txt 2>&1\n",
      "\n",
      "It seems to be running OK. \n",
      "\n",
      "\n",
      "         $$ September 9 2015\n",
      "==================================================================\n",
      "\n",
      "The Bayescenv runs finished on Garretts computer, so now I have all those runs done for the latitude. \n",
      "\n",
      "Sep 23\n",
      "\n",
      "#BayeScEnv_Sep15.bat\n",
      "\n",
      "Y:\\WORK\\TARPEY\\BayeScEnv\\bin\\win32\\bayescenv.exe asia_odd_bayescan.txt -env asia_odd_long.txt -threads 5 -out out_asia_odd_long.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >asia_odd_long_log.txt 2>&1\n",
      "Y:\\WORK\\TARPEY\\BayeScEnv\\bin\\win32\\bayescenv.exe  asia_even_bayescan.txt -env asia_even_long.txt -threads 5 -out out_asia_even_long.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >asia_even_long_log.txt 2>&1\n",
      "Y:\\WORK\\TARPEY\\BayeScEnv\\bin\\win32\\bayescenv.exe  NA_odd_bayescan.txt -env NA_odd_long.txt -threads 5 -out out_NA_odd_long.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NA_odd_long_log.txt 2>&1\n",
      "Y:\\WORK\\TARPEY\\BayeScEnv\\bin\\win32\\bayescenv.exe  NA_even_bayescan.txt -env NA_even_long.txt -threads 5 -out out_NA_even_long.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NA_even_long_log.txt 2>&1\n",
      "Y:\\WORK\\TARPEY\\BayeScEnv\\bin\\win32\\bayescenv.exe  16681_80_bayescan.txt -env stand_long.txt -out out_16681_80_long.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >16681_80__long_log.txt 2>&1\n",
      "\n",
      "\n",
      "\n",
      "         $$ October 12  2015\n",
      "==================================================================\n",
      "\n",
      "The BayeScEnv finished running on Saturday morning, so thats good news. Its done the latitude and longitude. \n",
      "\n",
      "\n",
      "I was looking back at the results of the previous bayescenv runs and they dont really follow the instructions on the manual, that sayus that in order to calibarate the pilot runs and yield informative results, the acceptance rates in the _AccRte.txt file should be between 0.2 and 0.4\n",
      " \n",
      " So there are two columns in that txt file, the first is labeled beta and the second is labled ances. \n",
      " \n",
      " There have been 5 runs of this program with the lat and the long file. \n",
      " \n",
      " Oct 18 \n",
      "    \n",
      " \n",
      " UGH I Cant figure out what to do with the bayescenv files, I dont think that they ran correctly, and Im not sure how to fix them except to start over and try again. \n",
      " \n",
      " This is the batch file that I made: it could take a long time to run. \n",
      " \n",
      " \n",
      ".\\bayescenv.exe asia_odd_bayescan.txt -env AS_LAT.env -out ASO_LAT_out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >ASO_LAT_out_log.txt 2>&1\n",
      ".\\bayescenv.exe asia_even_bayescan.txt -env AS_LAT.env -out ASE_LAT_out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >ASE_LAT_out_log.txt 2>&1\n",
      ".\\bayescenv.exe NA_odd_bayescan.txt -env NA_LAT.env -out NAO_LAT_out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NAO_LAT_out_log.txt 2>&1\n",
      ".\\bayescenv.exe NA_even_bayescan.txt -env NA_LAT.env -out NAE_LAT_out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NAE_LAT_out_log.txt 2>&1\n",
      ".\\bayescenv.exe 16681_80_bayescan.txt -env ALL_LAT.env -out 16681_LAT_out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >16681_LAT_out_log.txt 2>&1\n",
      "\n",
      ".\\bayescenv.exe asia_odd_bayescan.txt -env AS_LONG.env -out ASO_LONG_out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >ASO_LONG_out_log.txt 2>&1\n",
      ".\\bayescenv.exe asia_even_bayescan.txt -env AS_LONG.env -out ASE_LONG_out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >ASE_LONG_out_log.txt 2>&1\n",
      ".\\bayescenv.exe NA_odd_bayescan.txt -env NA_LONG.env -out NAO_LONG_out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NAO_LONG_out_log.txt 2>&1\n",
      ".\\bayescenv.exe NA_even_bayescan.txt -env NA_LONG.env -out NAE_LONG_out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NAE_LONG_out_log.txt 2>&1\n",
      ".\\bayescenv.exe 16681_80_bayescan.txt -env ALL_LONG.env -out 16681_LONG_out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >16681_LONG_out_log.txt 2>&1\n",
      "\n",
      ".\\bayescenv.exe asia_odd_bayescan.txt -env AS_PT1.env -out ASO_PT1_out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >ASO_PT1_out_log.txt 2>&1\n",
      ".\\bayescenv.exe asia_even_bayescan.txt -env AS_PT1.env -out ASE_PT1_out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >ASE_PT1_out_log.txt 2>&1\n",
      ".\\bayescenv.exe NA_odd_bayescan.txt -env NA_PT1.env -out NAO_PT1_out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NAO_PT1_out_log.txt 2>&1\n",
      ".\\bayescenv.exe NA_even_bayescan.txt -env NA_PT1.env -out NAE_PT1_out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NAE_PT1_out_log.txt 2>&1\n",
      ".\\bayescenv.exe 16681_80_bayescan.txt -env ALL_PT1.env -out 16681_PT1_out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >16681_PT1_out_log.txt 2>&1\n",
      "\n",
      ".\\bayescenv.exe asia_odd_bayescan.txt -env AS_PT2.env -out ASO_PT2_out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >ASO_PT2_out_log.txt 2>&1\n",
      ".\\bayescenv.exe asia_even_bayescan.txt -env AS_PT2.env -out ASE_PT2_out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >ASE_PT2_out_log.txt 2>&1\n",
      ".\\bayescenv.exe NA_odd_bayescan.txt -env NA_PT2.env -out NAO_PT2_out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NAO_PT2_out_log.txt 2>&1\n",
      ".\\bayescenv.exe NA_even_bayescan.txt -env NA_PT2.env -out NAE_PT2_out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NAE_PT2_out_log.txt 2>&1\n",
      ".\\bayescenv.exe 16681_80_bayescan.txt -env ALL_PT2.env -out 16681_PT2_out -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >16681_PT2_out_log.txt 2>&1\n",
      "\n",
      "\n",
      "          $$ October 27  2015\n",
      "==================================================================\n",
      "\n",
      "\n",
      "Talked to ryan about a host of issues. \n",
      "\n",
      "\n",
      "BAYESCENV\n",
      "calibrating the pilot runs: \n",
      "\t1. what are the beta and ances mentioned at the top of the file? \n",
      "\t2. are my acceptance rates between .2 and .4? \n",
      "General BAYES questions: \n",
      "When running programs and trying to up the number of iterations is it better to increase the length of a run or add more runs? \n",
      "\n",
      "\n",
      "\n",
      "the beta he thinks means the correlation with the variable\n",
      "the ances is like an ancestry thing, and has to do with the variation that is attributed to the structure of the populations, or the covariance matrix, so the non environmental\n",
      "\n",
      "when you are increasing the length of the runs, the best way to do it is to increase the length of the runs, so add iterations to the runs instead of add more runs. that will let them run longer and get closer to convergence. \n",
      "\n",
      "he isnt sure what the two columns mean, but he said that its a good sign that they are steadily decreasing but it might not be ok that they havent plateaued or reached a steady state. So he doesnt know what the instructions about the .2 or .4 mean in the context but these runs were probably not run long enough to reach convergence, though they certainly seem to be going in the right direction. Also he said that if I have a basic question that I should think about emailing the developer. If its basic enough he might answer. there are instructions for how to look at the convergence, doing a trace in R with the program Coda etc, and that i should do that and look at what its like. Maybe they are reaching convergence and theres no good obvious answer for what is happening in the two columns? \n",
      "So at the end of it. i need to redo the standardization of the environmental variables,\n",
      "And increase the number of iterations per run for the bayescenv- it doesnt seem to be getting to the convergence. do the coda in R to test the convergence. \n",
      "\n",
      "\n",
      "to bayescenv- trying to figure out if the acceptance rates that I have are ok, but skipping the step where i look because I dont know how to tell, and actually just going to the part where I plot them in R with coda to see if I have reached convergence. \n",
      "\n",
      "https://github.com/devillemereuil/bayescenv/wiki/5.-How-to-calibrate-the-MCMC\n",
      "\n",
      "\n",
      "### Check whether the MCMC has reached convergence\n",
      "###    using the package coda, and code from: https://github.com/devillemereuil/bayescenv/wiki/5.-How-to-calibrate-the-MCMC\n",
      "### Carolyn Tarpey | October 2015 \n",
      "### ---------------------------------------\n",
      "\n",
      "\n",
      "install.packages(\"coda\")\n",
      "\n",
      "library(coda)\n",
      "\n",
      "\n",
      "#set the working directory \n",
      "setwd(\"G:/Analysis/Pop_analysis/Populations_b3_may/BayeScEnv\")\n",
      "\n",
      "### 16681_80_bayescan.sel\n",
      "\n",
      "chain_16681 <-read.table(\"16681_80_bayescan.sel\",header=TRUE)\n",
      "chain_16681 <-mcmc(chain_16681, thin=10)     #Adapt thin to its actual value (10 is the default)\n",
      "heidel.diag(chain_16681)             #To test for convergence\n",
      "effectiveSize(chain_16681)           #To compute effective sample size\n",
      "autocorr.diag(chain_16681)           #To look for auto-correlation\n",
      "plot(chain_16681)                    #To plot the \"trace\" and the posterior distribution\n",
      "\n",
      "### NA_even_bayescan.sel\n",
      "\n",
      "chain_NAE <-read.table(\"NA_even_bayescan.sel\",header=TRUE)\n",
      "chain_NAE <-mcmc(chain_NAE, thin=10)     #Adapt thin to its actual value (10 is the default)\n",
      "heidel.diag(chain_NAE)             #To test for convergence\n",
      "effectiveSize(chain_NAE)           #To compute effective sample size\n",
      "autocorr.diag(chain_NAE)           #To look for auto-correlation\n",
      "plot(chain_NAE)                    #To plot the \"trace\" and the posterior distribution\n",
      "\n",
      "### NA_odd_bayescan.sel\n",
      "\n",
      "chain_NAO <-read.table(\"NA_odd_bayescan.sel\",header=TRUE)\n",
      "chain_NAO <-mcmc(chain_NAO, thin=10)     #Adapt thin to its actual value (10 is the default)\n",
      "heidel.diag(chain_NAO)             #To test for convergence\n",
      "effectiveSize(chain_NAO)           #To compute effective sample size\n",
      "autocorr.diag(chain_NAO)           #To look for auto-correlation\n",
      "plot(chain_NAO)                    #To plot the \"trace\" and the posterior distribution\n",
      "\n",
      "### asia_even_bayescan.sel\n",
      "\n",
      "chain_ASE <-read.table(\"asia_even_bayescan.sel\",header=TRUE)\n",
      "chain_ASE <-mcmc(chain_ASE ,thin=10)     #Adapt thin to its actual value (10 is the default)\n",
      "heidel.diag(chain_ASE)             #To test for convergence\n",
      "effectiveSize(chain_ASE)           #To compute effective sample size\n",
      "autocorr.diag(chain_ASE)           #To look for auto-correlation\n",
      "plot(chain_ASE)                    #To plot the \"trace\" and the posterior distribution\n",
      "\n",
      "### asia_odd_bayescan.sel\n",
      "\n",
      "chain_ASO <-read.table(\"asia_odd_bayescan.sel\",header=TRUE)\n",
      "chain_ASO <-mcmc(chain_ASO ,thin=10)     #Adapt thin to its actual value (10 is the default)\n",
      "heidel.diag(chain_ASO)             #To test for convergence\n",
      "effectiveSize(chain_ASO)           #To compute effective sample size\n",
      "autocorr.diag(chain_ASO)           #To look for auto-correlation\n",
      "plot(chain_ASO)                    #To plot the \"trace\" and the posterior distribution\n",
      "\n",
      "\n",
      "These are the results:\n",
      "\n",
      "> setwd(\"G:/Analysis/Pop_analysis/Populations_b3_may/BayeScEnv\")\n",
      "\n",
      "\n",
      "> chain_16681 <-read.table(\"16681_80_bayescan.sel\",header=TRUE)\n",
      "> chain_16681 <-mcmc(chain_16681 ,thin=10)     #Adapt thin to its actual value (10 is the default)\n",
      "> heidel.diag(chain_16681)             #To test for convergence\n",
      "                                    \n",
      "      Stationarity start     p-value\n",
      "      test         iteration        \n",
      "Iter  failed        NA           NA \n",
      "logL  passed         1       0.6950 \n",
      "Fst1  passed         1       0.8364 \n",
      "Fst2  passed         1       0.6883 \n",
      "Fst3  passed         1       0.6946 \n",
      "Fst4  passed         1       0.7177 \n",
      "Fst5  passed       501       0.0543 \n",
      "Fst6  passed         1       0.7774 \n",
      "Fst7  passed         1       0.1122 \n",
      "Fst8  passed         1       0.1052 \n",
      "Fst9  passed         1       0.9889 \n",
      "Fst10 passed         1       0.8628 \n",
      "Fst11 passed         1       0.0892 \n",
      "Fst12 passed         1       0.7511 \n",
      "Fst13 passed         1       0.8116 \n",
      "Fst14 passed         1       0.6060 \n",
      "                                   \n",
      "      Halfwidth Mean      Halfwidth\n",
      "      test                         \n",
      "Iter  <NA>             NA       NA \n",
      "logL  passed    -5.13e+05 5.05e+00 \n",
      "Fst1  passed     5.07e-02 2.97e-05 \n",
      "Fst2  passed     2.40e-02 2.21e-05 \n",
      "Fst3  passed     2.14e-02 2.08e-05 \n",
      "Fst4  passed     5.47e-02 3.51e-05 \n",
      "Fst5  passed     6.99e-02 3.55e-05 \n",
      "Fst6  passed     8.29e-02 4.36e-05 \n",
      "Fst7  passed     8.15e-02 3.98e-05 \n",
      "Fst8  passed     4.96e-02 2.98e-05 \n",
      "Fst9  passed     2.18e-02 2.21e-05 \n",
      "Fst10 passed     6.09e-02 4.00e-05 \n",
      "Fst11 passed     1.46e-01 6.08e-05 \n",
      "Fst12 passed     1.20e-01 5.33e-05 \n",
      "Fst13 passed     2.20e-02 2.20e-05 \n",
      "Fst14 passed     5.10e-02 3.22e-05 \n",
      "\n",
      "\n",
      "> effectiveSize(chain_16681)           #To compute effective sample size\n",
      "    Iter     logL     Fst1     Fst2     Fst3     Fst4     Fst5     Fst6     Fst7     Fst8     Fst9   Fst10    Fst11    Fst12    Fst13    Fst14 \n",
      "   0.000 2680.730 3486.337 2816.921 2808.690 3049.141 4288.851 3602.023 3743.843 3311.436 3055.559   3527.474 4023.342 4193.153 2810.235 3363.913\n",
      "\n",
      "\n",
      "\n",
      "> autocorr.diag(chain_16681)           #To look for auto-correlation\n",
      "             Iter         logL        Fst1         Fst2         Fst3        Fst4        Fst5       Fst6         Fst7         Fst8         Fst9        Fst10        Fst11        Fst12    Fst13        Fst14\n",
      "Lag 0   1.0000000  1.000000000 1.000000000  1.000000000  1.000000000 1.000000000 1.000000000  1.000000000  1.000000000  1.000000000  1.000000000  1.000000000  1.000000000  1.000000000  1.000000000  1.000000000\n",
      "Lag 10  0.9993999  0.203075959 0.137381894  0.207437318  0.226476093 0.159784698 0.076360490  0.110748138  0.143465925  0.151334693  0.193029453  0.137916228  0.108039087  0.087567583  0.205124317  0.132901445\n",
      "Lag 50  0.9969994  0.002491573 0.007357094  0.021197866  0.007996474 0.006693156 0.011191450 -0.016902719 -0.007761888  0.004310425  0.011689438 -0.016370899 -0.008531449  0.013982392  0.007435516  0.011494994\n",
      "Lag 100 0.9939988 -0.002780925 0.007080503 -0.014579255 -0.011326658 0.014043647 0.012038672  0.002185866 -0.001858880 -0.006620472 -0.001598554 -0.017215710  0.019290685  0.006978431  0.016497166 -0.004957534\n",
      "Lag 500 0.9699960 -0.005492100 0.027893915 -0.004382463 -0.010277262 0.003387921 0.002425106  0.020133634  0.006855558 -0.007160937  0.005565154 -0.004661507  0.013566505 -0.007772530 -0.013759333 -0.014639419\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> chain_NAE <-read.table(\"NA_even_bayescan.sel\",header=TRUE)\n",
      "> chain_NAE <-mcmc(chain_NAE, thin=10)     #Adapt thin to its actual value (10 is the default)\n",
      "> heidel.diag(chain_NAE)             #To test for convergence\n",
      "                                   \n",
      "     Stationarity start     p-value\n",
      "     test         iteration        \n",
      "Iter failed         NA          NA \n",
      "logL passed          1      0.1865 \n",
      "Fst1 passed          1      0.4132 \n",
      "Fst2 passed          1      0.3811 \n",
      "Fst3 passed       1001      0.0543 \n",
      "                                   \n",
      "     Halfwidth Mean      Halfwidth\n",
      "     test                         \n",
      "Iter <NA>             NA       NA \n",
      "logL passed    -9.38e+04 8.16e+00 \n",
      "Fst1 passed     3.96e-02 5.50e-05 \n",
      "Fst2 passed     5.52e-02 5.61e-05 \n",
      "Fst3 passed     7.44e-02 7.11e-05 \n",
      "\n",
      "\n",
      "> effectiveSize(chain_NAE)           #To compute effective sample size\n",
      "    Iter     logL     Fst1     Fst2     Fst3 \n",
      "   0.000 1265.716 1889.918 2401.221 2364.421 \n",
      "   \n",
      "> autocorr.diag(chain_NAE)           #To look for auto-correlation\n",
      "             Iter        logL         Fst1       Fst2        Fst3\n",
      "Lag 0   1.0000000  1.00000000  1.000000000 1.00000000 1.000000000\n",
      "Lag 10  0.9993999  0.49332356  0.391244882 0.26193295 0.280934086\n",
      "Lag 50  0.9969994  0.10226455  0.026328447 0.03550004 0.018988467\n",
      "Lag 100 0.9939988  0.02767923 -0.001010564 0.00215506 0.006419806\n",
      "Lag 500 0.9699960 -0.01864821 -0.019293639 0.01066389 0.012390068\n",
      "\n",
      "\n",
      "\n",
      "> chain_NAO <-read.table(\"NA_odd_bayescan.sel\",header=TRUE)\n",
      "> chain_NAO <-mcmc(chain_NAO, thin=10)     #Adapt thin to its actual value (10 is the default)\n",
      "> heidel.diag(chain_NAO)             #To test for convergence\n",
      "                                   \n",
      "     Stationarity start     p-value\n",
      "     test         iteration        \n",
      "Iter failed       NA            NA \n",
      "logL passed        1        0.3949 \n",
      "Fst1 passed        1        0.1340 \n",
      "Fst2 passed        1        0.7236 \n",
      "Fst3 passed        1        0.0667 \n",
      "                                  \n",
      "     Halfwidth Mean      Halfwidth\n",
      "     test                         \n",
      "Iter <NA>             NA       NA \n",
      "logL passed    -1.06e+05 7.50e+00 \n",
      "Fst1 passed     5.20e-02 6.14e-05 \n",
      "Fst2 passed     4.99e-02 5.45e-05 \n",
      "Fst3 passed     1.23e-01 7.87e-05 \n",
      "\n",
      "\n",
      "> effectiveSize(chain_NAO)           #To compute effective sample size\n",
      "    Iter     logL     Fst1     Fst2     Fst3 \n",
      "   0.000 1410.830 1859.693 1997.089 2803.935 \n",
      "   \n",
      "> autocorr.diag(chain_NAO)           #To look for auto-correlation\n",
      "             Iter         logL         Fst1        Fst2        Fst3\n",
      "Lag 0   1.0000000  1.000000000  1.000000000 1.000000000  1.00000000\n",
      "Lag 10  0.9993999  0.489099728  0.371123355 0.339880282  0.22084625\n",
      "Lag 50  0.9969994  0.073779919  0.024298106 0.042282189  0.01691076\n",
      "Lag 100 0.9939988  0.014452505 -0.004130594 0.017762678  0.01254567\n",
      "Lag 500 0.9699960 -0.001702757 -0.014337181 0.007602119 -0.02884206\n",
      "\n",
      "\n",
      "> chain_ASE <-read.table(\"asia_even_bayescan.sel\",header=TRUE)\n",
      "> chain_ASE <-mcmc(chain_ASE ,thin=10)     #Adapt thin to its actual value (10 is the default)\n",
      "> heidel.diag(chain_ASE)             #To test for convergence\n",
      "                                   \n",
      "     Stationarity start     p-value\n",
      "     test         iteration        \n",
      "Iter failed        NA           NA \n",
      "logL passed         1       0.0749 \n",
      "Fst1 passed         1       0.7549 \n",
      "Fst2 passed       501       0.1942 \n",
      "Fst3 passed         1       0.4883 \n",
      "Fst4 passed         1       0.2275 \n",
      "                                  \n",
      "     Halfwidth Mean      Halfwidth\n",
      "     test                         \n",
      "Iter <NA>             NA       NA \n",
      "logL passed    -1.25e+05 8.06e+00 \n",
      "Fst1 passed     5.09e-03 1.41e-05 \n",
      "Fst2 passed     1.47e-02 2.43e-05 \n",
      "Fst3 passed     3.16e-02 2.39e-05 \n",
      "Fst4 passed     7.94e-03 1.74e-05 \n",
      "\n",
      "> effectiveSize(chain_ASE)           #To compute effective sample size\n",
      "    Iter     logL     Fst1     Fst2     Fst3     Fst4 \n",
      "   0.000  963.803 2306.599 1501.853 3380.094 2285.886 \n",
      "   \n",
      "   \n",
      "> autocorr.diag(chain_ASE)           #To look for auto-correlation\n",
      "             Iter        logL         Fst1        Fst2          Fst3        Fst4\n",
      "Lag 0   1.0000000  1.00000000  1.000000000  1.00000000  1.0000000000  1.00000000\n",
      "Lag 10  0.9993999  0.43715007  0.310163436  0.27637469  0.1644703270  0.26121172\n",
      "Lag 50  0.9969994  0.09393317 -0.009081315  0.09008163 -0.0003398159  0.02209232\n",
      "Lag 100 0.9939988  0.09752206 -0.019605112  0.06463454  0.0204274331  0.01297151\n",
      "Lag 500 0.9699960 -0.01544757 -0.002718811 -0.01426701  0.0136205273 -0.01728628\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> chain_ASO <-read.table(\"asia_odd_bayescan.sel\",header=TRUE)\n",
      "> chain_ASO <-mcmc(chain_ASO ,thin=10)     #Adapt thin to its actual value (10 is the default)\n",
      "> heidel.diag(chain_ASO)             #To test for convergence\n",
      "                                   \n",
      "     Stationarity start     p-value\n",
      "     test         iteration        \n",
      "Iter failed       NA           NA  \n",
      "logL passed        1        0.381  \n",
      "Fst1 passed        1        0.355  \n",
      "Fst2 passed        1        0.591  \n",
      "Fst3 passed        1        0.627  \n",
      "Fst4 passed        1        0.850  \n",
      "                                  \n",
      "     Halfwidth Mean      Halfwidth\n",
      "     test                         \n",
      "Iter <NA>             NA       NA \n",
      "logL passed    -1.31e+05 7.50e+00 \n",
      "Fst1 passed     2.24e-03 1.17e-05 \n",
      "Fst2 passed     6.98e-03 1.80e-05 \n",
      "Fst3 passed     2.56e-02 1.94e-05 \n",
      "Fst4 passed     4.44e-03 1.45e-05 \n",
      "\n",
      "> effectiveSize(chain_ASO)           #To compute effective sample size\n",
      "    Iter     logL     Fst1     Fst2     Fst3     Fst4 \n",
      "   0.000 1017.142 1611.504 1526.173 3766.636 2034.358 \n",
      "   \n",
      "> autocorr.diag(chain_ASO)           #To look for auto-correlation\n",
      "             Iter       logL         Fst1        Fst2        Fst3        Fst4\n",
      "Lag 0   1.0000000 1.00000000  1.000000000  1.00000000  1.00000000 1.000000000\n",
      "Lag 10  0.9993999 0.45701381  0.413129743  0.35287801  0.13856561 0.290415919\n",
      "Lag 50  0.9969994 0.11654447  0.068301253  0.10033099 -0.01670955 0.048854526\n",
      "Lag 100 0.9939988 0.08042511  0.022772941  0.04302075  0.02812568 0.017050368\n",
      "Lag 500 0.9699960 0.01471647 -0.005711867 -0.01162968 -0.01763041 0.005880016\n",
      "\n",
      "\n",
      "\n",
      "Im not sure how to interpret these results. But theres this little tidbit in the manual: \n",
      "Also, since the parameters g and alpha are submitted to the Reversible Jump, many iterations are set to 0 in the chain, which can inflate auto-correlation. Null values can be removed to study to the chain when the parameters are actually included in the model by using:\n",
      "\n",
      ">effectiveSize(chain[chain[,'g256']!=0,'g256'])\n",
      "\n",
      "Note that performing these diagnostics might be daunting, but they are essential for the method to be trusted! We recommend effective sizes of at least the Fst's to be reported in Supplementary Material when using the method.\n",
      "\n",
      "and I dont know what to make of that either. SO. great start. I saved all the plots, and I rember that I have seen something like it in the course materials from the first year of the SISG courses. Ill look back through the handouts to see if Im doing it right. \n",
      "\n",
      "this is what this website says about it http://www.johnmyleswhite.com/notebook/2010/08/29/mcmc-diagnostics-in-r-with-the-coda-package/: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In this image, you can see that the trajectory of the chain is consistent over time and that its distribution looks appropriately normal. So the first takeaway message is simple: check the traces and distributions of your variables using plot() to make sure that they are reasonable and don\u2019t indicate clear deficiencies in the length of your adaptation period. When you know the distributional form of your posteriors, this is particularly effective, as in our example here, where we know to expect normal distributions.\n",
      "\n",
      "Oct 29 \n",
      "\n",
      "\n",
      "i dont think that the output of the bayescenv is really working- i think that the log is different each time but the actual output files are getting overwritten. which sucks. so i have a snapshot of the first four or so runs and I have a copy of right now, so maybe I can piece together somehtign and figure out what worked and what didnt and then get them started on separate computers or something like that so the problem doesnt happen again. \n",
      "\n",
      "The good news is that I remember that when you dont mess with the length of the iterations, it runs really fast, like all of them in three days fast on that computer over there, so that i could figure this out tomorrow and restart them. \n",
      "\n",
      "I may need to anyway, thinking about how the values need to be standardized individually. so thats cool\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "         $$ October 30  2015\n",
      "==================================================================\n",
      "\n",
      "just looked back at the code for testing the convergence of the MCMC chain for the bayescenv results and using some tips i found at this website: https://theoreticalecology.wordpress.com/2011/12/09/mcmc-chain-analysis-and-convergence-diagnostics-with-coda-in-r/ \n",
      "Im pretty sure that it looks good, like there was convergence. \n",
      "\n",
      "they really recommend doing some other things to tell for sure, but I cant figure out what they are and Im pretty sure they need more than one chain to complete, and i dont have more than one chain, i think. \n",
      "\n",
      "i tried this: \n",
      "pairs(data.frame(chain_16681))\n",
      "\n",
      "and it worked, like R ran and it finished and there were no errors, but it didnt print anything and im not sure what it does? So i assigned it into a variable and am going to look at the variable when its done running. x <- pairs(data.frame(chain_16681)) it doesnt know. wait jk it did work but i have no idea what the plots are showing. it makes absolutely no sense to me. at all. so im moving on. ok ok i read more: \n",
      "\n",
      "\t\"Marginal densities are an average over the values a parameter takes with all other parameters \u201cmarginalized\u201d, i.e. other parameters having any values according to their posterior probabilities. Often, marginal densities are treated as the main output of a Bayesian analysis (e.g. by reporting their mean and sd of), but I strongly advice against this practice without further analysis. The reason is that marginal densities \u201chide\u201d correlations between parameters, and if there are correlations, parameter uncertainties appear to be much greater in the marginals that they actually are. To check for pairwise correlations is quite easy \u2013 just use pairs on the MCMC chain: pairs(data.frame(chain_16681)) \"\n",
      "\n",
      "Since we dont have anything but tons of huge blobs, i dont think that is evidence of correlation. if they were correlated there would be distinct patterns- lines up or down or whatever. \n",
      "\n",
      "Other things i want to look into: \n",
      "\t\n",
      "\trestandardizing the env variables for LFMM? \n",
      "\trestandardizing the bayescenv env factors?\n",
      "\trestandardizing the bayenv env factors - def have to do. \n",
      "\tlook at the input files for bayenv2- theyre not right, the covariance matrix is whack. \n",
      "\t\n",
      "So the Bayescenv runs that im checking the convergence is the one that was the very first- the early october ones. \n",
      "\n",
      "When i look at the doubled ones, they look really good too- but even cleaner. \n",
      "\n",
      "So I think the defaults are fine. \n",
      "\n",
      "this is what the Bayescenv manual says about the environmental variables: \n",
      "\n",
      "Environmental data\n",
      "\n",
      "As explained in the related article, the environmental variable must be passed to the software in the form of an environmental differentiation. Thus, it should be computed as a contrast to a reference (usually the average environment, but not obligatory), and as a distance. BayeScEnv only makes sure this latter point is satisfied by taking the absolute value of the environmental variable. It is also strongly advised to standardise the environmental values (i.e. dividing by the standard deviation) so that the extreme values never are much bigger than 2 or 3.\n",
      "\n",
      "Regarding the input format, a value for each population must be provided in a separate text file and in one row, so that the content of the file must look like (16 populations in this example):\n",
      "\n",
      " -0.639230879683183 -0.995733945140612 -0.263228407909816 0.478694676547254 1.35474025903925 1.85405875486584 0.837131353114318 0.104061927522223 -0.103841631873915 -0.760901719688793 -1.82819181353382 -1.33847104010839 -0.495277642473072 0.216938433143385 0.954037642263244 0.625214033916083\n",
      "\n",
      " \n",
      " this is what the related article says: \n",
      " \n",
      " Implementation of the statistical model\n",
      "Our method uses two types of data: (i) the allele counts a for each locus in each population sample, and (ii) observed values E of an environmental variable (one value per population), which are transformed into environmental differentiation using an appropriate function. We chose the absolute-value distance, because it allows to weigh down the effect of outlier (i.e. strongly differentiated) environmental values and, therefore, makes the method more conservative. Note that measuring an environmental distance requires to define a reference. The most natural reference would be the average of the environmental values, but this would not be always the case (see the example of adaptation to altitude in humans presented below). Also, it is strongly advised to standardise the environmental values by dividing by the standard deviation, in order to avoid effect size issues regarding the inference of the parameter g. As stated in the previous section, there are three alternative models:\n",
      "\n",
      "M1 Neutral model: Bj \n",
      "M2 Local adaptation model with environmental differentiation Ej : Bj + giEj \\\n",
      "M3 Locus-specific model: ai + Bj\n",
      "\n",
      "\n",
      "##########################################################\n",
      "\n",
      "REstandardizing the environmental variables for the Bayescenv program \n",
      "\n",
      "\n",
      "i also think that im going to rename the input files for each of the runs so that the resulting output files are all different names, and there isnt the risk of them getting over written. \n",
      "\n",
      "\n",
      "\t\t\t\tPC1_temp_precip\tPC2_temp_precip\tadjusted_X\t\tY\n",
      "all\t\tKushiro\t-3.218451422\t-0.516398134\t144.378062\t42.980725\n",
      "\t\tAmur\t1.689167829\t\t-1.473602978\t139.727554\t52.938669\n",
      "\t\tTauy\t3.889188789\t\t-0.049769325\t148.929489\t59.714924\n",
      "\t\tHaylyluya\t2.205190255\t0.100932262\t\t162.485706\t57.769904\n",
      "\t\tNome\t3.861457466\t\t0.374320184\t\t194.6983\t64.4836\n",
      "\t\tKoppen-3.471874259\t\t4.791198571\t\t214.1013\t60.7062\n",
      "\t\tSnohomish-4.954678659\t-3.226680581\t237.8189\t48.014461\n",
      "\t\t\t\t\t\n",
      "ASE/ASO\tKushiro\t-3.218451422\t-0.516398134\t144.378062\t42.980725\n",
      "\t\tAmur\t1.689167829\t\t-1.473602978\t139.727554\t52.938669\n",
      "\t\tTauy\t3.889188789\t\t-0.049769325\t148.929489\t59.714924\n",
      "\t\tHaylyluya\t2.205190255\t\t0.100932262\t162.485706\t57.769904\n",
      "\t\t\t\t\t\t\n",
      "NAE/ASO\tNome\t3.861457466\t\t0.374320184\t\t194.6983\t64.4836\n",
      "\t\tKoppen\t-3.471874259\t4.791198571\t\t214.1013\t60.7062\n",
      "\t\tSnohomish-4.954678659\t-3.226680581\t237.8189\t48.014461\n",
      "\t\t\t\t\t\n",
      "added this section to the code to get the standardized values\n",
      "\n",
      "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "#  O C T O B E R 3 0\n",
      "\n",
      "all_env <- read.table (\"G:/Analysis/Pop_analysis/Populations_b3_may/EnvData/RestandBayescenv/all.txt\", header = FALSE, sep = \"\\t\")\n",
      "all_env <- as.data.frame(all_env) \n",
      "colsall_env <- c(names(all_env))\n",
      "all_env.dat <- scale(all_env)\n",
      "all_env.dat\n",
      "write.table(all_env.dat, file = \"G:/Analysis/Pop_analysis/Populations_b3_may/EnvData/RestandBayescenv/all_stand.txt\", sep =\"\\t\", row.names = TRUE, col.names = TRUE)\n",
      "\n",
      "\n",
      "#########  S T A N D A R D I Z E  T H E  ASIA AND NA seperately\n",
      "\n",
      "AS_env <- read.table (\"G:/Analysis/Pop_analysis/Populations_b3_may/EnvData/RestandBayescenv/asial.txt\", header = FALSE, sep = \"\\t\")\n",
      "AS_env <- as.data.frame(AS_env) \n",
      "colsAS_env <- c(names(AS_env))\n",
      "AS_env.dat <- scale(AS_env)\n",
      "AS_env.dat\n",
      "write.table(AS_env.dat, file = \"G:/Analysis/Pop_analysis/Populations_b3_may/EnvData/RestandBayescenv/asial_stand.txt\", sep =\"\\t\", row.names = TRUE, col.names = TRUE)\n",
      "\n",
      "NA_env <- read.table (\"G:/Analysis/Pop_analysis/Populations_b3_may/EnvData/RestandBayescenv/NA.txt\", header = FALSE, sep = \"\\t\")\n",
      "NA_env <- as.data.frame(NA_env) \n",
      "colsNA_env <- c(names(NA_env))\n",
      "NA_env.dat <- scale(NA_env)\n",
      "NA_env.dat\n",
      "write.table(NA_env.dat, file = \"G:/Analysis/Pop_analysis/Populations_b3_may/EnvData/RestandBayescenv/NA_stand.txt\", sep =\"\\t\", row.names = TRUE, col.names = TRUE)\n",
      "\n",
      "Here are the results \n",
      "\n",
      "> all_env.dat\n",
      "             V1          V2         V3         V4\n",
      "[1,] -0.8565349 -0.21094173 -0.8616006 -1.6048275\n",
      "[2,]  0.4495426 -0.60194710 -0.9827627 -0.3001735\n",
      "[3,]  1.0350400 -0.02033010 -0.7430199  0.5876270\n",
      "[4,]  0.5868730  0.04122948 -0.3898328  0.3327975\n",
      "[5,]  1.0276598  0.15290479  0.4494187  1.2124017\n",
      "[6,] -0.9239790  1.95714052  0.9549352  0.7175004\n",
      "[7,] -1.3186016 -1.31805585  1.5728621 -0.9453255\n",
      "attr(,\"scaled:center\")\n",
      "           V1            V2            V3            V4 \n",
      "-1.428570e-10 -1.428572e-10  1.774485e+02  5.522978e+01 \n",
      "attr(,\"scaled:scale\")\n",
      "       V1        V2        V3        V4 \n",
      " 3.757525  2.448061 38.382529  7.632633 \n",
      "\n",
      "> AS_env.dat\n",
      "             V1          V2           V3          V4\n",
      "[1,] -1.4273011 -0.04465007 -0.458581253 -1.38687200\n",
      "[2,]  0.1793713 -1.39337731 -0.932274969 -0.05515034\n",
      "[3,]  0.8996214  0.61284240  0.005020223  0.85106941\n",
      "[4,]  0.3483084  0.82518497  1.385835998  0.59095293\n",
      "attr(,\"scaled:center\")\n",
      "         V1          V2          V3          V4 \n",
      "  1.1412739  -0.4847095 148.8802027  53.3510555 \n",
      "attr(,\"scaled:scale\")\n",
      "       V1        V2        V3        V4 \n",
      "3.0545238 0.7097097 9.8175421 7.4774965 \n",
      "\n",
      "> NA_env.dat\n",
      "             V1          V2          V3         V4\n",
      "[1,]  1.1403698 -0.06772142 -0.96503808  0.7822666\n",
      "[2,] -0.4131260  1.03213941 -0.06659491  0.3444238\n",
      "[3,] -0.7272438 -0.96441799  1.03163299 -1.1266904\n",
      "attr(,\"scaled:center\")\n",
      "         V1          V2          V3          V4 \n",
      " -1.5216985   0.6462794 215.5395000  57.7347537 \n",
      "attr(,\"scaled:scale\")\n",
      "       V1        V2        V3        V4 \n",
      " 4.720535  4.015852 21.596246  8.627297 \n",
      " \n",
      " \n",
      " i put the results in excel and then from each of the columns, or variables I added the absolute value of the largest negative value to get them to all be positive. so that the scale would start at zero. \n",
      " \n",
      " what order are the samples in in the input files? \n",
      " \n",
      " I think these are in a different order and i dont want to mess it up. this is the order of these environmental variables: \n",
      " \n",
      " \n",
      " all\tKushiro\n",
      "\t\tAmur\n",
      "\t\tTauy\n",
      "\t\tHaylyluya\n",
      "\t\tNome\n",
      "\t\tKoppen\n",
      "\t\tSnohomish\n",
      "\t\n",
      "ASE/ASO\tKushiro\n",
      "\t\tAmur\n",
      "\t\tTauy\n",
      "\t\tHaylyluya\n",
      "\t\n",
      "NAE/ASO\tNome\n",
      "\t\tKoppen\n",
      "\t\tSnohomish\n",
      "\n",
      " \n",
      "OK so i put them into the file that I used to make the input files the time before and i made the input files for all the samples, the asia and the north america, and within each of those categories i had the four differnt types of environmental data, the LA LO and the two PC axies for the P and T. SO those were all calculated with all the samples, they are not individual. I have those calculations, but i dont want to run those this weekend. \n",
      "\n",
      ".\\bayescenv.exe 16681_bayescenv_LA.txt -env  ALL_LA_All_POS.txt -out 16681_bayescenv_LA_out.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >16681_bayescenv_LA_log.txt 2>&1\n",
      ".\\bayescenv.exe 16681_bayescenv_LO.txt -env  ALL_LO_All_POS.txt -out 16681_bayescenv_LO_out.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >16681_bayescenv_LO_log.txt 2>&1\n",
      ".\\bayescenv.exe 16681_bayescenv_PT1.txt -env  ALL_PT1_All_POS.txt -out 16681_bayescenv_PT1_out.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >16681_bayescenv_PT1_log.txt 2>&1\n",
      ".\\bayescenv.exe 16681_bayescenv_PT2.txt -env  ALL_PT2_All_POS.txt -out 16681_bayescenv_PT2_out.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >16681_bayescenv_PT2_log.txt 2>&1\n",
      "\n",
      ".\\bayescenv.exe ASE_bayescenv_LA.txt -env  AS_LA_All_POS.txt -out ASE_bayescenv_LA_out.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >ASE_bayescenv_LA_log.txt 2>&1\n",
      ".\\bayescenv.exe ASE_bayescenv_LO.txt -env  AS_LO_All_POS.txt -out ASE_bayescenv_LO_out.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >ASE_bayescenv_LO_log.txt 2>&1\n",
      ".\\bayescenv.exe ASE_bayescenv_PT1.txt -env  AS_PT1_All_POS.txt -out ASE_bayescenv_PT1_out.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >ASE_bayescenv_PT1_log.txt 2>&1\n",
      ".\\bayescenv.exe ASE_bayescenv_PT2.txt -env  AS_PT2_All_POS.txt -out ASE_bayescenv_PT2_out.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >ASE_bayescenv_PT2_log.txt 2>&1\n",
      "\n",
      ".\\bayescenv.exe ASO_bayescenv_LA.txt -env  AS_LA_All_POS.txt -out ASO_bayescenv_LA_out.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >ASO_bayescenv_LA_log.txt 2>&1\n",
      ".\\bayescenv.exe ASO_bayescenv_LO.txt -env  AS_LO_All_POS.txt -out ASO_bayescenv_LO_out.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >ASO_bayescenv_LO_log.txt 2>&1\n",
      ".\\bayescenv.exe ASO_bayescenv_PT1.txt -env  AS_PT1_All_POS.txt -out ASO_bayescenv_PT1_out.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >ASO_bayescenv_PT1_log.txt 2>&1\n",
      ".\\bayescenv.exe ASO_bayescenv_PT2.txt -env  AS_PT2_All_POS.txt -out ASO_bayescenv_PT2_out.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >ASO_bayescenv_PT2_log.txt 2>&1\n",
      "\n",
      ".\\bayescenv.exe NAE_bayescenv_LA.txt -env  NA_LA_All_POS.txt -out NAE_bayescenv_LA_out.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NAE_bayescenv_LA_log.txt 2>&1\n",
      ".\\bayescenv.exe NAE_bayescenv_LO.txt -env  NA_LO_All_POS.txt -out NAE_bayescenv_LO_out.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NAE_bayescenv_LO_log.txt 2>&1\n",
      ".\\bayescenv.exe NAE_bayescenv_PT1.txt -env  NA_PT1_All_POS.txt -out NAE_bayescenv_PT1_out.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NAE_bayescenv_PT1_log.txt 2>&1\n",
      ".\\bayescenv.exe NAE_bayescenv_PT2.txt -env  NA_PT2_All_POS.txt -out NAE_bayescenv_PT2_out.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NAE_bayescenv_PT2_log.txt 2>&1\n",
      "\n",
      ".\\bayescenv.exe NAO_bayescenv_LA.txt -env  NA_LA_All_POS.txt -out NAO_bayescenv_LA_out.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NAO_bayescenv_LA_log.txt 2>&1\n",
      ".\\bayescenv.exe NAO_bayescenv_LO.txt -env  NA_LO_All_POS.txt -out NAO_bayescenv_LO_out.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NAO_bayescenv_LO_log.txt 2>&1\n",
      ".\\bayescenv.exe NAO_bayescenv_PT1.txt -env  NA_PT1_All_POS.txt -out NAO_bayescenv_PT1_out.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NAO_bayescenv_PT1_log.txt 2>&1\n",
      ".\\bayescenv.exe NAO_bayescenv_PT2.txt -env  NA_PT2_All_POS.txt -out NAO_bayescenv_PT2_out.txt -n 5000 -thin 10 -nbp 20 -pilot 5000 -burn 50000 >NAO_bayescenv_PT2_log.txt 2>&1\n",
      "\n",
      "\n",
      "Im going over and run them now. \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}