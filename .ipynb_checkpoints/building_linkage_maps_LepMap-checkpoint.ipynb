{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Building Linkage Maps with LepMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I begin with LepMap, I have to identify the haploid data that I want to use for LepMap. More specifically, which runs of MSTMap I want to use to identify and compare linkage groups between families and rename the loci. Looking over the results from the various runs of MSTMap I did and paying attention to the size and number of the linkage groups as well as the number of markers and bins that were used, I chose the following runs for each of the four families. These runs of the familes were not overmerged, had seemingly uninflated linkage group size, robust numbers of markers, a resonable number of bins per linkage group and a fair number of samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PBIRD13X108H_80GR \n",
    "PBIRD13X110H_65GR\n",
    "PHOOD11x01H_4\n",
    "PHOOD11x05H_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Identifying duplicated loci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the ouput from Ryans pipeline for each of the families to identify loci that are duplicates. \n",
    "Garrett wrote a script called parseStats.pl that takes the .stats output from Ryans pipeline and for each of the loci tells you the most likely model, then gives a distinction for whether that model is a duplicated one or not (or whether we would be able to tell if it were duplicated). The singles are given the classification of 1 and the duplicates are given a 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/perl -w\n",
    "use strict;\n",
    "\n",
    "#Hash of single and duplicate marker models, key is model, value is 1 for single locus or 2 for duplicate.\n",
    "my%dupModels=(\t\"AA_xx\" => 1,\n",
    "\t\t\t\t\"AB\" => 1,\n",
    "\t\t\t\t\"AA_AB\" => 2,\n",
    "\t\t\t\t\"AA_BB\" => 2,\n",
    "\t\t\t\t\"AB_AB\" => 2,\n",
    "\t\t\t\t\"AA_AB_AB\" => 2,\n",
    "\t\t\t\t\"AA_BB_AB\" => 2,\n",
    "\t\t\t\t\"AB_AB_AB\" => 2,\n",
    "\t\t\t\t\"AA_BC\" => 2,\n",
    "\t\t\t\t\"AB_AC\" => 2,\n",
    "\t\t\t\t\"AB_CD\" => 2,\n",
    "\t\t\t\t\"AA_BB_AC\" => 2,\n",
    "\t\t\t\t\"AA_AB_AC\" => 2,\n",
    "\t\t\t\t\"AA_BB_CC\" => 2);\n",
    "\n",
    "\n",
    "while(my$line=<>){\n",
    "\tchomp$line;\n",
    "\tmy($tag,$mappable_loci,$epsilon,$bestModel,$bestLikelihood,$secondModel,$secondLikelihood,$segTest_x1,$segTest_x2)=split \"\\t\", $line, 9;\n",
    "\tif($mappable_loci>=1){\n",
    "\t\tprint \"$tag\\t$bestModel\\t$dupModels{$bestModel}\\n\";\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "perl parseStats.pl PBIRD13X108H_filtered.stats >PBIRD13X108H_parsed.txt\n",
    "perl parseStats.pl PBIRD13X110H_filtered.stats >PBIRD13X110H_parsed.txt\n",
    "perl parseStats.pl PHOOD11x01H_filtered.stats >PHOOD11x01H_parsed.txt\n",
    "perl parseStats.pl PHOOD11x05H_filtered.stats >PHOOD11x05H_parsed.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Comparing duplicated loci across families"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we need is a file that has all the loci that are genotyped in any family, and the distinction of whether they are duplicated or not. If they are duplicated in any family (even if they are not duplicated in other families)they are considered duplicated. \n",
    "\n",
    "To build this file we need the output from MSTMap that has been split into one set of linkage groups (_split) and run through the MST_format.pl script(_format) and the output from the runs of the parseStats.pl on the four families. The parseStats results I put in excel, one tab for each family. \n",
    "\n",
    "I took the list of loci in the file that has been split and formatted and put all the loci from the families in column all together. I had already removed the R from the loci at an earlier stage. I 'removed duplicates' in excel so that each loci is in the list only once, but all the loci that are present in any family is included in the list. Then I copied that column to another column and did 'text to column', splitting the names on the _ to get the beginning of the loci name alone. I did a vlookup for each family with that loci prefix to the output of the parseStats to find whether in that family the loci was a duplicate or not. \n",
    "\n",
    "Finally, i made a single file that had a list of the loci as one column and the distinction of whether it was a duplicated loci or not, using the 1 or 2. If the loci had a distinction of 2 in any of the families, I gave it 2 (duplicated) in the final list. Here is an example: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "22869_x1\t2\n",
    "12750_x1\t2\n",
    "14409_x1\t2\n",
    "18990_x1\t2\n",
    "20758_x1\t2\n",
    "30259_x1\t2\n",
    "28317_x1\t2\n",
    "32569_x1\t2\n",
    "45786_x1\t2\n",
    "45786_x2\t2\n",
    "87954_x1\t2\n",
    "11564_x1\t2\n",
    "15649_x1\t2\n",
    "564_x1\t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Compare the MSTMap results between families to find matching linkage groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Garrett wrote a script that takes two MSTMap output files and the list of all the loci (with their distinction of duplicated or not) and returns the linkage groups that match up to one another between families. The script also tells you how many loci the linkage groups had in common, so what the match was based on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#MST_MST_compare_all_dupFilter2.pl\n",
    "#!/usr/bin/perl -w\n",
    "use strict;\n",
    "\n",
    "my$MSTfile1=$ARGV[0];\n",
    "my$MSTfile2=$ARGV[1];\n",
    "my$DuplicateFile=$ARGV[2];\n",
    "\n",
    "my%LociDuplication;\n",
    "open(DUPLICATION, \"<$DuplicateFile\")||die \"cannot open $DuplicateFile:$!\";\n",
    "my$DupHeader=<DUPLICATION>;\n",
    "chomp $DupHeader;\n",
    "while(my$line=<DUPLICATION>){\n",
    "\tchomp $line;\n",
    "\tmy($loci,$duplication)=split \"\\t\", $line, 2;\n",
    "\t$LociDuplication{$loci}=$duplication;\n",
    "}\n",
    "\n",
    "my%MST1loci;\n",
    "my$MST1currentLG;\n",
    "my$MST1store_loci=0;\n",
    "my%MST1LGlociNum;\n",
    "my%MST1LGlength;\n",
    "my$MST1lociNum=0;\n",
    "my$MST1currentPosition;\n",
    "open(MST1, \"<$MSTfile1\")||die \"cannot open $MSTfile1:$!\";\n",
    "while(my $line=<MST1>){\n",
    "\tchomp $line;\n",
    "\tif($line=~/^group/){\n",
    "\t\tmy($discard, $LG)=split \" \", $line, 2;\n",
    "\t\t$MST1currentLG=$LG;\n",
    "\t}\n",
    "\tif($line=~/\\;BEGINOFGROUP/){\n",
    "\t\t$MST1store_loci=1;\n",
    "\t\tnext;\n",
    "\t}\n",
    "\tif($line=~/\\;ENDOFGROUP/){\n",
    "\t\t$MST1LGlociNum{$MST1currentLG}=$MST1lociNum;\n",
    "\t\t$MST1LGlength{$MST1currentLG}=$MST1currentPosition;\n",
    "\t\t$MST1store_loci=0;\n",
    "\t\t$MST1currentLG=\"\";\n",
    "\t\t$MST1currentPosition=\"\";\n",
    "\t\t$MST1lociNum=0;\n",
    "\t}\n",
    "\tif($MST1store_loci==1){\n",
    "\t\tmy($locus, $position)=split \"\\t\", $line, 2;\n",
    "\t\t#$locus=~s/R//;\n",
    "\t\tif($LociDuplication{$locus}==1){\n",
    "\t\t\t$MST1loci{$locus}=$MST1currentLG;\n",
    "\t\t\t$MST1currentPosition=$position;\n",
    "\t\t\t$MST1lociNum++;\n",
    "\t\t}\n",
    "\t}\n",
    "}\n",
    "close MST1;\n",
    "\n",
    "my%MST2loci;\n",
    "my$MST2currentLG;\n",
    "my$MST2store_loci=0;\n",
    "my%MST2LGlociNum;\n",
    "my%MST2LGlength;\n",
    "my$MST2lociNum=0;\n",
    "my$MST2currentPosition;\n",
    "open(MST2, \"<$MSTfile2\")||die \"cannot open $MSTfile2:$!\";\n",
    "while(my $line=<MST2>){\n",
    "\tchomp $line;\n",
    "\tif($line=~/^group/){\n",
    "\t\tmy($discard, $LG)=split \" \", $line, 2;\n",
    "\t\t$MST2currentLG=$LG;\n",
    "\t}\n",
    "\tif($line=~/\\;BEGINOFGROUP/){\n",
    "\t\t$MST2store_loci=1;\n",
    "\t\tnext;\n",
    "\t}\n",
    "\tif($line=~/\\;ENDOFGROUP/){\n",
    "\t\t$MST2LGlociNum{$MST2currentLG}=$MST2lociNum;\n",
    "\t\t$MST2LGlength{$MST2currentLG}=$MST2currentPosition;\n",
    "\t\t$MST2store_loci=0;\n",
    "\t\t$MST2currentLG=\"\";\n",
    "\t\t$MST2currentPosition=\"\";\n",
    "\t\t$MST2lociNum=0;\n",
    "\t}\n",
    "\tif($MST2store_loci==1){\n",
    "\t\tmy($locus, $position)=split \"\\t\", $line, 2;\n",
    "\t\t#$locus=~s/R//;\n",
    "\t\tif($LociDuplication{$locus}==1){\n",
    "\t\t\t$MST2loci{$locus}=$MST2currentLG;\n",
    "\t\t\t$MST2currentPosition=$position;\n",
    "\t\t\t$MST2lociNum++;\n",
    "\t\t\t#print \"$locus\\t$MST2currentLG\\n\";\n",
    "\t\t}\n",
    "\t}\n",
    "}\n",
    "close MST2;\n",
    "\n",
    "my%LGpairs;\n",
    "foreach my$key (keys%MST1loci){\n",
    "\tmy$Rkey=$key.\"R\";\n",
    "\tif(((exists $MST1loci{$key})&&(exists $MST2loci{$key}))||((exists $MST1loci{$Rkey})&&(exists $MST2loci{$key}))){\n",
    "\t\t#print \"$key\\n\";\n",
    "\t\t#print \"$MST1loci{$key}\\t$MST2loci{$key}\\n\";\n",
    "\t\tmy$pair=\"$MST1loci{$key} $MST1LGlociNum{$MST1loci{$key}} $MST1LGlength{$MST1loci{$key}}\\t$MST2loci{$key} $MST2LGlociNum{$MST2loci{$key}} $MST2LGlength{$MST2loci{$key}}\";\n",
    "\t\t$LGpairs{$pair}++;\n",
    "\t}\n",
    "}\n",
    "\n",
    "foreach my$key (sort keys %LGpairs){\n",
    "\tprint \"$key\\t$LGpairs{$key}\\n\";\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have four families and did three comparisons. \n",
    " \n",
    "- PBIRD13X110H_65GR to PBIRD13X108H_80GR\n",
    "- PBIRD13X108H_80GR to PHOOD11x05H_5\n",
    "- PHOOD11x05H_5 to PHOOD11x01H_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perl MST_MST_compare_all_dupFilter2.pl PBIRD13X110H_65GR_splitNEW_out.txt PBIRD13X108H_80GR_splitNEW_out.txt Loci.txt >MST_compare_110H_108H.txt\n",
    "perl MST_MST_compare_all_dupFilter2.pl PBIRD13X108H_80GR_splitNEW_out.txt PHOOD11X05H_5_splitNEW_out.txt Loci.txt >MST_compare_108H_05H.txt\n",
    "perl MST_MST_compare_all_dupFilter2.pl PHOOD11X05H_5_splitNEW_out.txt PHOOD11X01H_4_splitNEW_out.txt Loci.txt >MST_compare_05H_01H.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Making the input files for LepMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the following code was written by Ryan Waples, taken and adapted from his ipython notebook http://nbviewer.ipython.org/github/rwaples/chum_populations/blob/master/multi-family%20mapping.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\\n"
     ]
    }
   ],
   "source": [
    "cd G:\\  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\Analysis\\Mapping\\AllHaps\\LepMap\n"
     ]
    }
   ],
   "source": [
    "cd Analysis\\Mapping\\AllHaps\\LepMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rwaples/chum_populations/linkage_map/multi_family_mapping_functions.py\n",
    "#@rwaples rwaples on Mar 6 consensus mapping with paralogs\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import itertools\n",
    "import scipy.spatial.distance\n",
    "import pandas as pd\n",
    "import sys\n",
    "import operator\n",
    "\n",
    "#Functions\n",
    "   \n",
    "def get_ml_R_frac(R, NR):\n",
    "    #\"\"\"Returns the maximum likelihood recombination fraction, as given as: R / (R + NR)\"\"\"\n",
    "    if R.__class__ == numpy.ndarray:\n",
    "        R = R.astype(numpy.float)\n",
    "    return(R / (R + NR))\n",
    "    \n",
    "def get_LOD(R, NR, R_frac):\n",
    "    #\"\"\"Returns the LOD score for a set of the given values of [R, NR, R_frac].\n",
    "    #LOD scores calculated against the likelihood c =.5\"\"\"\n",
    "    Z = numpy.log10(\n",
    "                (numpy.power((1-R_frac), NR) * numpy.power(R_frac, R)) / numpy.power(.5, (R + NR))\n",
    "            )\n",
    "    return(Z)\n",
    "    \n",
    "def getR(pairs):\n",
    "    #\"\"\"generator, yields the number of recombinant individuals for a given pair of loci\"\"\"\n",
    "    for x, y in pairs:\n",
    "        mult = x * y\n",
    "        yield numpy.sum(mult == 2)\n",
    "         \n",
    "def getNR(pairs):\n",
    "    #\"\"\"generator, yields the number of NON-recombinant individuals for a given pair of loci\"\"\"\n",
    "    for x, y in pairs:\n",
    "        mult = x * y\n",
    "        yield (numpy.sum(mult == 1) + numpy.sum(mult == 4))\n",
    "    \n",
    "def import_MSTmap(filename):\n",
    "    with open(filename) as INFILE:\n",
    "        genotypes_at_locus = dict()\n",
    "        for line in INFILE:\n",
    "            if line.startswith('locus_name'): # start parsing\n",
    "                individuals = line.strip().split()[1:]\n",
    "                #print(individuals)\n",
    "                for line in INFILE:\n",
    "                    line_split = line.strip().split()\n",
    "                    locus = line_split[0]\n",
    "                    #print(locus)\n",
    "                    genotypes = [1 if xx == 'a' else 2 if xx == 'b' else 0 for xx in line_split[1:]]\n",
    "                    genotypes_at_locus[locus] = genotypes\n",
    "                    #print(genotypes)\n",
    "            else:\n",
    "                pass\n",
    "    return(individuals, genotypes_at_locus)\n",
    "    \n",
    "def remove_by_blacklist(blacklist, genotypes_at_locus):\n",
    "    print(\"Starting length of genotypes: {}\".format(len(genotypes_at_locus)))\n",
    "    for locus in blacklist:        \n",
    "        if locus in genotypes_at_locus.keys():\n",
    "            del genotypes_at_locus[locus]\n",
    "        elif locus + \"_x1\" in genotypes_at_locus.keys():\n",
    "            del genotypes_at_locus[locus + \"_x1\"]\n",
    "        elif locus + \"_x2\" in genotypes_at_locus.keys():\n",
    "            del genotypes_at_locus[locus + \"_x2\"]\n",
    "    print(\"Final length of genotypes: {}\".format(len(genotypes_at_locus)))\n",
    "    return(True)\n",
    "    \n",
    "def prep_data_pandas(individuals, genotypes_at_locus):\n",
    "    # prepare pandas data.frame\n",
    "    # columns are loci, rows are individuals\n",
    "    my_pd_genos = pd.DataFrame.from_dict(genotypes_at_locus)\n",
    "    # add an index\n",
    "    my_pd_genos.index = individuals\n",
    "    return(my_pd_genos)\n",
    "\n",
    "def prepare_matrix(*args):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for arg in args:\n",
    "        if not isinstance(arg, pd.DataFrame):\n",
    "            raise ValueError(\"need a pandas DataFrame\")\n",
    "        else:\n",
    "            pass\n",
    "    # combine all the data sets into one\n",
    "    all_genos = pd.concat(objs = args, join = 'outer')\n",
    "    # convert Nan to 0\n",
    "    all_genos.fillna(value = 0, inplace = True)\n",
    "    #list of loci\n",
    "    loci = [str(xx) for xx in all_genos.transpose().index]\n",
    "    # conver to integer numpy array\n",
    "    all_genos_np = np.array(all_genos).astype(int)\n",
    "    #all_genos_np = all_genos_np.astype(int)\n",
    "    # transpose\n",
    "    all_genos_np = np.transpose(all_genos_np)\n",
    "    return(all_genos_np, loci)\n",
    "    \n",
    "\n",
    "# returns a redundant square matix\n",
    "def get_matrix(data_in_array):\n",
    "    data_in_matrix = scipy.spatial.distance.squareform(data_in_array)\n",
    "    np.fill_diagonal(data_in_matrix, np.nan)\n",
    "    return(data_in_matrix)\n",
    "\n",
    "def get_recombination_stats(geno_array):\n",
    "    int_arr = geno_array\n",
    "    num_loci = int_arr.shape[0]\n",
    "    num_pairs =  int((num_loci * (num_loci-1))/2)\n",
    "    \n",
    "    print('Starting, num_pairs = {}'.format(num_pairs))\n",
    "    print(str(datetime.now()))\n",
    "    time_start = datetime.now()\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    pairs = itertools.combinations(int_arr, 2)\n",
    "    R = np.fromiter(getR(pairs), dtype = np.int, count = num_pairs)\n",
    "    time_R = datetime.now()\n",
    "    print('Finished R')\n",
    "    print(str(datetime.now()))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    pairs = itertools.combinations(int_arr, 2)\n",
    "    NR = np.fromiter(getNR(pairs), dtype = np.int, count = num_pairs)\n",
    "    time_NR = datetime.now()\n",
    "    print('Finished NR')\n",
    "    print(str(datetime.now()))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    ml_R_frac = get_ml_R_frac(R = R, NR = NR)\n",
    "    time_RF = datetime.now()\n",
    "    print('Finished RF')\n",
    "    print(str(datetime.now()))\n",
    "    \n",
    "    sys.stdout.flush()\n",
    "    Z = get_LOD(R = R, NR = NR, R_frac = ml_R_frac)\n",
    "    time_Z = datetime.now()\n",
    "    print('Finished Z')\n",
    "    print(str(datetime.now()))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    N = R + NR\n",
    "    MST = np.e**-(2*(N/2. - R)**2/N)\n",
    "    print('Finished MST')\n",
    "    time_MST = datetime.now()\n",
    "    print(str(datetime.now()))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    print(\"R took: {}\".format(str(time_R - time_start)))\n",
    "    print(\"NR took: {}\".format(str(time_NR - time_R)))\n",
    "    print(\"RF took: {}\".format(str(time_RF - time_NR)))\n",
    "    print(\"Z took: {}\".format(str(time_Z - time_RF)))\n",
    "    print(\"MST took: {}\".format(str(time_MST - time_Z)))   \n",
    "    \n",
    "    Z_mat = get_matrix(Z)\n",
    "    RF_mat = get_matrix(ml_R_frac)\n",
    "    R_mat = get_matrix(R)\n",
    "    NR_mat = get_matrix(NR)\n",
    "    MST_mat = get_matrix(MST)\n",
    "    \n",
    "    Recombination_stats = collections.namedtuple('Recombination_stats', \"R NR RF Z MST\" )\n",
    "    my_stats = Recombination_stats(R_mat, NR_mat, RF_mat, Z_mat, MST_mat)\n",
    "    return(my_stats)    \n",
    "\n",
    "def write_loci(loci, path):\n",
    "    with open(os.path.join(path, 'loci.txt'), 'w') as OUTFILE:\n",
    "        OUTFILE.write(\"\\n\".join(loci))\n",
    "        \n",
    "def write_rec_stats(stats, path):\n",
    "    for stat in stats._fields:\n",
    "        np.savetxt(X = getattr(stats, stat), fname = os.path.join(path, stat + \".tsv\"), delimiter = \"\\t\", fmt = '%1.4g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats_file_1 = ('PBIRD13X110H_filtered.stats')\n",
    "stats_file_2 = ('PBIRD13X108H_filtered.stats')\n",
    "stats_file_3 = ('PHOOD11X01H_filtered.stats')\n",
    "stats_file_4 = ('PHOOD11X05H_filtered.stats')\n",
    "\n",
    "paralogs_file = ('pink_paralogs4.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paralogs = set()\n",
    "\n",
    "for stats_file in [stats_file_1, stats_file_2, stats_file_3, stats_file_4]:\n",
    "    with open(stats_file) as INFILE:\n",
    "        for line in INFILE:\n",
    "            if line.strip().split()[3] not in [\"AA_xx\", \"AB\"]:\n",
    "                paralogs.add(line.strip().split()[0])\n",
    "                \n",
    "paralogs = sorted([int(xx) for xx in paralogs])\n",
    "\n",
    "with open(paralogs_file, 'w') as OUTFILE:\n",
    "    for xx in sorted(list(paralogs)):\n",
    "        OUTFILE.write('{}\\n'.format(xx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linkage_map_file_1 = ('PBIRD13X110H_65GR_splitNEW.txt')\n",
    "linkage_map_file_2 = ('PBIRD13X108H_80GR_splitNEW.txt')\n",
    "linkage_map_file_3 = ('PHOOD11X01H_4_splitNEW.txt')\n",
    "linkage_map_file_4 = ('PHOOD11X05H_5_splitNEW.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "individuals_110, genotypes_at_locus_110 = import_MSTmap(linkage_map_file_1)\n",
    "individuals_108, genotypes_at_locus_108 = import_MSTmap(linkage_map_file_2)\n",
    "individuals_01, genotypes_at_locus_01 = import_MSTmap(linkage_map_file_3)\n",
    "individuals_05, genotypes_at_locus_05 = import_MSTmap(linkage_map_file_4)\n",
    "\n",
    "my_pd_genos_110 = prep_data_pandas(individuals_110, genotypes_at_locus_110)\n",
    "my_pd_genos_108 = prep_data_pandas(individuals_108, genotypes_at_locus_108)\n",
    "my_pd_genos_01  = prep_data_pandas(individuals_01,  genotypes_at_locus_01)\n",
    "my_pd_genos_05  = prep_data_pandas(individuals_05,  genotypes_at_locus_05)\n",
    "\n",
    "fam_110 = prepare_matrix(my_pd_genos_110)\n",
    "fam_108 = prepare_matrix(my_pd_genos_108)\n",
    "fam_01 = prepare_matrix(my_pd_genos_01)\n",
    "fam_05 = prepare_matrix(my_pd_genos_05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rename_loci_by_family(paralogs_file, fam_names, families):\n",
    "    # check if each family listed in families is formatted as if returned from prepare matrix\n",
    "    for afam in families:\n",
    "        if not isinstance(afam, pd.core.frame.DataFrame):\n",
    "            raise ValueError(\"families should be a pandas DataFrame\")\n",
    "        else:\n",
    "            pass\n",
    "    if len(fam_names) != len(families) :\n",
    "        raise ValueError(\"names and families should have the same length\")\n",
    "    if not isinstance(fam_names, list ):\n",
    "        raise ValueError(\"names should be a list\")\n",
    "    \n",
    "    with open(paralogs_file) as INFILE: \n",
    "        paralogs = [yy.strip() for yy in INFILE.readlines()]\n",
    "    # for each family, for each locus, if the locus is a paralog, append family-specific text to locus name\n",
    "    # genotypes are unchanged\n",
    "    #new_familes = list()\n",
    "    for idx, afam in enumerate(families):\n",
    "        old_locus_names = afam.columns.values.tolist()\n",
    "        new_locus_names = []\n",
    "        for xx in old_locus_names:\n",
    "            base_name = xx[:-3]\n",
    "            if base_name in paralogs:\n",
    "                #print(\"{} is a paralog\".format(base_name))\n",
    "                new_name = \"{}_{}_{}\".format(base_name, fam_names[idx], xx[-2:])\n",
    "            else: \n",
    "                new_name = base_name\n",
    "            new_locus_names.append(new_name)\n",
    "        afam.columns = new_locus_names\n",
    "    return(families)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "renamed_110, renamed_108, renamed_01, renamed_05  = rename_loci_by_family(paralogs_file = paralogs_file, fam_names = ['pink_110','pink_108', 'pink_01', 'pink_05'], families = [my_pd_genos_110, my_pd_genos_108, my_pd_genos_01, my_pd_genos_05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.head of                   10003_pink_01_x1  100077  10007  100176  10017  \\\n",
       "PHOOD11X01H_0001                 1       2      1       2      1   \n",
       "PHOOD11X01H_0002                 1       0      2       1      1   \n",
       "PHOOD11X01H_0003                 2       2      2       1      1   \n",
       "PHOOD11X01H_0004                 1       2      2       2      1   \n",
       "PHOOD11X01H_0005                 1       1      2       2      1   \n",
       "PHOOD11X01H_0006                 1       1      1       1      1   \n",
       "PHOOD11X01H_0007                 2       1      1       1      1   \n",
       "PHOOD11X01H_0008                 1       2      1       2      1   \n",
       "PHOOD11X01H_0009                 2       2      2       2      2   \n",
       "PHOOD11X01H_0010                 1       2      1       2      1   \n",
       "PHOOD11X01H_0011                 1       2      2       1      1   \n",
       "PHOOD11X01H_0012                 1       0      1       2      1   \n",
       "PHOOD11X01H_0013                 1       1      2       1      1   \n",
       "PHOOD11X01H_0014                 0       0      2       1      2   \n",
       "PHOOD11X01H_0015                 1       1      1       2      1   \n",
       "PHOOD11X01H_0016                 1       2      2       1      2   \n",
       "PHOOD11X01H_0017                 1       2      2       1      1   \n",
       "PHOOD11X01H_0018                 2       1      1       2      2   \n",
       "PHOOD11X01H_0019                 1       1      1       1      1   \n",
       "PHOOD11X01H_0020                 1       2      2       1      1   \n",
       "PHOOD11X01H_0021                 1       1      1       1      2   \n",
       "PHOOD11X01H_0022                 0       2      2       1      1   \n",
       "PHOOD11X01H_0023                 1       2      2       1      2   \n",
       "PHOOD11X01H_0024                 2       1      1       1      1   \n",
       "PHOOD11X01H_0025                 1       0      1       1      1   \n",
       "PHOOD11X01H_0026                 2       1      2       1      2   \n",
       "PHOOD11X01H_0027                 1       2      2       2      1   \n",
       "PHOOD11X01H_0028                 1       2      2       1      1   \n",
       "PHOOD11X01H_0029                 1       2      2       2      1   \n",
       "PHOOD11X01H_0030                 1       1      2       2      1   \n",
       "...                            ...     ...    ...     ...    ...   \n",
       "PHOOD11X01H_0066                 2       2      2       2      2   \n",
       "PHOOD11X01H_0067                 1       2      1       1      1   \n",
       "PHOOD11X01H_0068                 1       0      1       2      1   \n",
       "PHOOD11X01H_0069                 1       0      2       2      1   \n",
       "PHOOD11X01H_0070                 1       2      1       1      2   \n",
       "PHOOD11X01H_0071                 1       0      1       2      1   \n",
       "PHOOD11X01H_0072                 2       0      2       2      2   \n",
       "PHOOD11X01H_0073                 2       2      2       1      1   \n",
       "PHOOD11X01H_0074                 1       2      1       2      1   \n",
       "PHOOD11X01H_0075                 1       0      2       1      2   \n",
       "PHOOD11X01H_0076                 1       2      1       1      2   \n",
       "PHOOD11X01H_0077                 2       1      1       2      2   \n",
       "PHOOD11X01H_0078                 1       1      1       1      2   \n",
       "PHOOD11X01H_0079                 1       1      2       1      1   \n",
       "PHOOD11X01H_0080                 2       0      1       2      1   \n",
       "PHOOD11X01H_0081                 2       1      1       1      1   \n",
       "PHOOD11X01H_0082                 2       1      2       1      2   \n",
       "PHOOD11X01H_0083                 2       1      2       2      1   \n",
       "PHOOD11X01H_0084                 2       1      2       2      2   \n",
       "PHOOD11X01H_0085                 2       2      2       1      1   \n",
       "PHOOD11X01H_0086                 2       1      2       1      1   \n",
       "PHOOD11X01H_0087                 1       0      2       1      1   \n",
       "PHOOD11X01H_0088                 2       1      2       0      2   \n",
       "PHOOD11X01H_0089                 1       0      2       2      2   \n",
       "PHOOD11X01H_0090                 2       2      1       2      1   \n",
       "PHOOD11X01H_0091                 1       1      1       2      2   \n",
       "PHOOD11X01H_0092                 1       2      2       2      1   \n",
       "PHOOD11X01H_0093                 1       2      1       2      1   \n",
       "PHOOD11X01H_0094                 2       1      2       2      2   \n",
       "PHOOD11X01H_0095                 1       1      1       2      1   \n",
       "\n",
       "                  100193_pink_01_x1  10021  10025  10047  10049  \\\n",
       "PHOOD11X01H_0001                  2      1      1      1      1   \n",
       "PHOOD11X01H_0002                  1      2      1      1      1   \n",
       "PHOOD11X01H_0003                  1      2      2      1      2   \n",
       "PHOOD11X01H_0004                  2      2      2      1      2   \n",
       "PHOOD11X01H_0005                  2      2      1      1      2   \n",
       "PHOOD11X01H_0006                  1      1      2      1      1   \n",
       "PHOOD11X01H_0007                  1      1      1      1      2   \n",
       "PHOOD11X01H_0008                  1      1      2      1      2   \n",
       "PHOOD11X01H_0009                  2      2      2      2      2   \n",
       "PHOOD11X01H_0010                  1      1      2      1      2   \n",
       "PHOOD11X01H_0011                  1      2      1      2      1   \n",
       "PHOOD11X01H_0012                  1      1      2      1      2   \n",
       "PHOOD11X01H_0013                  2      2      2      1      2   \n",
       "PHOOD11X01H_0014                  2      2      2      2      2   \n",
       "PHOOD11X01H_0015                  1      1      2      1      1   \n",
       "PHOOD11X01H_0016                  1      2      2      2      2   \n",
       "PHOOD11X01H_0017                  2      2      2      2      1   \n",
       "PHOOD11X01H_0018                  2      1      2      2      1   \n",
       "PHOOD11X01H_0019                  2      1      2      1      1   \n",
       "PHOOD11X01H_0020                  2      2      2      1      2   \n",
       "PHOOD11X01H_0021                  1      1      2      2      1   \n",
       "PHOOD11X01H_0022                  1      1      2      1      2   \n",
       "PHOOD11X01H_0023                  2      2      1      2      1   \n",
       "PHOOD11X01H_0024                  1      1      2      1      2   \n",
       "PHOOD11X01H_0025                  0      1      2      1      2   \n",
       "PHOOD11X01H_0026                  2      2      2      2      2   \n",
       "PHOOD11X01H_0027                  2      2      1      1      1   \n",
       "PHOOD11X01H_0028                  1      2      2      1      1   \n",
       "PHOOD11X01H_0029                  1      2      1      1      1   \n",
       "PHOOD11X01H_0030                  2      2      2      1      2   \n",
       "...                             ...    ...    ...    ...    ...   \n",
       "PHOOD11X01H_0066                  2      2      1      2      2   \n",
       "PHOOD11X01H_0067                  0      1      2      1      2   \n",
       "PHOOD11X01H_0068                  1      1      2      1      1   \n",
       "PHOOD11X01H_0069                  1      2      1      1      1   \n",
       "PHOOD11X01H_0070                  1      1      1      2      1   \n",
       "PHOOD11X01H_0071                  2      1      2      1      1   \n",
       "PHOOD11X01H_0072                  1      2      2      2      1   \n",
       "PHOOD11X01H_0073                  2      2      2      1      2   \n",
       "PHOOD11X01H_0074                  2      1      2      1      2   \n",
       "PHOOD11X01H_0075                  2      2      1      2      1   \n",
       "PHOOD11X01H_0076                  2      1      1      2      1   \n",
       "PHOOD11X01H_0077                  2      2      1      2      1   \n",
       "PHOOD11X01H_0078                  1      1      2      2      2   \n",
       "PHOOD11X01H_0079                  1      2      2      1      2   \n",
       "PHOOD11X01H_0080                  2      1      2      1      2   \n",
       "PHOOD11X01H_0081                  1      1      2      1      1   \n",
       "PHOOD11X01H_0082                  1      2      1      2      2   \n",
       "PHOOD11X01H_0083                  2      2      1      1      1   \n",
       "PHOOD11X01H_0084                  2      2      1      2      1   \n",
       "PHOOD11X01H_0085                  2      2      2      1      2   \n",
       "PHOOD11X01H_0086                  2      2      1      1      1   \n",
       "PHOOD11X01H_0087                  2      2      2      1      2   \n",
       "PHOOD11X01H_0088                  2      2      2      2      2   \n",
       "PHOOD11X01H_0089                  2      2      2      2      2   \n",
       "PHOOD11X01H_0090                  2      1      1      1      1   \n",
       "PHOOD11X01H_0091                  1      1      1      2      2   \n",
       "PHOOD11X01H_0092                  2      2      2      1      2   \n",
       "PHOOD11X01H_0093                  2      1      1      1      1   \n",
       "PHOOD11X01H_0094                  1      2      1      2      1   \n",
       "PHOOD11X01H_0095                  2      1      2      2      2   \n",
       "\n",
       "                       ...         9903  99050  9921  9924_pink_01_x1  9934  \\\n",
       "PHOOD11X01H_0001       ...            2      2     1                2     1   \n",
       "PHOOD11X01H_0002       ...            1      2     2                2     2   \n",
       "PHOOD11X01H_0003       ...            1      1     2                2     2   \n",
       "PHOOD11X01H_0004       ...            1      2     2                1     2   \n",
       "PHOOD11X01H_0005       ...            2      1     2                1     2   \n",
       "PHOOD11X01H_0006       ...            1      1     2                1     1   \n",
       "PHOOD11X01H_0007       ...            1      1     1                1     1   \n",
       "PHOOD11X01H_0008       ...            1      2     2                2     2   \n",
       "PHOOD11X01H_0009       ...            1      2     2                2     2   \n",
       "PHOOD11X01H_0010       ...            2      2     1                1     1   \n",
       "PHOOD11X01H_0011       ...            2      1     2                1     2   \n",
       "PHOOD11X01H_0012       ...            2      0     1                2     1   \n",
       "PHOOD11X01H_0013       ...            2      1     2                2     2   \n",
       "PHOOD11X01H_0014       ...            1      1     2                1     2   \n",
       "PHOOD11X01H_0015       ...            2      2     1                2     1   \n",
       "PHOOD11X01H_0016       ...            2      1     1                1     1   \n",
       "PHOOD11X01H_0017       ...            2      2     2                1     1   \n",
       "PHOOD11X01H_0018       ...            1      2     1                1     1   \n",
       "PHOOD11X01H_0019       ...            2      2     2                2     2   \n",
       "PHOOD11X01H_0020       ...            2      1     2                1     2   \n",
       "PHOOD11X01H_0021       ...            2      2     1                2     1   \n",
       "PHOOD11X01H_0022       ...            1      1     1                2     1   \n",
       "PHOOD11X01H_0023       ...            1      1     2                2     2   \n",
       "PHOOD11X01H_0024       ...            1      1     2                1     2   \n",
       "PHOOD11X01H_0025       ...            1      2     1                1     1   \n",
       "PHOOD11X01H_0026       ...            1      2     1                2     1   \n",
       "PHOOD11X01H_0027       ...            2      2     1                1     1   \n",
       "PHOOD11X01H_0028       ...            1      2     2                1     2   \n",
       "PHOOD11X01H_0029       ...            2      1     2                1     2   \n",
       "PHOOD11X01H_0030       ...            2      1     1                1     1   \n",
       "...                    ...          ...    ...   ...              ...   ...   \n",
       "PHOOD11X01H_0066       ...            1      1     2                1     2   \n",
       "PHOOD11X01H_0067       ...            2      2     2                2     2   \n",
       "PHOOD11X01H_0068       ...            1      2     1                2     1   \n",
       "PHOOD11X01H_0069       ...            1      1     2                1     2   \n",
       "PHOOD11X01H_0070       ...            1      1     1                2     1   \n",
       "PHOOD11X01H_0071       ...            1      1     1                2     1   \n",
       "PHOOD11X01H_0072       ...            2      0     1                2     1   \n",
       "PHOOD11X01H_0073       ...            2      2     1                1     1   \n",
       "PHOOD11X01H_0074       ...            1      2     1                2     1   \n",
       "PHOOD11X01H_0075       ...            2      2     2                1     2   \n",
       "PHOOD11X01H_0076       ...            2      1     2                1     2   \n",
       "PHOOD11X01H_0077       ...            2      2     2                2     2   \n",
       "PHOOD11X01H_0078       ...            1      1     1                1     1   \n",
       "PHOOD11X01H_0079       ...            2      2     2                1     2   \n",
       "PHOOD11X01H_0080       ...            2      2     2                1     2   \n",
       "PHOOD11X01H_0081       ...            2      2     2                1     2   \n",
       "PHOOD11X01H_0082       ...            2      2     2                2     2   \n",
       "PHOOD11X01H_0083       ...            2      2     2                2     2   \n",
       "PHOOD11X01H_0084       ...            2      2     2                1     2   \n",
       "PHOOD11X01H_0085       ...            2      2     2                1     2   \n",
       "PHOOD11X01H_0086       ...            2      1     1                2     1   \n",
       "PHOOD11X01H_0087       ...            2      1     1                1     1   \n",
       "PHOOD11X01H_0088       ...            1      2     2                2     2   \n",
       "PHOOD11X01H_0089       ...            1      1     1                1     1   \n",
       "PHOOD11X01H_0090       ...            1      1     2                2     2   \n",
       "PHOOD11X01H_0091       ...            2      1     1                1     1   \n",
       "PHOOD11X01H_0092       ...            1      1     2                2     2   \n",
       "PHOOD11X01H_0093       ...            0      2     1                1     1   \n",
       "PHOOD11X01H_0094       ...            1      2     1                2     1   \n",
       "PHOOD11X01H_0095       ...            2      1     1                2     1   \n",
       "\n",
       "                  9954  9962  99702  9983  9988_pink_01_x1  \n",
       "PHOOD11X01H_0001     1     2      1     2                1  \n",
       "PHOOD11X01H_0002     1     1      1     1                1  \n",
       "PHOOD11X01H_0003     2     2      2     1                2  \n",
       "PHOOD11X01H_0004     2     2      0     1                2  \n",
       "PHOOD11X01H_0005     1     2      1     1                2  \n",
       "PHOOD11X01H_0006     2     1      2     2                2  \n",
       "PHOOD11X01H_0007     2     2      1     1                1  \n",
       "PHOOD11X01H_0008     1     2      2     2                1  \n",
       "PHOOD11X01H_0009     1     2      1     1                1  \n",
       "PHOOD11X01H_0010     2     2      2     1                2  \n",
       "PHOOD11X01H_0011     1     1      1     1                1  \n",
       "PHOOD11X01H_0012     2     2      0     1                2  \n",
       "PHOOD11X01H_0013     1     1      1     2                1  \n",
       "PHOOD11X01H_0014     1     1      1     2                1  \n",
       "PHOOD11X01H_0015     2     2      1     2                1  \n",
       "PHOOD11X01H_0016     1     1      1     1                2  \n",
       "PHOOD11X01H_0017     2     2      1     2                2  \n",
       "PHOOD11X01H_0018     1     2      1     2                1  \n",
       "PHOOD11X01H_0019     1     2      1     1                1  \n",
       "PHOOD11X01H_0020     1     2      1     1                2  \n",
       "PHOOD11X01H_0021     1     2      2     2                2  \n",
       "PHOOD11X01H_0022     2     1      0     1                1  \n",
       "PHOOD11X01H_0023     2     1      2     1                2  \n",
       "PHOOD11X01H_0024     2     2      1     2                1  \n",
       "PHOOD11X01H_0025     1     1      2     1                1  \n",
       "PHOOD11X01H_0026     1     1      2     1                1  \n",
       "PHOOD11X01H_0027     1     2      2     1                1  \n",
       "PHOOD11X01H_0028     1     2      2     2                1  \n",
       "PHOOD11X01H_0029     2     1      1     1                2  \n",
       "PHOOD11X01H_0030     1     2      1     1                1  \n",
       "...                ...   ...    ...   ...              ...  \n",
       "PHOOD11X01H_0066     2     2      1     1                1  \n",
       "PHOOD11X01H_0067     1     1      0     1                1  \n",
       "PHOOD11X01H_0068     2     2      1     2                1  \n",
       "PHOOD11X01H_0069     2     1      2     2                2  \n",
       "PHOOD11X01H_0070     2     1      2     1                2  \n",
       "PHOOD11X01H_0071     1     2      0     2                1  \n",
       "PHOOD11X01H_0072     2     2      2     2                2  \n",
       "PHOOD11X01H_0073     1     1      1     1                2  \n",
       "PHOOD11X01H_0074     1     1      1     1                2  \n",
       "PHOOD11X01H_0075     1     1      1     1                1  \n",
       "PHOOD11X01H_0076     1     1      1     2                1  \n",
       "PHOOD11X01H_0077     1     1      0     2                1  \n",
       "PHOOD11X01H_0078     1     1      2     2                1  \n",
       "PHOOD11X01H_0079     1     2      1     2                1  \n",
       "PHOOD11X01H_0080     2     2      2     1                1  \n",
       "PHOOD11X01H_0081     1     1      2     2                1  \n",
       "PHOOD11X01H_0082     1     1      2     1                1  \n",
       "PHOOD11X01H_0083     2     1      1     2                1  \n",
       "PHOOD11X01H_0084     2     1      1     2                2  \n",
       "PHOOD11X01H_0085     2     1      2     1                2  \n",
       "PHOOD11X01H_0086     2     1      0     1                1  \n",
       "PHOOD11X01H_0087     1     1      1     2                2  \n",
       "PHOOD11X01H_0088     1     2      1     2                2  \n",
       "PHOOD11X01H_0089     2     2      0     1                1  \n",
       "PHOOD11X01H_0090     1     2      0     2                2  \n",
       "PHOOD11X01H_0091     2     2      2     2                2  \n",
       "PHOOD11X01H_0092     1     2      2     2                1  \n",
       "PHOOD11X01H_0093     1     1      1     1                2  \n",
       "PHOOD11X01H_0094     1     2      2     2                1  \n",
       "PHOOD11X01H_0095     2     1      1     2                1  \n",
       "\n",
       "[93 rows x 4772 columns]>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "renamed_01.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### this should be moved inside function\n",
    "renamed_110t = renamed_110.transpose()\n",
    "renamed_108t = renamed_108.transpose()\n",
    "renamed_01t = renamed_01.transpose()\n",
    "renamed_05t = renamed_05.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aa = pd.merge(left = renamed_110t, right = renamed_108t , how = 'outer', left_index =True, right_index =True)\n",
    "bb = pd.merge(left = aa, right = renamed_01t, how = 'outer', left_index =True, right_index =True) \n",
    "cc = pd.merge(left = bb, right = renamed_05t, how = 'outer', left_index =True, right_index =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_my_data, loci_all = prepare_matrix(cc.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_LEPmap(families, family_names, loci, genotypes, output_filename):\n",
    "    with open(output_filename, 'w') as OUTFILE:\n",
    "        header = \"\\t\".join([\"#family\", 'name', 'sire', 'dam', 'sex', 'blank'] + loci) + \"\\n\"\n",
    "        OUTFILE.write(header)\n",
    "        for fam_idx, fam in enumerate(families):\n",
    "            fam_name = family_names[fam_idx]\n",
    "            DAM_line = \"\\t\".join([fam_name, fam_name + \"_Dam\", '0', '0', '2', '0'] + ['1 1' for xx in loci]) + \"\\n\"\n",
    "            SIRE_line = \"\\t\".join([fam_name, fam_name + \"_Sire\", '0', '0', '1', '0'] + ['1 2' for xx in loci]) + \"\\n\"\n",
    "            OUTFILE.write(DAM_line)\n",
    "            OUTFILE.write(SIRE_line)\n",
    "            for ind in fam:\n",
    "                ind_info = \"\\t\".join([fam_name, ind, fam_name + \"_Sire\", fam_name + \"_Dam\", '0', '0'])\n",
    "                ind_genotypes = genotypes.loc[ind]\n",
    "                OUTFILE.write(ind_info + \"\\t\" + \"\\t\".join([str(xx) for xx in ind_genotypes]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fams = [individuals_110, individuals_108, individuals_01, individuals_05]\n",
    "LEPmap_filename = ('all_loci3.lepmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_genotypes = cc.transpose()\n",
    "my_genotypes = my_genotypes.replace(to_replace = [np.NaN, 0, 1, 2 ], value = ['0 0', '0 0', '1 1', '1 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(334, 13790)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_genotypes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_LEPmap(families = fams, family_names = ['pink_110','pink_108','pink_01', 'pink_05'], \n",
    "             loci = loci_all, genotypes = my_genotypes, output_filename = LEPmap_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(342, 13796)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('all_loci3.lepmap', sep = '\\t').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Run LepMap once to split chromosomes and to join singles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java -cp bin/ SeparateChromosomes data = all_loci2.lepmap lodLimit = 10 sizeLimit = 3 > all_loci2.inital_chromosomes 2> all_loci2.inital_chromosomes.log\n",
      "java -cp bin/ JoinSingles all_loci2.inital_chromosomes lodLimit = 8 data=all_loci2.lepmap > all_loci2.lod8_singles.chromosomes 2> all_loci2.lod8_singles.chromosomes.log\n",
      "java -cp bin/ JoinSingles all_loci2.lod8_singles.chromosomes lodLimit = 7 data= all_loci2.lepmap > all_loci2.lod7_singles.chromosomes 2> all_loci2.lod7_singles.chromosomes.log\n",
      "java -cp bin/ JoinSingles all_loci2.lod7_singles.chromosomes lodLimit = 6 data= all_loci2.lepmap > all_loci2.lod6_singles.chromosomes 2> all_loci2.lod6_singles.chromosomes.log\n",
      "java -cp bin/ JoinSingles all_loci2.lod6_singles.chromosomes lodLimit = 5 data= all_loci2.lepmap > all_loci2.lod5_singles.chromosomes 2> all_loci2.lod5_singles.chromosomes.log\n",
      "java -cp bin/ JoinSingles all_loci2.lod5_singles.chromosomes lodLimit = 4 data= all_loci2.lepmap > all_loci2.lod4_singles.chromosomes 2> all_loci2.lod4_singles.chromosomes.log\n",
      "java -cp bin/ JoinSingles all_loci2.lod4_singles.chromosomes lodLimit = 3.5 data= all_loci2.lepmap > all_loci2.lod3-5_singles.chromosomes 2> all_loci2.lod3-5_singles.chromosomes.log\n"
     ]
    }
   ],
   "source": [
    "print \"java -cp bin/ SeparateChromosomes data = all_loci2.lepmap lodLimit = 10 sizeLimit = 3 > all_loci2.inital_chromosomes 2> all_loci2.inital_chromosomes.log\"\n",
    "print \"java -cp bin/ JoinSingles all_loci2.inital_chromosomes lodLimit = 8 data=all_loci2.lepmap > all_loci2.lod8_singles.chromosomes 2> all_loci2.lod8_singles.chromosomes.log\"\n",
    "print \"java -cp bin/ JoinSingles all_loci2.lod8_singles.chromosomes lodLimit = 7 data= all_loci2.lepmap > all_loci2.lod7_singles.chromosomes 2> all_loci2.lod7_singles.chromosomes.log\"\n",
    "print \"java -cp bin/ JoinSingles all_loci2.lod7_singles.chromosomes lodLimit = 6 data= all_loci2.lepmap > all_loci2.lod6_singles.chromosomes 2> all_loci2.lod6_singles.chromosomes.log\"\n",
    "print \"java -cp bin/ JoinSingles all_loci2.lod6_singles.chromosomes lodLimit = 5 data= all_loci2.lepmap > all_loci2.lod5_singles.chromosomes 2> all_loci2.lod5_singles.chromosomes.log\"\n",
    "print \"java -cp bin/ JoinSingles all_loci2.lod5_singles.chromosomes lodLimit = 4 data= all_loci2.lepmap > all_loci2.lod4_singles.chromosomes 2> all_loci2.lod4_singles.chromosomes.log\"\n",
    "print \"java -cp bin/ JoinSingles all_loci2.lod4_singles.chromosomes lodLimit = 3.5 data= all_loci2.lepmap > all_loci2.lod3-5_singles.chromosomes 2> all_loci2.lod3-5_singles.chromosomes.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Use the output of LepMap to figure out which of the duplicated loci that have the same name in more than one family are on the same Linkage groups. Those will be collapsed down to have one name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_duplicate_names(paralogs_file, loci, LG_file, output_file):\n",
    "    with open(paralogs_file) as INFILE: \n",
    "        paralogs = [yy.strip() for yy in INFILE.readlines()]\n",
    "    #get list of the locus names\n",
    "        #  given by loci\n",
    "    #get list of LG assignments\n",
    "    with open(LG_file) as INFILE:\n",
    "        #skip first line\n",
    "        next(INFILE)\n",
    "        LG_assignments = [int(line.strip()) for line in INFILE]\n",
    "    LG_of_locus = dict(zip(loci, LG_assignments))\n",
    "    #print(paralogs)\n",
    "    with open(output_file, 'w') as OUTFILE:\n",
    "        OUTFILE.write(\"{}\\t{}\\t{}\\t{}\\n\".format('catalog_name', 'family', 'copy', 'LG'))\n",
    "        for locus in loci:\n",
    "            catalog_name = locus.split(\"_\")[0]\n",
    "            if catalog_name in paralogs:\n",
    "                family = locus.split(\"_\")[2]\n",
    "                copy = locus[-2:]\n",
    "                OUTFILE.write(\"{}\\t{}\\t{}\\t{}\\n\".format(catalog_name, family, copy, LG_of_locus[locus]))\n",
    "            #list of all loci sharing base name\n",
    "                sharing = [loc for loc in loci if locus.split(\"_\")[0] == loc.split(\"_\")[0]]\n",
    "                sharing.remove(locus)\n",
    "                agree_on_LG = sum([1 for loc in sharing if LG_of_locus[loc] == LG_of_locus[locus] ])\n",
    "                disagree_on_LG = sum([1 for loc in sharing if LG_of_locus[loc] != LG_of_locus[locus] ])\n",
    "                #print(locus, agree_on_LG, disagree_on_LG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "find_duplicate_names(paralogs_file = paralogs_file , loci = loci_all, LG_file = 'all_loci2.lod5_singles.chromosomes', output_file = 'LG_congruence3.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collapse_names(LG_110_x1, LG_110_x2, LG_108_x1, LG_108_x2, LG_01_x1, LG_01_x2, LG_05_x1, LG_05_x2):\n",
    "    # take them in order given here\n",
    "    possible_results = ['A','B','C','D','E','F','G','H']\n",
    "    #possible_results.reverse()\n",
    "    result = []\n",
    "    \n",
    "    # check for segmental\n",
    "    # if segmental do not try to resolve\n",
    "    for xx in ((LG_110_x1, LG_110_x2), (LG_108_x1, LG_108_x2), (LG_01_x1, LG_01_x2), (LG_05_x1, LG_05_x2)):\n",
    "        x1, x2 = xx\n",
    "        if x1 == x2 and x1 != 0: # segmental\n",
    "            result = possible_results\n",
    "\n",
    "    else:\n",
    "        mapping = dict()\n",
    "        for assign in (LG_110_x1, LG_110_x2, LG_108_x1, LG_108_x2, LG_01_x1, LG_01_x2, LG_05_x1, LG_05_x2):\n",
    "            if assign in mapping:\n",
    "                result.append(mapping[assign])\n",
    "            else:\n",
    "                if assign == 0:\n",
    "                     result.append(possible_results.pop(0))\n",
    "                else:\n",
    "                    mapping[assign] = possible_results.pop(0)\n",
    "                    result.append(mapping[assign])\n",
    "    return(result)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_LG_congruence_line(line):\n",
    "    catalog_name, family, copy, LG = line.strip().split(\"\\t\")\n",
    "    return(catalog_name, family, copy, LG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'B', 'A', 'B', 'A', 'B', 'A']\n",
      "['A', 'B', 'C', 'D', 'C', 'E', 'C', 'B']\n",
      "['F', 'G', 'H', 'A', 'B', 'C', 'C', 'A', 'D', 'C', 'E']\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n",
      "['F', 'G', 'H', 'A', 'B', 'C', 'D', 'E', 'C', 'C', 'C']\n"
     ]
    }
   ],
   "source": [
    "#examples\n",
    "print collapse_names(1,2,2,1,2,1,2,1)\n",
    "print collapse_names(0,1,2,0,2,0,2,1)\n",
    "print collapse_names(1,0,2,2,1,0,2,0)\n",
    "print collapse_names(0,0,1,0,2,0,0,0)\n",
    "print collapse_names(0,1,2,0,0,2,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_rename_table(LG_congruence_file, out_file):\n",
    "    famLG_of_locus = collections.defaultdict(dict)\n",
    "    with open(out_file, 'w') as OUTFILE:\n",
    "        OUTFILE.write(\"{}\\t{}\\t{}\\n\".format(\"old_name\", \"new_name\", \"LG\"))\n",
    "        with open(LG_congruence_file) as PARALOG_LGS:\n",
    "            #skip first line\n",
    "            next(PARALOG_LGS)\n",
    "            for line in PARALOG_LGS:\n",
    "                catalog_name, family, copy, LG = parse_LG_congruence_line(line)\n",
    "                famLG_of_locus[catalog_name][family, copy] = int(LG)\n",
    "            for cn, famLG in famLG_of_locus.items():\n",
    "                LG_110_x1 = famLG.get(('110', 'x1'), 0)\n",
    "                LG_110_x2 = famLG.get(('110', 'x2'), 0)\n",
    "                LG_108_x1 = famLG.get(('108', 'x1'), 0)\n",
    "                LG_108_x2 = famLG.get(('108', 'x2'), 0)\n",
    "                LG_01_x1 = famLG.get(('01', 'x1'), 0)\n",
    "                LG_01_x2 = famLG.get(('01', 'x2'), 0)\n",
    "                LG_05_x1 = famLG.get(('05', 'x1'), 0)\n",
    "                LG_05_x2 = famLG.get(('05', 'x2'), 0)\n",
    "                #print cn, [LG_110_x1, LG_110_x2, LG_108_x1, LG_108_x2, LG_01_x1, LG_01_x2, LG_05_x1, LG_05_x2], collapse_names(LG_110_x1, LG_110_x2, LG_108_x1, LG_108_x2, LG_01_x1, LG_01_x2, LG_05_x1, LG_05_x2)\n",
    "                fam_names = [\"{}_pink_110_x1\", \"{}_pink_110_x2\", \"{}_pink_108_x1\", \"{}_pink_108_x2\", \"{}_pink_01_x1\", \"{}_pink_01_x2\", \"{}_pink_05_x1\", \"{}_pink_05_x2\"]\n",
    "                collaped_names = collapse_names(LG_110_x1, LG_110_x2, LG_108_x1, LG_108_x2, LG_01_x1, LG_01_x2, LG_05_x1, LG_05_x2)\n",
    "                LGs = [LG_110_x1, LG_110_x2, LG_108_x1, LG_108_x2, LG_01_x1, LG_01_x2, LG_05_x1, LG_05_x2]\n",
    "                family_specific_names = [xx.format(cn) for xx in fam_names]\n",
    "                for cnt, fsn in enumerate(family_specific_names):\n",
    "                    OUTFILE.write(\"{}\\t{}\\t{}\\n\".format(fsn, cn+\"_{}\".format(collaped_names[cnt]), LGs[cnt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_rename_table(LG_congruence_file =  \"LG_congruence3.tsv\", out_file = \"rename_table3.tsv\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get rename dict\n",
    "rename_table = pd.read_table(\"rename_table3.tsv\")\n",
    "rename_dict = dict(zip(rename_table.old_name,rename_table.new_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "renamed_110t = renamed_110.rename(columns=rename_dict).transpose()\n",
    "renamed_108t = renamed_108.rename(columns=rename_dict).transpose()\n",
    "renamed_01t = renamed_01.rename(columns=rename_dict).transpose()\n",
    "renamed_05t = renamed_05.rename(columns=rename_dict).transpose()\n",
    "aa = pd.merge(left = renamed_110t, right = renamed_108t, how = 'outer', left_index =True, right_index =True)\n",
    "bb = pd.merge(left = aa, right = renamed_01t, how = 'outer', left_index =True, right_index =True)\n",
    "cc = pd.merge(left = bb, right = renamed_05t, how = 'outer', left_index =True, right_index =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fams = [individuals_110, individuals_108, individuals_01, individuals_05]\n",
    "LEPmap_filename = \"collapsed_loci2.lepmap\"\n",
    "my_genotypes = cc.transpose()\n",
    "my_genotypes = my_genotypes.replace(to_replace = [np.NaN, 0, 1, 2 ], value = ['0 0', '0 0', '1 1', '1 2'])\n",
    "write_LEPmap(families = fams, family_names = [\"pink_110\", \"pink_108\", \"pink_01\", \"pink_05\"], loci = my_genotypes.columns.values.tolist(),\n",
    "    genotypes = my_genotypes, output_filename = LEPmap_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Run LepMap again with the newly renamed loci, the loci with the same base name in multiple families that mapped to the same LG have been collapsed down to one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java -cp bin/ SeparateChromosomes data=collapsed_loci.lepmap lodLimit = 10 sizeLimit = 20 > collapsed_loci.inital.chromosomes 2> collapsed_loci.initial_chromosomes.log\n",
      "java -cp bin/ JoinSingles collapsed_loci.initial_chromosomes lodLimit = 8 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci.lod8_singles.chromosomes 2> collapsed_loci.lod8_singles.chromosomes.log\n",
      "java -cp bin/ JoinSingles collapsed_loci.lod8_singles.chromosomes lodLimit = 7 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci.lod7_singles.chromosomes 2> collapsed_loci.lod7_singles.chromosomes.log\n",
      "java -cp bin/ JoinSingles collapsed_loci.lod7_singles.chromosomes lodLimit = 6 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci.lod6_singles.chromosomes 2> collapsed_loci.lod6_singles.chromosomes.log\n",
      "java -cp bin/ JoinSingles collapsed_loci.lod6_singles.chromosomes lodLimit = 5 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci.lod5_singles.chromosomes 2> collapsed_loci.lod5_singles.chromosomes.log\n"
     ]
    }
   ],
   "source": [
    "# form linkage groups\n",
    "print \"java -cp bin/ SeparateChromosomes data=collapsed_loci.lepmap lodLimit = 10 sizeLimit = 20 > collapsed_loci.inital.chromosomes 2> collapsed_loci.initial_chromosomes.log\"\n",
    "print \"java -cp bin/ JoinSingles collapsed_loci.initial_chromosomes lodLimit = 8 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci.lod8_singles.chromosomes 2> collapsed_loci.lod8_singles.chromosomes.log\"\n",
    "print \"java -cp bin/ JoinSingles collapsed_loci.lod8_singles.chromosomes lodLimit = 7 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci.lod7_singles.chromosomes 2> collapsed_loci.lod7_singles.chromosomes.log\"\n",
    "print \"java -cp bin/ JoinSingles collapsed_loci.lod7_singles.chromosomes lodLimit = 6 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci.lod6_singles.chromosomes 2> collapsed_loci.lod6_singles.chromosomes.log\"\n",
    "print \"java -cp bin/ JoinSingles collapsed_loci.lod6_singles.chromosomes lodLimit = 5 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci.lod5_singles.chromosomes 2> collapsed_loci.lod5_singles.chromosomes.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###There are 25 linkage groups at the end of this step, which means that there is likely some overmerging. We are expecting there to be 26 linkage groups. (R B Phillips and A R Kapuscinski, 1988) 52 chromosomes in a normal diploid pink salmon, occasionally 53 chromosomes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java -cp bin/ SeparateChromosomes data=collapsed_loci.lepmap lodLimit = 11 sizeLimit = 20 > collapsed_loci_11.initial.chromosomes 2> collapsed_loci_11.initial_chromosomes.log\n",
      "java -cp bin/ JoinSingles collapsed_loci_11.initial.chromosomes lodLimit = 8 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_11.lod8_singles.chromosomes 2> collapsed_loci_11.lod8_singles.chromosomes.log\n",
      "java -cp bin/ JoinSingles collapsed_loci_11.lod8_singles.chromosomes lodLimit = 7 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_11.lod7_singles.chromosomes 2> collapsed_loci_11.lod7_singles.chromosomes.log\n",
      "java -cp bin/ JoinSingles collapsed_loci_11.lod7_singles.chromosomes lodLimit = 6 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_11.lod6_singles.chromosomes 2> collapsed_loci_11.lod6_singles.chromosomes.log\n",
      "java -cp bin/ JoinSingles collapsed_loci_11.lod6_singles.chromosomes lodLimit = 5 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_11.lod5_singles.chromosomes 2> collapsed_loci_11.lod5_singles.chromosomes.log\n"
     ]
    }
   ],
   "source": [
    "# form linkage groups\n",
    "\n",
    "print \"java -cp bin/ SeparateChromosomes data=collapsed_loci.lepmap lodLimit = 11 sizeLimit = 20 > collapsed_loci_11.initial.chromosomes 2> collapsed_loci_11.initial_chromosomes.log\"\n",
    "print \"java -cp bin/ JoinSingles collapsed_loci_11.initial.chromosomes lodLimit = 8 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_11.lod8_singles.chromosomes 2> collapsed_loci_11.lod8_singles.chromosomes.log\"\n",
    "print \"java -cp bin/ JoinSingles collapsed_loci_11.lod8_singles.chromosomes lodLimit = 7 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_11.lod7_singles.chromosomes 2> collapsed_loci_11.lod7_singles.chromosomes.log\"\n",
    "print \"java -cp bin/ JoinSingles collapsed_loci_11.lod7_singles.chromosomes lodLimit = 6 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_11.lod6_singles.chromosomes 2> collapsed_loci_11.lod6_singles.chromosomes.log\"\n",
    "print \"java -cp bin/ JoinSingles collapsed_loci_11.lod6_singles.chromosomes lodLimit = 5 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_11.lod5_singles.chromosomes 2> collapsed_loci_11.lod5_singles.chromosomes.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Testing the LOD scores to find 26 Linkage Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"java -cp bin/ SeparateChromosomes data=collapsed_loci.lepmap lodLimit = 12 sizeLimit = 20 > collapsed_loci_12.initial.chromosomes 2> collapsed_loci_12.initial_chromosomes.log\"\n",
    "print \"java -cp bin/ SeparateChromosomes data=collapsed_loci.lepmap lodLimit = 13 sizeLimit = 20 > collapsed_loci_13.initial.chromosomes 2> collapsed_loci_13.initial_chromosomes.log\"\n",
    "print \"java -cp bin/ SeparateChromosomes data=collapsed_loci.lepmap lodLimit = 14 sizeLimit = 20 > collapsed_loci_14.initial.chromosomes 2> collapsed_loci_14.initial_chromosomes.log\"\n",
    "print \"java -cp bin/ SeparateChromosomes data=collapsed_loci.lepmap lodLimit = 15 sizeLimit = 20 > collapsed_loci_15.initial.chromosomes 2> collapsed_loci_15.initial_chromosomes.log\"\n",
    "print \"java -cp bin/ SeparateChromosomes data=collapsed_loci.lepmap lodLimit = 16 sizeLimit = 20 > collapsed_loci_16.initial.chromosomes 2> collapsed_loci_16.initial_chromosomes.log\"\n",
    "print \"java -cp bin/ SeparateChromosomes data=collapsed_loci.lepmap lodLimit = 17 sizeLimit = 20 > collapsed_loci_17.initial.chromosomes 2> collapsed_loci_17.initial_chromosomes.log\"\n",
    "print \"java -cp bin/ SeparateChromosomes data=collapsed_loci.lepmap lodLimit = 18 sizeLimit = 20 > collapsed_loci_18.initial.chromosomes 2> collapsed_loci_18.initial_chromosomes.log\"\n",
    "print \"java -cp bin/ SeparateChromosomes data=collapsed_loci.lepmap lodLimit = 19 sizeLimit = 20 > collapsed_loci_19.initial.chromosomes 2> collapsed_loci_19.initial_chromosomes.log\"\n",
    "print \"java -cp bin/ SeparateChromosomes data=collapsed_loci.lepmap lodLimit = 20 sizeLimit = 20 > collapsed_loci_20.initial.chromosomes 2> collapsed_loci_20.initial_chromosomes.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LOD 10: 25 lgs 13027 markers in lgs 763 singles\n",
    "LOD 11: 25 lgs 12328 markers in lgs 7769 singles\n",
    "LOD 12: 25 lgs 12318 markers in lgs 779 singles\n",
    "LOD 13: 25 lgs 12294 markers in lgs 803 singles\n",
    "LOD 14: 25 lgs 12273 markers in lgs 824 singles\n",
    "LOD 15: 25 lgs 12186 markers in lgs 911 singles\n",
    "LOD 16: 26 lgs 12076 markers in lgs 1021 singles\n",
    "LOD 17: 27 lgs 11833 markers in lgs 1264 singles\n",
    "LOD 18: 27 lgs 11483 markers in lgs 1614 singles\n",
    "LOD 19: 30 lgs 10871 markers in lgs 2226 singles\n",
    "LOD 20: 33 lgs 9357 markers in lgs 3740 singles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java -cp bin/ SeparateChromosomes data=collapsed_loci.lepmap lodLimit = 16 sizeLimit = 20 > collapsed_loci_16.initial.chromosomes 2> collapsed_loci_16.initial_chromosomes.log\n",
      "java -cp bin/ JoinSingles collapsed_loci_16.initial.chromosomes lodLimit = 8 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_16.lod8_singles.chromosomes 2> collapsed_loci_16.lod8_singles.chromosomes.log\n",
      "java -cp bin/ JoinSingles collapsed_loci_16.lod8_singles.chromosomes lodLimit = 7 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_16.lod7_singles.chromosomes 2> collapsed_loci_16.lod7_singles.chromosomes.log\n",
      "java -cp bin/ JoinSingles collapsed_loci_16.lod7_singles.chromosomes lodLimit = 6 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_16.lod6_singles.chromosomes 2> collapsed_loci_16.lod6_singles.chromosomes.log\n",
      "java -cp bin/ JoinSingles collapsed_loci_16.lod6_singles.chromosomes lodLimit = 5 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_16.lod5_singles.chromosomes 2> collapsed_loci_16.lod5_singles.chromosomes.log\n",
      "java -cp bin/ JoinSingles collapsed_loci_16.lod5_singles.chromosomes lodLimit = 4 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_16.lod4_singles.chromosomes 2> collapsed_loci_16.lod4_singles.chromosomes.log\n",
      "java -cp bin/ JoinSingles collapsed_loci_16.lod4_singles.chromosomes lodLimit = 3.5 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_16.lod3-5_singles.chromosomes 2> collapsed_loci_16.lod3-5_singles.chromosomes.log\n"
     ]
    }
   ],
   "source": [
    "print \"java -cp bin/ SeparateChromosomes data=collapsed_loci.lepmap lodLimit = 16 sizeLimit = 20 > collapsed_loci_16.initial.chromosomes 2> collapsed_loci_16.initial_chromosomes.log\"\n",
    "\n",
    "print \"java -cp bin/ JoinSingles collapsed_loci_16.initial.chromosomes lodLimit = 8 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_16.lod8_singles.chromosomes 2> collapsed_loci_16.lod8_singles.chromosomes.log\"\n",
    "\n",
    "print \"java -cp bin/ JoinSingles collapsed_loci_16.lod8_singles.chromosomes lodLimit = 7 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_16.lod7_singles.chromosomes 2> collapsed_loci_16.lod7_singles.chromosomes.log\"\n",
    "\n",
    "print \"java -cp bin/ JoinSingles collapsed_loci_16.lod7_singles.chromosomes lodLimit = 6 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_16.lod6_singles.chromosomes 2> collapsed_loci_16.lod6_singles.chromosomes.log\"\n",
    "\n",
    "print \"java -cp bin/ JoinSingles collapsed_loci_16.lod6_singles.chromosomes lodLimit = 5 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_16.lod5_singles.chromosomes 2> collapsed_loci_16.lod5_singles.chromosomes.log\"\n",
    "\n",
    "print \"java -cp bin/ JoinSingles collapsed_loci_16.lod5_singles.chromosomes lodLimit = 4 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_16.lod4_singles.chromosomes 2> collapsed_loci_16.lod4_singles.chromosomes.log\"\n",
    "\n",
    "print \"java -cp bin/ JoinSingles collapsed_loci_16.lod4_singles.chromosomes lodLimit = 3.5 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_16.lod3-5_singles.chromosomes 2> collapsed_loci_16.lod3-5_singles.chromosomes.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Using this perl script to figure out which of the linkage groups are the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SepChrom_SepChrom_compare_all.pl\n",
    "\n",
    "#!/usr/bin/perl -w\n",
    "use strict;\n",
    "\n",
    "my$MSTfile1=$ARGV[0];\n",
    "my$MSTfile2=$ARGV[1];\n",
    "\n",
    "my%MST1loci;\n",
    "my$MST1currentLG;\n",
    "my$MST1store_loci=0;\n",
    "my%MST1LGlociNum;\n",
    "#my%MST1LGlength;\n",
    "my$MST1lociNum=0;\n",
    "my$MST1currentPosition;\n",
    "my$MST1locus=0;\n",
    "open(MST1, \"<$MSTfile1\")||die \"cannot open $MSTfile1:$!\";\n",
    "my$header=<MST1>;\n",
    "while(my $line=<MST1>){\n",
    "\tchomp $line;\n",
    "\t$MST1locus++;\n",
    "\t$MST1loci{$MST1locus}=$line;\n",
    "\t$MST1LGlociNum{$line}++;\n",
    "\t# if($line=~/^group/){\n",
    "\t\t# my($discard, $LG)=split \" \", $line, 2;\n",
    "\t\t# $MST1currentLG=$LG;\n",
    "\t# }\n",
    "\t# if($line=~/\\;BEGINOFGROUP/){\n",
    "\t\t# $MST1store_loci=1;\n",
    "\t\t# next;\n",
    "\t# }\n",
    "\t# if($line=~/\\;ENDOFGROUP/){\n",
    "\t\t# $MST1LGlociNum{$MST1currentLG}=$MST1lociNum;\n",
    "\t\t# $MST1LGlength{$MST1currentLG}=$MST1currentPosition;\n",
    "\t\t# $MST1store_loci=0;\n",
    "\t\t# $MST1currentLG=\"\";\n",
    "\t\t# $MST1currentPosition=\"\";\n",
    "\t\t# $MST1lociNum=0;\n",
    "\t# }\n",
    "\t# if($MST1store_loci==1){\n",
    "\t\t# my($locus, $position)=split \"\\t\", $line, 2;\n",
    "\t\t# #$locus=~s/R//;\n",
    "\t\t# $MST1loci{$locus}=$MST1currentLG;\n",
    "\t\t# $MST1currentPosition=$position;\n",
    "\t\t# $MST1lociNum++;\n",
    "\t# }\n",
    "}\n",
    "close MST1;\n",
    "\n",
    "my%MST2loci;\n",
    "my$MST2currentLG;\n",
    "my$MST2store_loci=0;\n",
    "my%MST2LGlociNum;\n",
    "#my%MST2LGlength;\n",
    "my$MST2lociNum=0;\n",
    "my$MST2currentPosition;\n",
    "my$MST2locus=0;\n",
    "\n",
    "open(MST2, \"<$MSTfile2\")||die \"cannot open $MSTfile2:$!\";\n",
    "$header=<MST2>;\n",
    "while(my $line=<MST2>){\n",
    "\tchomp $line;\n",
    "\t$MST2locus++;\n",
    "\t$MST2loci{$MST2locus}=$line;\n",
    "\t$MST2LGlociNum{$line}++;\n",
    "\t# if($line=~/^group/){\n",
    "\t\t# my($discard, $LG)=split \" \", $line, 2;\n",
    "\t\t# $MST2currentLG=$LG;\n",
    "\t# }\n",
    "\t# if($line=~/\\;BEGINOFGROUP/){\n",
    "\t\t# $MST2store_loci=1;\n",
    "\t\t# next;\n",
    "\t# }\n",
    "\t# if($line=~/\\;ENDOFGROUP/){\n",
    "\t\t# $MST2LGlociNum{$MST2currentLG}=$MST2lociNum;\n",
    "\t\t# $MST2LGlength{$MST2currentLG}=$MST2currentPosition;\n",
    "\t\t# $MST2store_loci=0;\n",
    "\t\t# $MST2currentLG=\"\";\n",
    "\t\t# $MST2currentPosition=\"\";\n",
    "\t\t# $MST2lociNum=0;\n",
    "\t# }\n",
    "\t# if($MST2store_loci==1){\n",
    "\t\t# my($locus, $position)=split \"\\t\", $line, 2;\n",
    "\t\t# #$locus=~s/R//;\n",
    "\t\t# $MST2loci{$locus}=$MST2currentLG;\n",
    "\t\t# $MST2currentPosition=$position;\n",
    "\t\t# $MST2lociNum++;\n",
    "\t\t# #print \"$locus\\t$MST2currentLG\\n\";\n",
    "\t# }\n",
    "}\n",
    "close MST2;\n",
    "\n",
    "my%LGpairs;\n",
    "foreach my$key (keys%MST1loci){\n",
    "\t#my$Rkey=$key.\"R\";\n",
    "\t#if(((exists $MST1loci{$key})&&(exists $MST2loci{$key}))||((exists $MST1loci{$Rkey})&&(exists $MST2loci{$key}))){\n",
    "\tif((exists $MST1loci{$key})&&(exists $MST2loci{$key})){\n",
    "\t\t#print \"$key\\n\";\n",
    "\t\t#print \"$MST1loci{$key}\\t$MST2loci{$key}\\n\";\n",
    "\t\tmy$pair=\"$MST1loci{$key} $MST1LGlociNum{$MST1loci{$key}}\\t$MST2loci{$key} $MST2LGlociNum{$MST2loci{$key}}\";\n",
    "\t\t$LGpairs{$pair}++;\n",
    "\t}\n",
    "}\n",
    "\n",
    "foreach my$key (sort keys %LGpairs){\n",
    "\tprint \"$key\\t$LGpairs{$key}\\n\";\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Ordering Markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=27 > chr_27.map 2> chr_27.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=25 > chr_25.map 2> chr_25.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=23 > chr_23.map 2> chr_23.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=21 > chr_21.map 2> chr_21.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=19 > chr_19.map 2> chr_19.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=17 > chr_17.map 2> chr_17.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=15 > chr_15.map 2> chr_15.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=13 > chr_13.map 2> chr_13.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=11 > chr_11.map 2> chr_11.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=9 > chr_9.map 2> chr_9.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=7 > chr_7.map 2> chr_7.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=5 > chr_5.map 2> chr_5.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=3 > chr_3.map 2> chr_3.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=1 > chr_1.map 2> chr_1.map.log\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for xx in reversed(range(1,29,2)):\n",
    "    print(\"java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome={} > chr_{}.map 2> chr_{}.map.log\\n\".format(xx, xx, xx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=26 > chr_26.map 2> chr_26.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=24 > chr_24.map 2> chr_24.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=22 > chr_22.map 2> chr_22.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=20 > chr_20.map 2> chr_20.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=18 > chr_18.map 2> chr_18.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=16 > chr_16.map 2> chr_16.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=14 > chr_14.map 2> chr_14.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=12 > chr_12.map 2> chr_12.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=10 > chr_10.map 2> chr_10.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=8 > chr_8.map 2> chr_8.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=6 > chr_6.map 2> chr_6.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=4 > chr_4.map 2> chr_4.map.log\n",
      "\n",
      "java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=2 > chr_2.map 2> chr_2.map.log\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for xx in reversed(range(2,28,2)):\n",
    "    print(\"java -cp bin OrderMarkers map=collapsed_loci_16.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome={} > chr_{}.map 2> chr_{}.map.log\\n\".format(xx, xx, xx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Plotting the linkage map in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I took the results of the run and compiled them in a single txt file, with four columns. The first is the linkage group, the second is the locus name, the third is the position and the fourth is whether it is a paralog (listed in the pink_paralogs2.txt file that is output from an earlier step). This file is saved as the MAP.txt, and only the paralogs of this file are saved as the MAPdups.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(stringr)\n",
    "library(plyr)\n",
    "\n",
    "\n",
    "map_positions <- read.table(\"G:\\\\Analysis\\\\Mapping\\\\AllHaps\\\\LepMap\\\\MAP.txt\", sep = \"\\t\", header = TRUE)\n",
    "duplicates <- read.table(\"G:\\\\Analysis\\\\Mapping\\\\AllHaps\\\\LepMap\\\\MAPdups.txt\", sep = \"\\t\", header = TRUE)\n",
    "\n",
    "###this one works ok to show duplicates in the data\n",
    "dup_color <-c(yes = \"#000f2d\", no = \"#ffd9f2\")\n",
    "ggplot(data = map_positions) + geom_point(aes(x = LG, y = position, color = paralog), alpha = .5, size = 3) + scale_colour_manual(values = dup_color)+ theme_classic()\n",
    "\n",
    "\n",
    "##plotting only the duplicates\n",
    "ggplot(data = duplicates) + geom_point(aes(x = LG, y = position), alpha = .5, size = 3, color = \"blue\") \n",
    "\n",
    "\n",
    "##this jitters the duplicates so you can see them \n",
    "ggplot(data = duplicates, aes(x = LG, y = position)) + geom_point(position = position_jitter(w = 0.3, h = 0.3), alpha = .5, size = 3, color = \"firebrick\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 13th chromosome looks different than the others, it has a large gap and is less compact. I used Garretts SepChrom_SepChrom_compare_all.pl script from earlier to compare the results of the Separate Chromosome runs from earlier to see if there is an obvious fragmentation or overmerging issue that has to do with differnt LOD scores at that stage. There were no overlaps with the 13th linkage group, they all matched one to one in the different LOD score runs. Garrett recommmends that I re-order the linkage group using the output of a more conservative initial separateChromosome run. I'll rty a LOD score of 17. I have the Separate Chromosomes, but need the results of the Join Singles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java -cp bin/ JoinSingles collapsed_loci_17.initial.chromosomes lodLimit = 8 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_17.lod8_singles.chromosomes 2> collapsed_loci_17.lod8_singles.chromosomes.log\n",
      "java -cp bin/ JoinSingles collapsed_loci_17.lod8_singles.chromosomes lodLimit = 7 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_17.lod7_singles.chromosomes 2> collapsed_loci_17.lod7_singles.chromosomes.log\n",
      "java -cp bin/ JoinSingles collapsed_loci_17.lod7_singles.chromosomes lodLimit = 6 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_17.lod6_singles.chromosomes 2> collapsed_loci_17.lod6_singles.chromosomes.log\n",
      "java -cp bin/ JoinSingles collapsed_loci_17.lod6_singles.chromosomes lodLimit = 5 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_17.lod5_singles.chromosomes 2> collapsed_loci_17.lod5_singles.chromosomes.log\n",
      "java -cp bin/ JoinSingles collapsed_loci_17.lod5_singles.chromosomes lodLimit = 4 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_17.lod4_singles.chromosomes 2> collapsed_loci_17.lod4_singles.chromosomes.log\n",
      "java -cp bin/ JoinSingles collapsed_loci_17.lod4_singles.chromosomes lodLimit = 3.5 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_17.lod3-5_singles.chromosomes 2> collapsed_loci_17.lod3-5_singles.chromosomes.log\n"
     ]
    }
   ],
   "source": [
    "print \"java -cp bin/ JoinSingles collapsed_loci_17.initial.chromosomes lodLimit = 8 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_17.lod8_singles.chromosomes 2> collapsed_loci_17.lod8_singles.chromosomes.log\"\n",
    "\n",
    "print \"java -cp bin/ JoinSingles collapsed_loci_17.lod8_singles.chromosomes lodLimit = 7 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_17.lod7_singles.chromosomes 2> collapsed_loci_17.lod7_singles.chromosomes.log\"\n",
    "\n",
    "print \"java -cp bin/ JoinSingles collapsed_loci_17.lod7_singles.chromosomes lodLimit = 6 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_17.lod6_singles.chromosomes 2> collapsed_loci_17.lod6_singles.chromosomes.log\"\n",
    "\n",
    "print \"java -cp bin/ JoinSingles collapsed_loci_17.lod6_singles.chromosomes lodLimit = 5 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_17.lod5_singles.chromosomes 2> collapsed_loci_17.lod5_singles.chromosomes.log\"\n",
    "\n",
    "print \"java -cp bin/ JoinSingles collapsed_loci_17.lod5_singles.chromosomes lodLimit = 4 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_17.lod4_singles.chromosomes 2> collapsed_loci_17.lod4_singles.chromosomes.log\"\n",
    "\n",
    "print \"java -cp bin/ JoinSingles collapsed_loci_17.lod4_singles.chromosomes lodLimit = 3.5 lodDifference=3 data=collapsed_loci.lepmap > collapsed_loci_17.lod3-5_singles.chromosomes 2> collapsed_loci_17.lod3-5_singles.chromosomes.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "java -cp bin OrderMarkers map=collapsed_loci_17.lod8_singles.chromosomes data=collapsed_loci.lepmap alpha=1 maxDistance=30 useKosambi=1 initError=0.001 initRecombination=0.01 0.01 chromosome=13 > LOD17_chr_13.map 2> LOD17_chr_13.map.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Comparing MSTMap and LepMap results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Data: \n",
    "    LepMap output from the run with a Separate Chromosome LOD 17 and a Join Singles LOD 8 \n",
    "    MSTMap output from: \n",
    "                    PBIRD13X108H_80GR \n",
    "                    PBIRD13X110H_65GR\n",
    "                    PHOOD11x01H_4\n",
    "                    PHOOD11x05H_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
