{
 "metadata": {
  "name": "",
  "signature": "sha256:38d4bbf7cf641c9c867aa3d197e63c989a72fe363102c5275302e7acaa403607"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Outlier tests"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Bayenv2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "I downloaded Bayenv2 from their github and talked to Ryan about it, he had some good ideas: \n",
      "\n",
      "\n",
      "there are three files that you need:\n",
      "one is the genotypes file, (got it, same format as the Bayescan file)\n",
      "\n",
      "the second is a file with the environmental variable. the variable should be standardized, so that the mean is zero and they have a standard deviation of one. his instructions for this were average, divide and subtract. I think ill google it. but he also said that the longitude should be all together, so that they are on the same scale, dont treat them as seperate coast lines with different scales. And I brought up that i had thought he mean to put them on a circle, where the center of the space between them was a zero, and they were around that spot in a circle, he said that is a good idea, i should do that too, that I can do more than one way. But i dont really know how to get that value, he said i could plot the PCA of them, which is pretty interesting. He says that the order matters, you need one line for the environmental gradient and one column for each of the populations. he said another good one is temperature, to look up in some papers that do this sort of thing and pick their variable, then go to some site that has all the data we need. I can even just pick the latitude off the map and then have temperature for some close by site, it doesn't have to be that exact place. it could be something like the temperature in August or something general like that. \n",
      "\n",
      "the third file you need is the covariance matrix. this one the program will help you make. it just makes a ton of them (or you can ask it to) and they should all be pretty much identical. From there you can take the average of a couple or you can just pick one of them. Just look through them and make sure that they are all pretty much the same. \n",
      "\n",
      "the output of bayenv2 is more raw than you would expect, and the key is that you make sure that the outputs converged, so assess it to make sure that the MCMC chain actually reached a convergence point. this is to show at the end that it stabilized. and to assess that it did. Ryan said there are a couple ways to do this, and that i can read up on it. \n",
      "\n",
      "I read the manual and the imput file is not the geste format. its just allele counts, and its two lines per count. So i googled it and found some code that takes structure files and gives the allele counts for this very reason. its the same guy who has a blog on popgen coding: \n",
      "\n",
      "oK, ITS a bit of a mess and is taking forever so i decided that using the geste format, it wouldnt be that hard to edit myself into the right format. Im doing that right now. in excel. nope. that doesnt work either. \n",
      "\n",
      "i think i have to write something in python that will do it. I got ryans help and have been looking through some of the code I already have. Ryan sent me this, its nested dictionaries. \n",
      "\n",
      "\n",
      "I am going to finish the conversion of the bayesenv file today. Its hard to get back into it and figure out where and why i had given up last time. Luckily, i had basically gotten the dictionary in a dictionary format to work, but was running into some minor errors and then couldnt get the output started. \n",
      "\n",
      "i asked for ryans help and he sat with me for about 40 minutes and looked at the code and then just kindof ended up doing it for me which is what i needed. I was at my wits end having worked on it for so long without actually ever finishing it. \n",
      "\n",
      "This is the finished product that he came up with: \n",
      "\n",
      "#convert_bayescan_to_bayenv2.py\n",
      "\n",
      "###Python code to convert a bayescan input file to Bayesenv2\n",
      "###Carolyn Tarpey and Ryan Waples | September 2015\n",
      "\n",
      "###takes two arguments: 1) bayescan input file, 2) output file name for Bayesenv2\n",
      "# ie: convert_bayescan_to_bayenv2.py 16681_80_bayescan.txt 16681_80_bayenv2_in.txt\n",
      "\n",
      "#!/bin/bash\n",
      "import sys\n",
      "import re\n",
      "\n",
      "pat1 = r'[pop]='\n",
      "AC_of_pop = dict()\n",
      "\n",
      "with open(sys.argv[1], 'r') as INFILE:\n",
      "\tfor line in INFILE:\n",
      "\t\tif pat1 in line:\n",
      "\t\t\tpopsplit = line.strip().split('=')\n",
      "\t\t\tcurrent_pop = int(popsplit[1])\n",
      "\t\t\t#print current_pop\n",
      "\t\t\tAC_of_locus = dict()\n",
      "\t\t\t\n",
      "\t\telif line == '\\n':\n",
      "\t\t\tcontinue\n",
      "\t\telse: \n",
      "\t\t\tlinesplit = line.strip().split(\"\\t\")\n",
      "\t\t\tcurrent_locus = int(linesplit[0])\n",
      "\t\t\tallele_count_A = linesplit[3]\n",
      "\t\t\tallele_count_B = linesplit[4]\n",
      "\t\t\t#put the allele counts as the value in the dict with the locus as the key\n",
      "\t\t\tAC_list = [allele_count_A, allele_count_B]\n",
      "\t\t\tAC_of_locus[current_locus] = (AC_list)\n",
      "\t\t\t#put that dict in the overall dict with the pop as the key\n",
      "\t\t\tAC_of_pop[current_pop] = AC_of_locus\n",
      "\n",
      "keys = AC_of_pop.keys()\n",
      "print keys\n",
      "\n",
      "with open(sys.argv[2], 'w') as OUTFILE:\n",
      "\tfor locus in sorted(AC_of_locus.keys()):\n",
      "\t\ttop_line = [] \n",
      "\t\tbottom_line = [] \n",
      "\t\tfor pop in sorted(AC_of_pop.keys()):\n",
      "\t\t\ttop_line.append(AC_of_pop[pop][locus][0])\n",
      "\t\t\tbottom_line.append(AC_of_pop[pop][locus][1])\n",
      "\t\t\n",
      "\t\tOUTFILE.write('\\t'.join(top_line))\n",
      "\t\tOUTFILE.write('\\t\\n')\n",
      "\t\tOUTFILE.write('\\t'.join(bottom_line))\n",
      "\t\tOUTFILE.write('\\t\\n')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "############# test the order of the output data\n",
      "\n",
      "# with open(sys.argv[2], 'w') as OUTFILE:\n",
      "\t# for locus in sorted(AC_of_locus.keys()):\n",
      "\t\t# top_line = [] \n",
      "\t\t# bottom_line = [] \n",
      "\t\t# for pop in sorted(AC_of_pop.keys()): \n",
      "\t\t\t# top_line.append(str(pop))\n",
      "\t\t\t# top_line.append(str(locus))\n",
      "\t\t\t# bottom_line.append(str(pop))\n",
      "\t\t\t# bottom_line.append(str(locus))\n",
      "\t\t\n",
      "\t\t# OUTFILE.write('\\t'.join(top_line))\n",
      "\t\t# OUTFILE.write('\\t\\n')\n",
      "\t\t# OUTFILE.write('\\t'.join(bottom_line))\n",
      "\t\t# OUTFILE.write('\\t\\n')\n",
      "\n",
      "# with open(sys.argv[2], 'w') as OUTFILE:\n",
      "\t# for locus in sorted(AC_of_locus.keys()):\n",
      "\t\t# top_line = [] \n",
      "\t\t# bottom_line = [] \n",
      "\t\t# for pop in sorted(AC_of_pop.keys()):\n",
      "\t\t\t# top_line.append(AC_of_pop[pop][locus][0])\n",
      "\t\t\t# top_line.append(str(locus))\n",
      "\t\t\t# bottom_line.append(AC_of_pop[pop][locus][1])\n",
      "\t\t\t# bottom_line.append(str(locus))\n",
      "\t\t\n",
      "\t\t# OUTFILE.write('\\t'.join(top_line))\n",
      "\t\t# OUTFILE.write('\\t\\n')\n",
      "\t\t# OUTFILE.write('\\t'.join(bottom_line))\n",
      "\t\t# OUTFILE.write('\\t\\n')\t\t\t\t\t\n",
      "\t\t\n",
      "#for pop, ac_dict in AC_of_pop.items():\n",
      "#\tprint pop\n",
      "#\tprint ac_dict.items()\n",
      "#\tprint '\\n'\n",
      "\n",
      "\n",
      "\n",
      "Gettign ready to run the program\n",
      "\n",
      "runs on linux side, only polymorphic loci in the dataset (arlequin has told us which are polymorphic for each of the subsets) \n",
      "environmental data has to in order of the populations\n",
      "\n",
      "\n",
      "the manual says that we have to have one file for each of the snps, but it seems like he has changed that in the newest version: added a bash script which loops through all SNPs in a file. \n",
      "\n",
      "So im not going to try to figure out how to make that until i realize that i have to. I may have to run it on Ryans or Garretts computer though, I dont think i have the capability to run the newest version on my computer because i have the linux 32 bit system. :/\n",
      "\n",
      "\n",
      "\n",
      "it wants me to come up with a matrix file as the input where  you also get one as an output file. so maybe i use the one that they provide in the example bundle? Im not really sure whats up with that. \n",
      "\n",
      "but ill try to get it all running tomorrow, and I have to think about the tests that i want to run with this and make the input files for them. \n",
      "\n",
      "\n",
      "         $$ September 9 2015\n",
      "==================================================================\n",
      "\n",
      "I want to move forward with the Bayenv2 run today- but it might make more sense to set it up on RYans or garretts computer, considering how fast these runs of Bayescenv finished compared to when i had them running on here. \n",
      "\n",
      "I need the Matrix file and im still not really sure what to do- do you start it with the example and then get your own? or do you estimate your own using a separate process? \n",
      "\n",
      "./bayenv2 -i SNPSFILE -p NUMPOPS -k 100000 -r 14637 >matrix.out\n",
      "\n",
      "./bayenv2 -i 16681_80_bayenv2_in.txt -p 14 -k 100000 -r 14637 >matrix.out\n",
      "(draws the covariance matrix into matrix.out every 500 iterations)\n",
      "\n",
      "Im wondering if I need a separate one for each of the runs? My guess is yes? because theyre all different? Then what about the polymorphic loci thing? I wonder if I need to adjust the other runs of the programs to only be the polymorphic loci? \n",
      "\n",
      "anyway, i tried it on the linux side and the core dumped\n",
      "./bayenv2_32bit -i 16681_80_bayenv2_in.txt -p 14 -k 100000 -r 14637 >matrix.out\n",
      "\n",
      "i tried this on Garretts and so far its working. He has the 64 bit version of Linux so i was able to use that program version that is the one that is maintained.\n",
      "\n",
      "./bayenv2 -i 16681_80_bayenv2_in.txt -p 14 -k 100000 -r 14637 >matrix.out 2>&1 matrix_log.txt\n",
      "\n",
      "In the mean time, Ill get the files ready to run the individual covariance matrixes on.\n",
      "\n",
      "\n",
      "Ok, back to converting the files. I need pgdspider to convert from the genepop file to the bayescan file. \n",
      " \n",
      "i have the genepop files from arlequin, each is all the populations in it, all 14 even though they are the data sets for just some of the populations. i opened them up and took out the other populations and named them with the NA or ASIAN. i get them into bayescan format (geste) using the pgdspider and then manually edited them, converting the spaces to tabs (i guess i could totally code this in but whatever) then use the script to get them into the bayesenv2 format.\n",
      "  \n",
      "INFO  13:34:36 - loading SPID file: genepop_bayescan.spid\n",
      "INFO  13:34:37 - Convert...\n",
      "INFO  13:34:37 - convert GENEPOP:15945_ASIAN_polymorphic.txt to GESTE / BayeScan:15945_polymorphic_ASIAN_bayescan.txt\n",
      "INFO  13:37:47 - translation completed\n",
      "INFO  13:38:47 - Convert...\n",
      "INFO  13:38:47 - convert GENEPOP:16409_NA_polymorphic.txt to GESTE / BayeScan:16409_polymorphic_NA_bayescan.txt\n",
      "INFO  13:41:07 - translation completed\n",
      "\n",
      " convert_bayescan_to_bayenv2.py 15945_polymorphic_ASIAN_bayescan.txt 15945_polymorphic_ASIAN_bayenv2_in.txt\n",
      " \n",
      " convert_bayescan_to_bayenv2.py 16409_polymorphic_NA_bayescan.txt 16409_polymorphic_NA_bayenv2_in.txt\n",
      " \n",
      "sweet! done. \n",
      "\n",
      "theres something wrong with the 16409, there are some of the loci that only have one allele, so the conversion script wont work on them, and also they are wrong. So I went back to the original Arlequin file and got the original file, renamed it 16409_North_polymorphic.txt and restarted it in PGD spider ot get it converted to the bayescan format\n",
      "\n",
      "INFO  13:54:24 - Convert...\n",
      "INFO  13:54:24 - convert GENEPOP:16409_North_polymorphic.txt to GESTE / BayeScan:16409_North_polymorphic_bayescan.txt\n",
      "INFO  13:56:50 - translation completed\n",
      "\n",
      " convert_bayescan_to_bayenv2.py 16409_North_polymorphic_bayescan.txt 16409_polymorphic_North_bayenv2_in.txt\n",
      "\n",
      " same- still missing a single locus. \n",
      " \n",
      " I went back to the original genepop file again and deleted the title line, renamed it 16409_genepop.txt and tried pgdspider again\n",
      "It did not solve the problem. still missing a locus\n",
      "\n",
      "I tried it on the file that has all the individuals, north and asian. still not right. \n",
      "\n",
      "going to the source, to remake the file from the 16681 genepop. \n",
      "\n",
      "perl subset_genepop_by_SNPs.pl NApolymorphic.txt 16681_80_genepop_one_per_line.txt > 16409_revamp.txt\n",
      "\n",
      "made a new spid file to convert it in into a bayescan, but I didnt take the populations out, i feel like at this point if i can just get the right number of samples, i can do that in excel. \n",
      "\n",
      "it still failed to get the number of samples right. SO I went online and googled other ways of converting from genepop to bayescan format, and got this guys code from github: \n",
      "\n",
      "\n",
      "it still failed to get the number of samples right. SO I went online and googled other ways of converting from genepop to bayescan format, and got this guys code from github: \n",
      "\n",
      "#markravinet/bayescan_convert\n",
      "\n",
      "#bayescan_convert/big_convert.R\n",
      "#@markravinet markravinet on Jun 6, 2014 Script upload\n",
      "\n",
      "#### Convert LARGE genepop SNP file to BayesScan input file ####\n",
      "# To be used on cluster\n",
      "# Mark Ravinet September 2013, University of Gothenburg\n",
      "## Modified heavily from code provided by Kevin Keenan (cheers Kevin!)\n",
      "\n",
      "I opened it up in R and tried it on the 16409_revamp.txt file as the genepop, and tried to convert it to 16409_revamp_bayescan.txt, and its running right now. \n",
      "\n",
      "it gives 16409 loci, but the last one in each population is empty, like 0 0 for the count. So i opened the file that it takes in, the 16409_revamp and at the end of each line there are 2 tabs, and at the end of a line of a regular genepop file there are no tabs. so it has something to do with the additional tabs? \n",
      "\n",
      "i tried a couple things with the 16681_80_genepop_one_per_line_txt file and the subset script to see if I could get the right number, like change the line endings, remove the actual name of the population after POP and making sure that the last locus is actully in the file, but none of them are working. each time there are one too few loci and the end of the lines end in two tabs, where there should be no tabs. \n",
      "\n",
      "i tried to subset the genotyppes with the NA monomorphic list, and there were no problems. It came out perfectly fine. garrett looked at it with me and we decided that there is somethign wrong with this locus 100029_14 if you take it out, the file looks fine. the input file does have a genotype for it. so that is weird. the script to subset the genotypes doesnt have any digit limit on the name of the loci, so thats not an issue. \n",
      "\n",
      "Im worried that this locus has been missing or left out of all the other analyiss that was was done using a genepop file, but luckily since it was the last of the loci, it wouldnt have screwed up the indexing at all, since there is nothing that comes after it. \n",
      "\n",
      "I checked in batch_3_16681_pop_80.genepop the original file that doesnt even have one per line, and there were 16681 genotypes in the first sample the one that Ive been using to check every time. \n",
      "\n",
      "        $$ September 10 2015\n",
      "==================================================================\n",
      "lepMap at chr 4 and 5\n",
      "\n",
      "I talked to lisa about the problems I had had in Arlequin, and she said to forget that one locus 100029_14, let it go. Just make sure its the only  one that is missing from the files and just move on. (she also said to figure out if its the name or the fact that it is the last thing in the file) \n",
      "\n",
      "Then she looked at the results of the Arlequin and pulled up some of the projects that she had on her computer. It didnt look like she had used the hierarchical model, so that might be what was the difference? Anyway, she asked for my .arp file and is checking it out on her computer to see if she can get it to run.\n",
      "\n",
      "I went over to Garretts, and she is messing with the Asian arp so I started the ALL 16681 and the North American 15945- I think the secret is that they shouldnt be run with the hierarchical model because they were totally working. \n",
      "\n",
      "She said to plot the null distribution and then plot the outliers over top of it. \n",
      "\n",
      "I want to continue to put together the analysisMetaData sheet. \n",
      "\n",
      "Ok, So im looking for the results for NA to put in the sheet and in the only output that has the individual based F statistics, I find this: \n",
      "********************\n",
      "Locus by locus AMOVA:\n",
      "********************\n",
      "\n",
      "Distance method for locus-by-locus analysis: Pairwise differences\n",
      "\n",
      "List of loci with only one allele:\n",
      "----------------------------------\n",
      "\n",
      "94 172 180 200 206 228 249 266 294 307 377 515 528 576 642 692 700 743 746 747 812 817 818 829 889 916 925 962 996 1003 1023 1051 1057 1084 1095 1096 1121 1163 1213 1218 1248 1250 1274 1285 1358 1365 1402 1425 1450 1453 1457 1472 1490 1499 1574 1610 1662 1695 1704 1717 1719 1744 1750 1792 1807 1809 1813 1848 1899 1959 1983 1987 2000 2041 2075 2097 2111 2119 2124 2127 2144 2212 2216 2230 2242 2339 2373 2469 2478 2534 2537 2560 2566 2592 2613 2626 2644 2694 2724 2740 2751 2761 2792 2813 2833 2852 2865 2876 2884 2897 3060 3107 3128 3174 3189 3193 3203 3223 3237 3251 3356 3363 3414 3457 3472 3530 3541 3598 3654 3657 3671 3676 3687 3696 3772 3822 3843 3932 3936 3938 3940 4012 4034 4071 4086 4102 4125 4203 4247 4250 4266 4294 4302 4333 4362 4369 4371 4373 4478 4536 4554 4603 4620 4772 4851 4905 4909 4930 5043 5052 5066 5118 5161 5196 5282 5348 5354 5362 5385 5404 5412 5453 5466 5501 5549 5554 5559 5634 5635 5638 5794 5815 5860 5877 5879 5901 5902 5940 5947 5970 5975 5994 6028 6033 6059 6082 6093 6099 6108 6123 6131 6137 6212 6250 6329 6337 6368 6391 6486 6527 6546 6550 6556 6591 6612 6632 6636 6641 6649 6673 6680 6688 6709 6737 6755 6807 6814 6842 6845 6873 6889 6989 7041 7076 7086 7087 7108 7155 7185 7254 7277 7283 7284 7305 7334 7341 7527 7555 7619 7629 7630 7641 7663 7787 7809 7828 7839 7930 7950 7955 7992 8000 8022 8044 8049 8092 8168 8174 8194 8204 8235 8238 8246 8253 8270 8290 8302 8336 8372 8450 8458 8460 8515 8524 8569 8575 8576 8601 8606 8607 8632 8664 8684 8828 8836 8945 8972 8994 9052 9082 9152 9156 9197 9201 9216 9382 9395 9446 9475 9535 9584 9613 9615 9636 9648 9658 9669 9715 9724 9805 9852 9876 9990 10023 10083 10153 10161 10285 10287 10307 10343 10366 10406 10407 10410 10549 10561 10593 10646 10669 10705 10730 10748 10777 10780 10815 10818 10824 10855 10857 10883 10886 10890 10915 10979 11023 11034 11051 11074 11084 11153 11211 11224 11232 11301 11353 11365 11422 11444 11453 11462 11504 11517 11528 11596 11611 11674 11693 11858 11871 11906 11961 11985 12023 12028 12073 12078 12136 12156 12162 12166 12168 12174 12200 12219 12258 12281 12341 12410 12433 12446 12453 12474 12475 12494 12499 12546 12590 12615 12653 12711 12721 12728 12778 12800 12812 12828 12856 12861 12877 13018 13038 13045 13051 13086 13088 13109 13118 13207 13223 13246 13264 13290 13315 13332 13360 13388 13395 13396 13397 13461 13576 13598 13645 13746 13806 13858 13863 13874 13921 13953 14021 14059 14094 14103 14107 14130 14157 14209 14223 14227 14246 14259 14293 14300 14315 14342 14352 14353 14378 14379 14389 14400 14401 14403 14408 14455 14525 14531 14533 14551 14570 14592 14599 14626 14679 14725 14733 14737 14752 14754 14832 14860 14955 14965 14974 14975 15042 15056 15075 15081 15095 15111 15116 15123 15158 15178 15187 15202 15218 15232 15260 15265 15279 15283 15288 15293 15311 15315 15350 15354 15397 15408 15412 15416 15423 15457 15477 15488 15537 15545 15551 15574 15589 15595 15602 15607 15618 15665 15682 15704 15733 15744 15757 15798 15804 15811 15824 15835 15877 15900 15907 15917 15919 15923 15924 15926 15937 15997 16007 16016 16030 16047 16061 16096 16115 16124 16127 16146 16160 16189 16210 16236 16271 16312 16334 16361 16366 16370 16373 16389 16391 16402 \n",
      "\n",
      "which i totally thought I had already solved that problem when i made the sheets that cut out the monomorphic loci. what has changed since i cut it down from 16681 to 16409 that there are  now all these new monomorphic loci? could it be possible that I messed up and deleted the wrong populations? \n",
      "\n",
      "im sure that the original list came from the 16681_80_named_NA run and was using the NA pops:  \n",
      "\t 1   KOPPE96  -0.01734            0.615836\n",
      "     2    NOME94   0.01221            0.407625\n",
      "     3    SNOH96  -0.02381            0.770283\n",
      "     4   KOPPE91  -0.00009            0.517107\n",
      "     5    NOME91  -0.00913            0.621701\n",
      "     6    SNOH03  -0.01152            0.641251\n",
      "\t \n",
      "\t \n",
      "But it still doesnt solve the problem of like what is going on? \n",
      "\n",
      "\n",
      "From 15945_Asian2.res this is the results when you do  GENETIC STRUCTURE ANALYSIS\n",
      "\n",
      "\n",
      "Number of usable loci for distance computation : 15259\n",
      "Allowed level of missing data                  : 0.05\n",
      "\n",
      "\n",
      "List of loci with too much missing data :\n",
      "-----------------------------------------\n",
      "    4    41   142   150   156   172   195   241   286   287   292   307   330   344   353    432   434   436   453   497   504   512   514   519   546   558   577   584   602   618    625   628   662   666   706   745   746   772   785   790   796   802   808   809   813    820   821   831   865   879   900   917   944   965   975   999  1021  1033  1064  1066   1082  1105  1108  1133  1183  1239  1242  1289  1293  1308  1318  1331  1355  1361  1371   1377  1395  1411  1415  1425  1459  1480  1534  1566  1567  1577  1594  1604  1679  1736   1765  1796  1814  1835  1905  1917  1943  1961  2001  2011  2034  2040  2044  2060  2066   2087  2091  2171  2179  2180  2225  2256  2308  2357  2365  2366  2371  2402  2415  2511   2549  2593  2605  2610  2622  2624  2649  2655  2660  2674  2713  2752  2761  2783  2786   2801  2863  2887  2930  2936  2967  3038  3075  3111  3150  3159  3243  3252  3306  3312   3338  3380  3431  3437  3442  3462  3468  3473  3527  3563  3571  3599  3719  3744  3765   3766  3822  3835  3845  3849  3851  3878  3882  3884  3886  3956  3996  4045  4048  4079   4118  4145  4148  4172  4205  4242  4256  4270  4274  4333  4365  4368  4435  4452  4470   4477  4481  4489  4501  4518  4520  4538  4558  4566  4578  4586  4602  4609  4613  4636   4656  4679  4698  4719  4763  4788  4798  4835  4853  4858  4866  4876  4898  4918  4947   4957  4968  4993  4994  5001  5019  5050  5100  5117  5124  5140  5166  5198  5224  5257   5258  5264  5322  5331  5345  5346  5355  5360  5389  5423  5465  5475  5491  5512  5523   5525  5584  5594  5602  5624  5628  5659  5664  5666  5669  5672  5691  5719  5728  5748   5762  5800  5809  5854  5864  5868  5926  5933  5937  5940  5978  5980  5991  5998  6005   6034  6102  6114  6123  6152  6164  6175  6179  6181  6200  6238  6244  6269  6284  6289   6291  6295  6310  6321  6334  6366  6405  6456  6460  6486  6495  6504  6521  6580  6586   6632  6659  6666  6694  6719  6726  6745  6750  6771  6811  6836  6846  6860  6940  6996   7012  7013  7014  7041  7057  7113  7174  7187  7188  7205  7213  7214  7223  7228  7241   7273  7400  7429  7433  7439  7471  7474  7484  7503  7514  7515  7533  7561  7577  7583   7584  7627  7631  7634  7694  7696  7711  7769  7795  7918  7927  7939  7941  7943  7950   7951  7966  7973  7993  8011  8079  8084  8113  8147  8156  8181  8248  8264  8285  8294   8308  8319  8344  8351  8358  8376  8408  8441  8445  8448  8480  8551  8552  8662  8673   8721  8746  8757  8826  8892  8926  8979  8988  8990  9024  9037  9060  9062  9083  9103   9105  9165  9172  9186  9192  9195  9200  9228  9236  9245  9264  9270  9271  9309  9318   9353  9373  9379  9444  9467  9483  9500  9510  9556  9588  9597  9614  9684  9698  9729   9743  9782  9790  9841  9853  9871  9877  9935  9938  9952  9973  10009  10033  10078  10080   10099  10135  10136  10144  10192  10203  10219  10305  10308  10315  10337  10352  10435  10460  10491   10494  10515  10530  10538  10557  10574  10608  10610  10674  10740  10745  10780  10782  10798  10836   10858  10860  10870  10876  10901  10905  10917  10965  10996  11038  11047  11056  11066  11074  11086   11092  11115  11116  11179  11197  11239  11266  11274  11281  11321  11368  11385  11432  11433  11451   11465  11491  11559  11613  11690  11732  11782  11795  11842  11865  11916  11980  12162  12191  12204   12216  12217  12262  12326  12338  12415  12482  12540  12574  12613  12620  12656  12668  12715  12716   12749  12759  12763  12773  12878  12903  12957  12976  12977  13036  13043  13051  13082  13115  13206   13227  13241  13276  13285  13300  13401  13409  13416  13476  13502  13506  13519  13526  13529  13589   13591  13597  13606  13631  13634  13688  13697  13705  13707  13769  13806  13833  13880  13884  13890   13895  13957  14025  14026  14042  14101  14122  14135  14149  14159  14188  14208  14246  14263  14269   14282  14296  14321  14340  14444  14447  14454  14472  14483  14505  14506  14570  14623  14626  14631   14653  14708  14758  14772  14779  14810  14820  14823  14951  14966  14986  14993  14994  15020  15039   15052  15094  15118  15144  15145  15195  15196  15198  15204  15212  15232  15245  15330  15358  15391   15422  15434  15464  15470  15472  15474  15532  15545  15615  15654  15701  15703  15756  15757  15803   15808  15858  15876  15934  15935  15936  15937  15939  15941  15943  15944  \n",
      "\t\n",
      "And the output for the Asian 15045 original has the same list of loci with too much missing data. OOOOOugh what is going on?\n",
      " \n",
      "Ok, so the levels for the missing data are different for those. And the arlequin manual says that you can set that level in the general settings tab. \n",
      "\n",
      "Lisa talked with me a long time about the programs and what I should be looking for and doing, she couldnt get the hierarchical version of arlequin to run; she thought it was because she was using an newer version, I am running a version from 2010, she just downloaded the 2015 and it was giving the same errors that I was having all along. \n",
      "\n",
      "WE talked about using the program galaxy to do some summary stats on the files to explore what the data looks like. \n",
      "\n",
      "So I think i have to figure out what the MAF and missing data are in those new data sets, throw out that one locus that was giving me trouble in the conversion and go from there. I have the perl scripts that garrett wrote to count the missing and look at the hets, and I can do basic genepop stats with the files and throw shit out from that too. \n",
      "\n",
      "         $$ October 19  2015\n",
      "==================================================================\n",
      "\n",
      "So the next thing that i want to do is build the bayenv2 env files. so that i can get those started. I have the genotype file made, im pretty positive. Especially since I have already estimated the covariance matrix. I dont know if its one of those things where you have to estimate the covariance matrix each time you do it, or with each separate grouping of genotypes? my guess is yes probably. \n",
      "\n",
      " so today Im going to build inport files. \n",
      " \n",
      " \n",
      "asia_odd_lat\t\t\t2\t3\t8\t13\n",
      "asia_even_lat\t\t\t1\t4\t7\t14\n",
      "NA_odd_lat\t\t\t\t5\t9\t11\t\n",
      "NA_even_lat\t\t\t\t6\t10\t12\t\n",
      "\n",
      "\n",
      "this is what i ran for bayenv2 back in september: ./bayenv2_32bit -i 16681_80_bayenv2_in.txt -p 14 -k 100000 -r 14637 >matrix.out\n",
      "  \n",
      "and I have the input file! So I just need to edit it into the 4 groupings ASE ASO NAO NAE. \n",
      "\n",
      "i made the environmental files that would be formatted right for the LFMM, and I can use the NAO and NAE and compare the ASE and ASO with what I made and used earlier. \n",
      "The input files for the 4 lineage groupings are made for the bayenv2 file now. the environmental files are the same as is used for the LFMM. \n",
      "Im going to make the matrix file for each of the groups of inputs.. and im going to try the all file with the matrix that ive already made. \n",
      "\n",
      "Im looking at the instructions in the manual about the matrix and this is what it says: The command line for estimating the matrix is:\n",
      "\n",
      "./bayenv2 -i SNPSFILE -p NUMPOPS -k 100000 -r 63479 > matrix.out\n",
      "\n",
      "This outputs the current draw of the covariance matrix into matrix.out every 500 iterations. The rows and columns in the covariance matrix appear in the same population order as they appeared in the allele count file. The covariance matrix can be visualized by the image command in R, the cov2cor function can be used to convert a covariance matrix into a correlation matrix. The correlation matrix computed from the estimated covariance matrix should be very correlated with the matrix of pairwise FST (see the paper for discussion). The matrix should be inspected for unexpected low or high correlations, as judged from pairwise FST , as these may indicate problems in the labeling of populations. Note that the entries of the covariance matrix are not forced to be positive, thus small negative entries in the covariance matrix are possible. Matrices should be compared within an across independent runs of the program to ensure that the matrix is well estimated.  \n",
      "\n",
      "I am going to run the matrix for these four and then try to figure out what to do with the visualizing the matrix at the same time for all 5 of them. \n",
      "\n",
      "\n",
      "./bayenv2_32bit -i NAE_bayesenv_in.txt -p 3 -k 100000 -r 14637 >NAE_matrix.out\n",
      "./bayenv2_32bit -i NAO_bayesenv_in.txt -p 3 -k 100000 -r 14637 >NAO_matrix.out\n",
      "./bayenv2_32bit -i ASE_bayesenv_in.txt -p 4 -k 100000 -r 14637 >ASE_matrix.out\n",
      "./bayenv2_32bit -i ASO_bayesenv_in.txt -p 4 -k 100000 -r 14637 >ASO_matrix.out\n",
      "\n",
      " Ill run the LFMM for the North America set today too, and see what else is going on with the runs that i started on friday. \n",
      " \n",
      " the environmental files for Bayenv2 are the same as for the bayescenv. \n",
      " \n",
      " \n",
      " \n",
      "          $$ October 23  2015\n",
      "==================================================================\n",
      "\n",
      "\n",
      "Garrett figured out why the bayenv2 program was failing for so long, even though it was running (and crashing) on my computer. \n",
      "\n",
      "It turns out it cant handle cases where there are no alternate alleles between the populations. So if there are 3 populations, it can only take the polymorphic loci. Makes sense. \n",
      "\n",
      "So he wrote a perl script that filters out the loci that are monomorpic from the input files: \n",
      "\n",
      "#/usr/bin/perl -w\n",
      "use strict;\n",
      "\n",
      "my$inFile=$ARGV[0];\n",
      "my($file,$ext)=split '\\.', $inFile, 2;\n",
      "my$filteredOut=$file.\"_filtered.txt\";\n",
      "my$failedOut=$file.\"_failedLoci.txt\";\n",
      "\n",
      "\n",
      "open(INFILE,\"<$inFile\")||die \"cannot open $inFile:$!\";\n",
      "open(FILTERED,\">$filteredOut\")||die \"cannot open $filteredOut:$!\";\n",
      "open(FAILED,\">$failedOut\")||die \"cannot open $failedOut:$!\";\n",
      "my$locusCount=0;\n",
      "while (my$line=<INFILE>){\n",
      "\t$locusCount++;\n",
      "\tchomp $line;\n",
      "\tmy$line2=<INFILE>;\n",
      "\tchomp $line2;\n",
      "\tmy@col1=split \"\\t\", $line;\n",
      "\tmy@col2=split \"\\t\", $line2;\n",
      "\tmy$keep1=0;\n",
      "\tmy$keep2=0;\n",
      "\tforeach my$i (0..$#col1){\n",
      "\t\tif($col1[$i]>0){\n",
      "\t\t\t$keep1=1;\n",
      "\t\t}\n",
      "\t\tif($col2[$i]>0){\n",
      "\t\t\t$keep2=1;\n",
      "\t\t}\n",
      "\t}\n",
      "\tif(($keep1==1)&&($keep2==1)){\n",
      "\t\tprint FILTERED \"$line\\n$line2\\n\";\n",
      "\t}else{\n",
      "\t\tprint FAILED \"$locusCount\\n\";\n",
      "\t}\n",
      "}\n",
      "close INFILE;\n",
      "close FILTERED;\n",
      "close FAILED;\n",
      "\n",
      "\n",
      "So this is what I did: \n",
      "perl removeZeroCounts.pl NAE_bayesenv_in.txt\n",
      "perl removeZeroCounts.pl NAO_bayesenv_in.txt\n",
      "perl removeZeroCounts.pl ASE_bayesenv_in.txt\n",
      "perl removeZeroCounts.pl ASO_bayesenv_in.txt\n",
      "\n",
      "it creates a file that is the loci that passed the test, in the same format as the program needs and a list of the index of the loci that failed. It is starting at 1. \n",
      "\n",
      "\n",
      "This is the new shell script to run the program: \n",
      "\n",
      "./bayenv2 -i NAE_bayesenv_in_filtered.txt -p 3 -k 100000 >NAE_matrix.out\n",
      "./bayenv2 -i NAO_bayesenv_in_filtered.txt -p 3 -k 100000 >NAO_matrix.out\n",
      "./bayenv2 -i ASE_bayesenv_in_filtered.txt -p 4 -k 100000 >ASE_matrix.out\n",
      "./bayenv2 -i ASO_bayesenv_in_filtered.txt -p 4 -k 100000 >ASO_matrix.out\n",
      "\n",
      "I started it and its working! \n",
      "\n",
      "\n",
      "\n",
      "          $$ October 26  2015\n",
      "==================================================================\n",
      "\n",
      "talked to lisa, we talked about Bayenv2 and how the program distance transforms the variables (so it just takes the variables that I have already standardized and does the absolute value of them) which i think is a bad idea and im not sure why the program does it. it means that things that are really far on the edge of the scale have the same values after you take the absolute value of them. a -1.6 will be the same as a 1.6 when in fact theyre really really different. \n",
      "\n",
      "The nuthatch people used the standardized numbers and let the program take the absolute values, Lisa says that that doesnt seem right to her either and so I have to look into that a little more\n",
      "This is where I got their info: http://datadryad.org/resource/doi:10.5061/dryad.8hm4p\n",
      "\n",
      "        $$ October 27  2015\n",
      "==================================================================\n",
      "\n",
      "\n",
      "Talked to ryan about a host of issues. \n",
      "\n",
      "BAYENV2\t\n",
      "estimating the covariance matrix: \n",
      "\t1. what is the pop_spec.out file and is it OK that mine is all zeros? \n",
      "\t2. how do I know if the covariance matrices are a small set of very similar covariance matrices? \n",
      "\n",
      "in the environmental file, what is up with it taking the absolute value of the environmental variables? (distance transforming the varibles) Should I rescale or make all mine positive? \n",
      "\n",
      "\n",
      "\n",
      "interpreting the covariance matrix: \n",
      "\tthe diagonals are the variance within a population, and the others are the covariance between the populations, pairwise. So the closer the pops are to eachother, related wise, or the more similar they are, the bigger the number should be. if its zero than they are not related at all and knowing something about one population will tell you absolutely nothing about another population.  So the covariance matrices that i have are not right. \n",
      "\t\n",
      "\tthey should not be such tiny numbers. the covariance matrix for all the populations are all like this: \n",
      "\t\n",
      "\tVAR-COVAR MATRIX: ITER = 99500\n",
      "2.607952e-01\t2.408774e-01\t2.412271e-01\t2.611193e-01\t2.435891e-01\t2.611281e-01\t2.624981e-01\t2.411676e-01\t2.412103e-01\t2.600593e-01\t2.492190e-01\t2.627749e-01\t2.413656e-01\t2.603849e-01\t\n",
      "2.408774e-01\t2.439801e-01\t2.421166e-01\t2.420893e-01\t2.455869e-01\t2.411968e-01\t2.423487e-01\t2.445997e-01\t2.420919e-01\t2.405774e-01\t2.536834e-01\t2.427992e-01\t2.425350e-01\t2.413946e-01\t\n",
      "2.412271e-01\t2.421166e-01\t2.421246e-01\t2.424411e-01\t2.447295e-01\t2.416624e-01\t2.427418e-01\t2.435000e-01\t2.412529e-01\t2.409669e-01\t2.527309e-01\t2.433016e-01\t2.416363e-01\t2.417605e-01\t\n",
      "2.611193e-01\t2.420893e-01\t2.424411e-01\t2.632416e-01\t2.450296e-01\t2.624906e-01\t2.636209e-01\t2.423635e-01\t2.424429e-01\t2.613356e-01\t2.508730e-01\t2.642203e-01\t2.425988e-01\t2.616443e-01\t\n",
      "2.435891e-01\t2.455869e-01\t2.447295e-01\t2.450296e-01\t2.525655e-01\t2.456028e-01\t2.448108e-01\t2.469368e-01\t2.448334e-01\t2.435311e-01\t2.624722e-01\t2.482239e-01\t2.451158e-01\t2.442712e-01\t\n",
      "2.611281e-01\t2.411968e-01\t2.416624e-01\t2.624906e-01\t2.456028e-01\t2.643888e-01\t2.635775e-01\t2.413674e-01\t2.417208e-01\t2.615159e-01\t2.524568e-01\t2.656636e-01\t2.417774e-01\t2.617215e-01\t\n",
      "2.624981e-01\t2.423487e-01\t2.427418e-01\t2.636209e-01\t2.448108e-01\t2.635775e-01\t2.661666e-01\t2.426014e-01\t2.427120e-01\t2.626095e-01\t2.501021e-01\t2.651486e-01\t2.428876e-01\t2.629219e-01\t\n",
      "2.411676e-01\t2.445997e-01\t2.435000e-01\t2.423635e-01\t2.469368e-01\t2.413674e-01\t2.426014e-01\t2.471917e-01\t2.434992e-01\t2.408211e-01\t2.551210e-01\t2.429389e-01\t2.439459e-01\t2.416743e-01\t\n",
      "2.412103e-01\t2.420919e-01\t2.412529e-01\t2.424429e-01\t2.448334e-01\t2.417208e-01\t2.427120e-01\t2.434992e-01\t2.421314e-01\t2.409748e-01\t2.529524e-01\t2.433953e-01\t2.416149e-01\t2.417464e-01\t\n",
      "2.600593e-01\t2.405774e-01\t2.409669e-01\t2.613356e-01\t2.435311e-01\t2.615159e-01\t2.626095e-01\t2.408211e-01\t2.409748e-01\t2.612199e-01\t2.492890e-01\t2.632430e-01\t2.411145e-01\t2.606044e-01\t\n",
      "2.492190e-01\t2.536834e-01\t2.527309e-01\t2.508730e-01\t2.624722e-01\t2.524568e-01\t2.501021e-01\t2.551210e-01\t2.529524e-01\t2.492890e-01\t2.772195e-01\t2.559598e-01\t2.531670e-01\t2.500269e-01\t\n",
      "2.627749e-01\t2.427992e-01\t2.433016e-01\t2.642203e-01\t2.482239e-01\t2.656636e-01\t2.651486e-01\t2.429389e-01\t2.433953e-01\t2.632430e-01\t2.559598e-01\t2.692204e-01\t2.434116e-01\t2.634254e-01\t\n",
      "2.413656e-01\t2.425350e-01\t2.416363e-01\t2.425988e-01\t2.451158e-01\t2.417774e-01\t2.428876e-01\t2.439459e-01\t2.416149e-01\t2.411145e-01\t2.531670e-01\t2.434116e-01\t2.429301e-01\t2.418841e-01\t\n",
      "2.603849e-01\t2.413946e-01\t2.417605e-01\t2.616443e-01\t2.442712e-01\t2.617215e-01\t2.629219e-01\t2.416743e-01\t2.417464e-01\t2.606044e-01\t2.500269e-01\t2.634254e-01\t2.418841e-01\t2.617930e-01\t\n",
      "\n",
      "\n",
      "\n",
      "so that tells you a little bit about what we are going for. \n",
      "\n",
      "the covariance can be negative because, overall it should be positive, but what is happening is that its calculated on standardized data, so the data has a mean and it is telling us the deviation of them from the mean. so naturally some will fall above the mean and some will be below the mean. \n",
      "\n",
      "looking at the diagonals between the covariance matrices, they should be the ones that vary the least because they are just representing the variation within the populatins and that should be less variable between the runs, that should not be as dependent on the other things going on. \n",
      "\n",
      "the issue that I might be running into is that I may have hit the lower bound for the program. It may not be designed to work well on so few populations. it gets a lot of its power from the number of populations it has, so the greater the structure it can draw from. \n",
      "\n",
      "Recomputing the covariance each time, we should also be recomputing or restandardizing the environmental variables for each of the sets of populations that Im running. \n",
      "\n",
      "\n",
      "and remake the input files for the bayenv2 because that is not right.  do the coda in R to test the convergence. \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}