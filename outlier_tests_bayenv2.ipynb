{
 "metadata": {
  "name": "",
  "signature": "sha256:fbf1c5e2bd3b9c1236a82311f4373ed35953641da0bbf737940d05640f4cf171"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Outlier tests with the program Bayenv2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Bayenv2: \n",
      "http://gcbias.org/bayenv/\n",
      "    \n",
      "https://bitbucket.org/tguenther/bayenv2_public/src\n",
      "\n",
      "This program runs on a 64bit Linux system. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Background: \n",
      "I am running this program to identify outliers that are associated with environmental gradients for all \n",
      "\n",
      "##Input files: \n",
      "\n",
      "There are three files that you need to run this program: \n",
      "\n",
      "###The genotype file\n",
      "\n",
      "The genotype file is a format of the allele counts per locus per population, and is not in a format that is recognized by any other program. Unfortunately I have to write something that will take the GESTE file (has the allele counts) and convert it into this specialized format. \n",
      "\n",
      "This is the finished python script that Ryan and I came up with to convert a bayescan input file (GESTE format) into the specialized format needed for Bayenv2: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "#convert_bayescan_to_bayenv2.py\n",
      "\n",
      "###Python code to convert a bayescan input file to Bayesenv2\n",
      "###Carolyn Tarpey and Ryan Waples | September 2015\n",
      "\n",
      "###takes two arguments: 1) bayescan input file, 2) output file name for Bayesenv2\n",
      "# ie: convert_bayescan_to_bayenv2.py 16681_80_bayescan.txt 16681_80_bayenv2_in.txt\n",
      "\n",
      "#!/bin/bash\n",
      "import sys\n",
      "import re\n",
      "\n",
      "pat1 = r'[pop]='\n",
      "AC_of_pop = dict()\n",
      "\n",
      "with open(sys.argv[1], 'r') as INFILE:\n",
      "\tfor line in INFILE:\n",
      "\t\tif pat1 in line:\n",
      "\t\t\tpopsplit = line.strip().split('=')\n",
      "\t\t\tcurrent_pop = int(popsplit[1])\n",
      "\t\t\t#print current_pop\n",
      "\t\t\tAC_of_locus = dict()\n",
      "\t\t\t\n",
      "\t\telif line == '\\n':\n",
      "\t\t\tcontinue\n",
      "\t\telse: \n",
      "\t\t\tlinesplit = line.strip().split(\"\\t\")\n",
      "\t\t\tcurrent_locus = int(linesplit[0])\n",
      "\t\t\tallele_count_A = linesplit[3]\n",
      "\t\t\tallele_count_B = linesplit[4]\n",
      "\t\t\t#put the allele counts as the value in the dict with the locus as the key\n",
      "\t\t\tAC_list = [allele_count_A, allele_count_B]\n",
      "\t\t\tAC_of_locus[current_locus] = (AC_list)\n",
      "\t\t\t#put that dict in the overall dict with the pop as the key\n",
      "\t\t\tAC_of_pop[current_pop] = AC_of_locus\n",
      "\n",
      "keys = AC_of_pop.keys()\n",
      "print keys\n",
      "\n",
      "with open(sys.argv[2], 'w') as OUTFILE:\n",
      "\tfor locus in sorted(AC_of_locus.keys()):\n",
      "\t\ttop_line = [] \n",
      "\t\tbottom_line = [] \n",
      "\t\tfor pop in sorted(AC_of_pop.keys()):\n",
      "\t\t\ttop_line.append(AC_of_pop[pop][locus][0])\n",
      "\t\t\tbottom_line.append(AC_of_pop[pop][locus][1])\n",
      "\t\t\n",
      "\t\tOUTFILE.write('\\t'.join(top_line))\n",
      "\t\tOUTFILE.write('\\t\\n')\n",
      "\t\tOUTFILE.write('\\t'.join(bottom_line))\n",
      "\t\tOUTFILE.write('\\t\\n')\n",
      "\n",
      "\n",
      "############# test the order of the output data\n",
      "\n",
      "# with open(sys.argv[2], 'w') as OUTFILE:\n",
      "\t# for locus in sorted(AC_of_locus.keys()):\n",
      "\t\t# top_line = [] \n",
      "\t\t# bottom_line = [] \n",
      "\t\t# for pop in sorted(AC_of_pop.keys()): \n",
      "\t\t\t# top_line.append(str(pop))\n",
      "\t\t\t# top_line.append(str(locus))\n",
      "\t\t\t# bottom_line.append(str(pop))\n",
      "\t\t\t# bottom_line.append(str(locus))\n",
      "\t\t\n",
      "\t\t# OUTFILE.write('\\t'.join(top_line))\n",
      "\t\t# OUTFILE.write('\\t\\n')\n",
      "\t\t# OUTFILE.write('\\t'.join(bottom_line))\n",
      "\t\t# OUTFILE.write('\\t\\n')\n",
      "\n",
      "# with open(sys.argv[2], 'w') as OUTFILE:\n",
      "\t# for locus in sorted(AC_of_locus.keys()):\n",
      "\t\t# top_line = [] \n",
      "\t\t# bottom_line = [] \n",
      "\t\t# for pop in sorted(AC_of_pop.keys()):\n",
      "\t\t\t# top_line.append(AC_of_pop[pop][locus][0])\n",
      "\t\t\t# top_line.append(str(locus))\n",
      "\t\t\t# bottom_line.append(AC_of_pop[pop][locus][1])\n",
      "\t\t\t# bottom_line.append(str(locus))\n",
      "\t\t\n",
      "\t\t# OUTFILE.write('\\t'.join(top_line))\n",
      "\t\t# OUTFILE.write('\\t\\n')\n",
      "\t\t# OUTFILE.write('\\t'.join(bottom_line))\n",
      "\t\t# OUTFILE.write('\\t\\n')\t\t\t\t\t\n",
      "\t\t\n",
      "#for pop, ac_dict in AC_of_pop.items():\n",
      "#\tprint pop\n",
      "#\tprint ac_dict.items()\n",
      "#\tprint '\\n'\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Filtering out monomorphic loci\n",
      "\n",
      "The program only takes the polymorphic loci in each group, so the 16681 loci in the data set that has 14 populations are all polymorphic, but when you break out the lineages or the rivers there are loci in that set that are not polymorphic. For those groupings, the monomorphic loci should be filtered out of the genotype file. I have these lists of monomorphic loci from running Arlequin.  \n",
      "\n",
      "Garrett wrote a perl script that filters out the loci that are monomorpic from the input files: \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#/usr/bin/perl -w\n",
      "use strict;\n",
      "\n",
      "my$inFile=$ARGV[0];\n",
      "my($file,$ext)=split '\\.', $inFile, 2;\n",
      "my$filteredOut=$file.\"_filtered.txt\";\n",
      "my$failedOut=$file.\"_failedLoci.txt\";\n",
      "\n",
      "\n",
      "open(INFILE,\"<$inFile\")||die \"cannot open $inFile:$!\";\n",
      "open(FILTERED,\">$filteredOut\")||die \"cannot open $filteredOut:$!\";\n",
      "open(FAILED,\">$failedOut\")||die \"cannot open $failedOut:$!\";\n",
      "my$locusCount=0;\n",
      "while (my$line=<INFILE>){\n",
      "\t$locusCount++;\n",
      "\tchomp $line;\n",
      "\tmy$line2=<INFILE>;\n",
      "\tchomp $line2;\n",
      "\tmy@col1=split \"\\t\", $line;\n",
      "\tmy@col2=split \"\\t\", $line2;\n",
      "\tmy$keep1=0;\n",
      "\tmy$keep2=0;\n",
      "\tforeach my$i (0..$#col1){\n",
      "\t\tif($col1[$i]>0){\n",
      "\t\t\t$keep1=1;\n",
      "\t\t}\n",
      "\t\tif($col2[$i]>0){\n",
      "\t\t\t$keep2=1;\n",
      "\t\t}\n",
      "\t}\n",
      "\tif(($keep1==1)&&($keep2==1)){\n",
      "\t\tprint FILTERED \"$line\\n$line2\\n\";\n",
      "\t}else{\n",
      "\t\tprint FAILED \"$locusCount\\n\";\n",
      "\t}\n",
      "}\n",
      "close INFILE;\n",
      "close FILTERED;\n",
      "close FAILED;\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Workflow for creating the genotype files"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are two main ways to make the genotype files. \n",
      "\n",
      "A. \n",
      "\n",
      "1. Take the genepop file that has all the populations and edit it down to the group being tested \n",
      "\n",
      "2. Use PGDspider to convert the genepop to Bayescan format (GESTE)\n",
      "\n",
      "3. Edit the Bayescan format by hand: \n",
      "    Convert the spaces to Tabs\n",
      "    \n",
      "4. Use the python script to convert it to the Bayenv2 format\n",
      "\n",
      "5. Use the perl script to remove the monomorphic loci. \n",
      "\n",
      "\n",
      "B. \n",
      "\n",
      "1. Use a genepop file that has only polymorphic loci: ie 16409_North_polymorphic.txt. This can be made using a list of loci that are polymorphic and the script called subset_genepop_by_SNPs.pl \n",
      "\n",
      "2. Use PGDspider to convert the genepop to Bayescan format (GESTE)\n",
      "\n",
      "3. Edit the Bayescan format by hand: \n",
      "    Convert the spaces to Tabs\n",
      "    \n",
      "4. Use the python script to convert it to the Bayenv2 format\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#subset_genepop_by_SNPs.pl\n",
      "\n",
      "#Subset a specified set of SNPs and output appended genepop file\n",
      "#written by Wes Larson\n",
      "\n",
      "#arg1 SNPlist each line the full name of a SNP from the header in the genepop file\n",
      "#arg2 genepop file\n",
      "\n",
      "#usage\n",
      "#perl subset_genepop_by_SNPs.pl SNP_names.txt 96_snps_2_1_genepop.txt > new_genepop_file.txt\n",
      "#need to change this to match your SNP name prefix i.e. Ots, MY, One, RAD\n",
      "\n",
      "print(\"Title Line:\\n\");\n",
      "\n",
      "open(SNPs, \"<$ARGV[0]\") or die \"Error!!! reading $ARGV[0] for reading\";\n",
      "@SNP_names=();\n",
      "while(<SNPs>)\n",
      "{\n",
      "  chomp($_);\n",
      "  $SNP=\"$_\";\n",
      "  push(@SNP_names, $SNP);\n",
      "}\n",
      "close SNPs;\n",
      "\n",
      "#print(\"@SNP_names\");\n",
      "\n",
      "\n",
      "open(IN, \"<$ARGV[1]\") or die \"Error!!! reading $ARGV[1] for reading\"; #opens genepop file\n",
      "$z=0;\n",
      "@SNP_index=();\n",
      "while(<IN>)\n",
      " {\n",
      "    chomp($_);\n",
      "    $line=$_;\n",
      "    foreach $SNP (@SNP_names)\n",
      "    {\n",
      "      if($line eq $SNP) \n",
      "      {\n",
      "      print(\"$line\\n\");     \n",
      "      push(@SNP_index,$z);\n",
      "      }      \n",
      "    }\n",
      "    if($line !~m/Title|Pop|pop|,|Stacks/){$z++;}\n",
      "  }   \n",
      "  close IN;  \n",
      "\n",
      "open(IN, \"<$ARGV[1]\") or die \"Error!!! reading $ARGV[1] for reading\"; #opens genepop file\n",
      "while(<IN>)\n",
      "  {\n",
      "  chomp($_);\n",
      "  $line=$_;\n",
      "    if($line=~m/Pop|pop/){print(\"Pop\\n\");}\n",
      "    if($line =~ m/,/)\n",
      "    {\n",
      "    $ind=$line;\n",
      "    @split_ind_line=split(\" \" , $ind);\n",
      "    print(\"$split_ind_line[0]\\t\");\n",
      "      foreach $index (@SNP_index){print(\"$split_ind_line[$index+1]\\t\");}\n",
      "    print(\"\\n\");\n",
      "    }\n",
      "  }\n",
      "  close IN;\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Making the lineage genotype files using pre-edited (only polymorphic loci) genepop files. \n",
      "\n",
      "Using genepop files that had been edited to contain only polymorphic loci for either North America or Asia, I made the input files for Bayenv2. This led to some complications, as detailed below. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##A record of issues with 16409_North_polymorphic.txt "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There is something wrong with the file 16409_North_polymorphic.txt. \n",
      "\n",
      "The conversion script wont work to convert it from bayescan to bayenv2 because some of the loci only have one allele.\n",
      "\n",
      "I got the original file from the  Arlequin folder, renamed it 16409_North_polymorphic.txt and restarted it in PGD spider to convert it to the bayescan format, but it was still missing a single locus. \n",
      " \n",
      "I went back to the original Genepop file again and deleted the title line, renamed it 16409_genepop.txt and tried pgdspider again, it did not solve the problem. There is still missing a locus.\n",
      "\n",
      "I tried it on the file that has all the individuals from all the populations, but the outcome is still not right. \n",
      "\n",
      "I remade the 16409 file from the 16681 genepop file and a list of the polymorphic loci from Arlequin: \n",
      "\n",
      "perl subset_genepop_by_SNPs.pl NApolymorphic.txt 16681_80_genepop_one_per_line.txt > 16409_revamp.txt\n",
      "\n",
      "It still failed to get the number of samples right. SO I went online and googled other ways of converting from genepop to bayescan format, and got this guys code from github: \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#markravinet/bayescan_convert\n",
      "\n",
      "#bayescan_convert/big_convert.R\n",
      "#@markravinet markravinet on Jun 6, 2014 Script upload\n",
      "\n",
      "#### Convert LARGE genepop SNP file to BayesScan input file ####\n",
      "# To be used on cluster\n",
      "# Mark Ravinet September 2013, University of Gothenburg\n",
      "## Modified heavily from code provided by Kevin Keenan (cheers Kevin!)\n",
      "\n",
      "\n",
      "big_convert <- function(infile = FALSE, outfile = FALSE){\n",
      "    #require(diveRsity)\n",
      "\n",
      "    fastScan <- function(fname) {\n",
      "    # function reads genepop file in as a single vecto\n",
      "    s <- file.info(fname)$size\n",
      "    buf <- readChar(fname, s, useBytes = TRUE)\n",
      "    return(strsplit(buf, \"\\n\", fixed = TRUE, useBytes = TRUE)[[1]])\n",
      "  }\n",
      "  \n",
      "  # fastscan - read in as a vector\n",
      "  dat <- fastScan(fname = infile)\n",
      "  \n",
      "  if (length(strsplit(dat[length(dat)], split = \"\\\\s+\")[[1]]) == \n",
      "       1) {\n",
      "    dat <- dat[-(length(dat))]\n",
      "  }\n",
      "  rm(fastScan)\n",
      "  z <- gc()\n",
      "  rm(z)\n",
      "  popLocation <- grep(\"^([[:space:]]*)POP([[:space:]]*)$\", \n",
      "                      toupper(dat))\n",
      "  pop_pos <- c(popLocation, (length(dat) + 1))\n",
      "  loci_names <- as.vector(sapply(dat[2:(pop_pos[1] - 1)], \n",
      "                                 function(x) {\n",
      "                                   gsub(pattern = \"\\\\s+\", replacement = \"\", x)\n",
      "                                 }))\n",
      "  popSizes <- NULL\n",
      "  for (i in 1:(length(pop_pos) - 1)) {\n",
      "    popSizes[i] <- length((pop_pos[i] + 1):(pop_pos[(i + \n",
      "                                                       1)] - 1))\n",
      "  }\n",
      "  pops <- dat[-(c(1:(popLocation[1] - 1), popLocation))]\n",
      "  popList <- lapply(seq_along(popSizes), function(i) {\n",
      "    if (i == 1) {\n",
      "      indx <- 1:popSizes[i]\n",
      "    }\n",
      "    else {\n",
      "      indx <- (sum(popSizes[1:(i - 1)]) + 1):((sum(popSizes[1:(i - \n",
      "                                                                 1)])) + popSizes[i])\n",
      "    }\n",
      "    return(pops[indx])\n",
      "  })\n",
      "  npops <- length(popList)\n",
      "  nloci <- length(loci_names)\n",
      "  pop_sizes <- popSizes\n",
      "  rm(dat, pops)\n",
      "  z <- gc(reset = TRUE)\n",
      "  rm(z)\n",
      "  testStr <- strsplit(popList[[1]][1], split = \"\\\\s+\")[[1]]\n",
      "  gpEst <- sapply(testStr, function(x) {\n",
      "    if (is.character(x)) {\n",
      "      nchar(x)/2\n",
      "    }\n",
      "    else {\n",
      "      NA\n",
      "    }\n",
      "  })\n",
      "  rm(testStr)\n",
      "  gp <- as.numeric(names(sort(-table(gpEst)))[1])\n",
      "  prePopList <- lapply(popList, function(x) {\n",
      "    y <- array(data = NA, dim = c(length(x), (nloci + 1), \n",
      "                                  2))\n",
      "    colnames(y) <- c(\"ind\", loci_names)\n",
      "    for (j in 1:length(x)) {\n",
      "      data <- strsplit(x[j], split = \"\\\\s+\")[[1]]\n",
      "      if (data[2] == \",\") {\n",
      "        data <- data[-2]\n",
      "      }\n",
      "      data[data == \"0000\"] <- NA\n",
      "      data[data == \"NANA\"] <- NA\n",
      "      data[data == \"0\"] <- NA\n",
      "      data[data == \"000000\"] <- NA\n",
      "      data[data == \"999999\"] <- NA\n",
      "      data[data == \"-9-9\"] <- NA\n",
      "      y[j, 2:(nloci + 1), 1] <- substr(data[2:(nloci + \n",
      "                                                 1)], 1, gp)\n",
      "      y[j, 2:(nloci + 1), 2] <- substr(data[2:(nloci + \n",
      "                                                 1)], gp + 1, gp * 2)\n",
      "      y[j, 1, 1] <- data[1]\n",
      "      y[j, 1, 2] <- data[1]\n",
      "    }\n",
      "    return(y)\n",
      "  })\n",
      "  rm(popList)\n",
      "  ind_names <- lapply(prePopList, function(x) {\n",
      "    return(x[, 1, 1])\n",
      "  })\n",
      "  pop_names <- sapply(ind_names, function(x) {\n",
      "    return(x[1])\n",
      "  })\n",
      "  \n",
      "  # From here the function diverges from BigDivPart and uses\n",
      "  # some code lifted from bigPreDiv\n",
      "  popList <- lapply(prePopList, function(x){\n",
      "    return(x[,(2:(nloci+1)),])\n",
      "  })\n",
      "  # count the numbers of individuals typed per population\n",
      "  indtyp <- lapply(popList, function(x){\n",
      "    apply(x, 2, function(y){\n",
      "      length(na.omit(y[,1]))\n",
      "    })\n",
      "  })\n",
      "  # get unique alleles per pop\n",
      "  alls <- lapply(seq_along(popList), function(i){\n",
      "    apply(popList[[i]], 2, function(x){\n",
      "      return(unique(c(x[,1], x[,2])))\n",
      "    })\n",
      "  })\n",
      "  # get unique alleles across pops (maybe unneccess)\n",
      "  all_alleles <- lapply(1:nloci, function(i){\n",
      "    alleles <- lapply(alls, function(x){\n",
      "      return(x[[i]])\n",
      "    })\n",
      "    return(sort(unique(unlist(alleles))))\n",
      "  })\n",
      "  rm(alls)\n",
      "  # count all observed allele numbers per population\n",
      "  # (parallel is slower)\n",
      "  obsAlls <- lapply(popList, function(x){\n",
      "    apply(x, 2, function(y){\n",
      "      als <- unique(c(na.omit(y[,1]), na.omit(y[,2])))\n",
      "      counts <- sapply(als, function(z){ # takes allele, then looks for it in y\n",
      "      res <- length(which(y == z))   # returns count of alleles\n",
      "      names(res) <- names(z)\n",
      "      return(res)\n",
      "      })\n",
      "    })\n",
      "  })\n",
      "  \n",
      "  # observed alleles \n",
      "  obs_all <- lapply(1:nloci, function(i){\n",
      "    loc <- matrix(nrow = length(all_alleles[[i]]),\n",
      "                  ncol = npops)\n",
      "    rownames(loc) <- all_alleles[[i]]\n",
      "    for(j in 1:npops){\n",
      "      o <- obsAlls[[j]][[i]]\n",
      "      loc[names(o), j] <- o\n",
      "    }\n",
      "    loc[is.na(loc)] <- 0\n",
      "    return(loc)\n",
      "  })\n",
      "  rm(obsAlls)\n",
      "\n",
      "  # Some other necessary calculations\n",
      "  lociID <- 1:nloci\n",
      "  n_alls <- unlist(lapply(all_alleles, length))\n",
      "  rm(all_alleles)\n",
      "  \n",
      "  #### Produce BayesScan file ####\n",
      "  format_write <- function(info, add = TRUE){\n",
      "    cat(unlist(info), file = outfile, append = add)\n",
      "    write(\" \", file = outfile, append = TRUE)\n",
      "  }\n",
      "  \n",
      "  big_pop_format <- function(pop){\n",
      "    total_genes_pop <- as.numeric(unlist(indtyp[[pop]]))*2\n",
      "    obs_all <- sapply(1:nloci, function(x) obs_all[[x]][, pop])\n",
      "    # This line is what should be written per locus to a file\n",
      "    pop_info <- paste(\"[pop]=\", pop, \"\\n\", sep = \"\")\n",
      "    obs_loc_pop <- sapply(1:nloci, function(j) {\n",
      "      as.vector(c(lociID[j], total_genes_pop[j], n_alls[j], obs_all[[j]], \"\\n\"))\n",
      "    })\n",
      "    out <- list(pop_info, obs_loc_pop)\n",
      "    return(out)\n",
      "  }\n",
      "  \n",
      "  # Write header\n",
      "  header <- paste(\"[loci]=\", nloci, \"\\n\\n\", \"[populations]=\", npops, \"\\n\", sep = \"\")\n",
      "  format_write(header, add = FALSE)\n",
      "  # Write pop info\n",
      "  info <- sapply(1:length(pop_sizes), big_pop_format, simplify = FALSE)\n",
      "  lapply(1:length(pop_sizes), function(x) format_write(info[[x]], add = TRUE))\n",
      "  \n",
      "  \n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The program ouputs 16409 loci, but the last one in each population is empty: 0 0 for the count. \n",
      "\n",
      "Looking at the input file, here the 16409_revamp.txt, at the end of each line there are 2 tabs. At the end of a line of a regular genepop file there are no tabs. \n",
      "\n",
      "To address this, I tried a couple things with the 16681_80_genepop_one_per_line.txt file and the subset script to see if I could get the right number of loci. \n",
      "\n",
      "1. change the line endings\n",
      "2. remove the actual name of the population after POP\n",
      "3. make sure that the last locus is actully in the file\n",
      "\n",
      "None of these solved the problem, each time there are one too few loci and the lines end in two tabs, where there should be no tabs. \n",
      "\n",
      "There is nothing wrong with the file that has the North American subset loci. I tested this by subsetting the genotypes with the NA monomorphic list, and there were no problems. \n",
      "\n",
      "The issue seems to be locus 100029_14.  If you take it out, the file looks fine. the strange thing is that the input file does have a genotype for that locus and the script to subset the genotypes doesnt have any digit limit on the name of the loci, so the longer name than usual is not an issue. \n",
      "\n",
      "What worries me is that this might have been a problem with other input files the I may have missed. The good news is that since locus 100029_14 is the last locus on the list, were it left out of any analysis, it would not have altered the indexing of the other loci. \n",
      "\n",
      "The good news is that the very original genepop file batch_3_16681_pop_80.genepop has all the genotypes. \n",
      "\n",
      " "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Making the genotype files for the lineages\n",
      "\n",
      "The populations are numbered from 1 to 14, in this order: \n",
      "\n",
      "- AMUR10\n",
      "- AMUR11\n",
      "- HAYLY09\n",
      "- HAYLY10\n",
      "- KOPPE91\n",
      "- KOPPE96\n",
      "- KUSHI06\n",
      "- KUSHI07\n",
      "- NOME91\n",
      "- NOME94\n",
      "- SNOH03\n",
      "- SNOH96\n",
      "- TAUY09\n",
      "- TAUY12\n",
      "\n",
      "The following matrix has the index number of the populations and the environmental data, which is in the same order, for each of the groupings. This is taken from excel, where formulas pulled the requisite data for each of the files.  \n",
      " \n",
      "asia_odd_lat\t\t\t2\t3\t8\t13\n",
      "asia_even_lat\t\t\t1\t4\t7\t14\n",
      "NA_odd_lat\t\t\t\t5\t9\t11\t\n",
      "NA_even_lat\t\t\t\t6\t10\t12\t\n",
      "\n",
      "  \n",
      "I used the 16681 genepop file, and deled the populations that I was not comparing. I named each of the files using this naming scheme: ASE ASO NAO NAE.\n",
      "\n",
      "Because the genotype files were not exclusively polymorphic, I used the code that Garrett wrote in perl to remove the monomorphic loci. The script he wrote creates a file that contains the loci that passed the test (being polymorphic, ie not having 0 0 0 etc as one of the allele counts) in the same format as required by the program. It also creates a list of the loci that did not pass the test. The list is a list of the index number of the loci that failed, and starts at 1, like the number scheme in excel. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "perl removeZeroCounts.pl NAE_bayesenv_in.txt\n",
      "perl removeZeroCounts.pl NAO_bayesenv_in.txt\n",
      "perl removeZeroCounts.pl ASE_bayesenv_in.txt\n",
      "perl removeZeroCounts.pl ASO_bayesenv_in.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###The environmental variable file. \n",
      "\n",
      "The environmental files are formatted with each line as an environmental variable across the populations. The values are tab separated and each line ends in a tab followed by an end of line character. The variables are listed in the same population order as they appear in the genotype file. \n",
      "The environmental variable should be standardized, so that the mean is zero and they values have a standard deviation of one. \n",
      "\n",
      "I have four variables, details about how I came to them are in the ipython notebook calld ##########Environmental variables. \n",
      "\n",
      "Im using the latitude and longitude for each location as well as the PC axies 1 and 2 from a PCA of the mean temperature and average rainfall of each month for each location.  \n",
      "\n",
      "The environmental variables need to be in the same order as the populations in the genotype file. \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###The covariance matrix\n",
      "\n",
      "The program will generate a lot of covariance matrices and they should all be pretty much identical. From there you can take the average of a couple or you can just pick one of them. Just look through them and make sure that they are all pretty much the same. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Estimating the covariance matrix "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is the example command from the manual: \n",
      "\n",
      "./bayenv2 -i SNPSFILE -p NUMPOPS -k 100000 -r 14637 >matrix.out\n",
      "\n",
      "And a bit on the command and the covariance matrix itself from the manual: \n",
      "\n",
      "This outputs the current draw of the covariance matrix into matrix.out every 500 iterations. The rows and columns in the covariance matrix appear in the same population order as they appeared in the allele count file. The covariance matrix can be visualized by the image command in R, the cov2cor function can be used to convert a covariance matrix into a correlation matrix. The correlation matrix computed from the estimated covariance matrix should be very correlated with the matrix of pairwise FST (see the paper for discussion). The matrix should be inspected for unexpected low or high correlations, as judged from pairwise FST , as these may indicate problems in the labeling of populations. Note that the entries of the covariance matrix are not forced to be positive, thus small negative entries in the covariance matrix are possible. Matrices should be compared within an across independent runs of the program to ensure that the matrix is well estimated.  \n",
      "\n",
      "\n",
      "Here is the command that I ran on Garrett's computer to get the covariance matrix for all 14 populations: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "./bayenv2 -i 16681_80_bayenv2_in.txt -p 14 -k 100000 -r 14637 >matrix.out 2>&1 matrix_log.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here is the command that I ran on Garrett's computer to get the covariance matrices for each of the four groups, as they each require a separate covariance matrix. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "./bayenv2 -i NAE_bayesenv_in_filtered.txt -p 3 -k 100000 >NAE_matrix.out\n",
      "./bayenv2 -i NAO_bayesenv_in_filtered.txt -p 3 -k 100000 >NAO_matrix.out\n",
      "./bayenv2 -i ASE_bayesenv_in_filtered.txt -p 4 -k 100000 >ASE_matrix.out\n",
      "./bayenv2 -i ASO_bayesenv_in_filtered.txt -p 4 -k 100000 >ASO_matrix.out\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Understanding the matrix output of Bayenv2\n",
      "\n",
      "I have some questions about the output of the program. \n",
      "\n",
      "First, what is the pop_spec.out file and is it OK that mine is all zeros? \n",
      "\n",
      "Second, how do I know if the covariance matrices are \"a small set of very similar covariance matrices\"? \n",
      "\n",
      "The first is still a bit of a mystery, Ryan's file is empty. As for the second question, Ryan explained that the diagonals are the variance within a population, and the other cells are the covariance between the populations, pairwise. So the more closely related the populations are to eachother, or the more similar the populations are to eachother, the bigger the number should be. If the number is zero than the populations are not related at all and knowing something about one population will tell you absolutely nothing about the other population.\n",
      "\n",
      "The covariance can be negative. Realistically, variance should be positive, but what is happening is that its calculated on standardized data, which means that the data has a mean and the covariance is telling us the deviation of them from the mean. Naturally, some will fall above the mean and some will be below the mean. \n",
      "\n",
      "The diagonals, of the covariance matrices should be the numbers that vary the least between covariance matrices because they show the variation within the populatins.  That within population variation should be less variable between the runs of the covariance matrix as it is less dependent on other factors. \n",
      "\n",
      "Looking at the covariance matrix for all the populations:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\t\n",
      "\tVAR-COVAR MATRIX: ITER = 99500\n",
      "2.607952e-01\t2.408774e-01\t2.412271e-01\t2.611193e-01\t2.435891e-01\t2.611281e-01\t2.624981e-01\t2.411676e-01\t2.412103e-01\t2.600593e-01\t2.492190e-01\t2.627749e-01\t2.413656e-01\t2.603849e-01\t\n",
      "2.408774e-01\t2.439801e-01\t2.421166e-01\t2.420893e-01\t2.455869e-01\t2.411968e-01\t2.423487e-01\t2.445997e-01\t2.420919e-01\t2.405774e-01\t2.536834e-01\t2.427992e-01\t2.425350e-01\t2.413946e-01\t\n",
      "2.412271e-01\t2.421166e-01\t2.421246e-01\t2.424411e-01\t2.447295e-01\t2.416624e-01\t2.427418e-01\t2.435000e-01\t2.412529e-01\t2.409669e-01\t2.527309e-01\t2.433016e-01\t2.416363e-01\t2.417605e-01\t\n",
      "2.611193e-01\t2.420893e-01\t2.424411e-01\t2.632416e-01\t2.450296e-01\t2.624906e-01\t2.636209e-01\t2.423635e-01\t2.424429e-01\t2.613356e-01\t2.508730e-01\t2.642203e-01\t2.425988e-01\t2.616443e-01\t\n",
      "2.435891e-01\t2.455869e-01\t2.447295e-01\t2.450296e-01\t2.525655e-01\t2.456028e-01\t2.448108e-01\t2.469368e-01\t2.448334e-01\t2.435311e-01\t2.624722e-01\t2.482239e-01\t2.451158e-01\t2.442712e-01\t\n",
      "2.611281e-01\t2.411968e-01\t2.416624e-01\t2.624906e-01\t2.456028e-01\t2.643888e-01\t2.635775e-01\t2.413674e-01\t2.417208e-01\t2.615159e-01\t2.524568e-01\t2.656636e-01\t2.417774e-01\t2.617215e-01\t\n",
      "2.624981e-01\t2.423487e-01\t2.427418e-01\t2.636209e-01\t2.448108e-01\t2.635775e-01\t2.661666e-01\t2.426014e-01\t2.427120e-01\t2.626095e-01\t2.501021e-01\t2.651486e-01\t2.428876e-01\t2.629219e-01\t\n",
      "2.411676e-01\t2.445997e-01\t2.435000e-01\t2.423635e-01\t2.469368e-01\t2.413674e-01\t2.426014e-01\t2.471917e-01\t2.434992e-01\t2.408211e-01\t2.551210e-01\t2.429389e-01\t2.439459e-01\t2.416743e-01\t\n",
      "2.412103e-01\t2.420919e-01\t2.412529e-01\t2.424429e-01\t2.448334e-01\t2.417208e-01\t2.427120e-01\t2.434992e-01\t2.421314e-01\t2.409748e-01\t2.529524e-01\t2.433953e-01\t2.416149e-01\t2.417464e-01\t\n",
      "2.600593e-01\t2.405774e-01\t2.409669e-01\t2.613356e-01\t2.435311e-01\t2.615159e-01\t2.626095e-01\t2.408211e-01\t2.409748e-01\t2.612199e-01\t2.492890e-01\t2.632430e-01\t2.411145e-01\t2.606044e-01\t\n",
      "2.492190e-01\t2.536834e-01\t2.527309e-01\t2.508730e-01\t2.624722e-01\t2.524568e-01\t2.501021e-01\t2.551210e-01\t2.529524e-01\t2.492890e-01\t2.772195e-01\t2.559598e-01\t2.531670e-01\t2.500269e-01\t\n",
      "2.627749e-01\t2.427992e-01\t2.433016e-01\t2.642203e-01\t2.482239e-01\t2.656636e-01\t2.651486e-01\t2.429389e-01\t2.433953e-01\t2.632430e-01\t2.559598e-01\t2.692204e-01\t2.434116e-01\t2.634254e-01\t\n",
      "2.413656e-01\t2.425350e-01\t2.416363e-01\t2.425988e-01\t2.451158e-01\t2.417774e-01\t2.428876e-01\t2.439459e-01\t2.416149e-01\t2.411145e-01\t2.531670e-01\t2.434116e-01\t2.429301e-01\t2.418841e-01\t\n",
      "2.603849e-01\t2.413946e-01\t2.417605e-01\t2.616443e-01\t2.442712e-01\t2.617215e-01\t2.629219e-01\t2.416743e-01\t2.417464e-01\t2.606044e-01\t2.500269e-01\t2.634254e-01\t2.418841e-01\t2.617930e-01\t\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tells us the sort of pattern and magnitude in number we should expect. \n",
      "\n",
      "Mine do not follow this pattern. The numbers are incredibly variable and the magnitude is way smaller, the numbers are nearly zero. \n",
      "\n",
      "The issue that I might be running into is that the program may not be designed to work well on so few populations. The program gets a lot of its power from the number of populations it has, because the greater the number of populations, the greater the structure it can draw from. \n",
      "\n",
      "I need to make sure that my input files are correct. Another issue is that the program is misreading the genotypes. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Testing the covergence of the MCMC chain\n",
      "\n",
      "We have to test that the chain did stabalize in the end and that there was covergence. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Distance transforming the environmental variables \n",
      "\n",
      "The manual for Bayenv2 explains that the program distance transforms the environmental variables, meaning that it takes the standardized environmental variables and takes their absolute value.\n",
      "\n",
      "That makes no sense to me, because it ruins the spread. It means that values that are really far on the edge of the scale have the same values after you take the absolute value of them, for instanve a -1.6 will be the same as a 1.6 when in fact they're really really different. Lisa says that that doesnt seem right to her either.\n",
      "\n",
      "I looked at the Borrin paper that used Bayenv2 to see if they mention how the approached this issue. But all that I could find is that they used the standardized numbers and let the program take the absolute values. This is where I got their info: http://datadryad.org/resource/doi:10.5061/dryad.8hm4p\n",
      "\n",
      "Im thinking of solving this problem by rescaling my environmental variables to make them all  positive."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Restandardizing the environmental variables\n",
      "\n",
      "Ryan also thought that I should consider restandardizing the environmental variables for each of the sets of populations that I'm running. For instance, the environmental variables for Asia would just be the Asian range, not standardized taking the North American variables into account. \n",
      "\n",
      "The balance comes down to what is statistically appropriate, versus biologically interpretable. The program assumes that the variables are standardized, that they have a mean of zero. So it is not clear what happens when they do not have a mean of zero because they have been standardized outside that test. A mean of zero means that that there is no effect if you are assuming a linear relationship. Some pops should be high and some should be low. If you have populations that are all above the mean, then the model doesnt work because the variable isnt splitting them up, they arent different enough. \n",
      "\n",
      "Ryan thinks that the real answer is that I dont have to run seperate tests for each of the lineages. That the test of all the lineages together with the environmental variables standardized is good enough. To him, there is no clear benefit to ding it seperately. He thinks I should look at how people who have a strict hierarchy like this, dealt with the problem. How well does this model deal with the hierarchy? \n",
      "\n",
      "If I do more than one test, make sure that the variables are standardized for each of the groupings. \n",
      "\n",
      "The real issue is that it is not clear how to interpret the output of the program. What it gives you is a kindof likelihood ratio, something that is akin to the relative support of the two different models. The authors are very clear about not comparing the results across two differnet environmental variables, they want to make sure that you only compare within a single test. Even if you test multiple environmental variables within a set of populations, you cant compare those. So that really complicates the way that  you interpret the results. \n",
      "\n",
      "he says that the way that people interpret these test results is that they take the log or the negative log of the Bayes Factor, he cant remember if the numbers are closer to 0 or not, and then pick a threshold. That is what people usually do. \n",
      "\n",
      "As for the covariance matrix,  you dont have to estimate it from the set of SNPs that you are testing, so you can use a subset.  You just dont want to pick a subset of SNPs that is going to really skew the matrix (which he said is pretty hard to do)\n",
      "\n",
      "Take the covariance matrices that I have and plot them as a heatmap, and see if they look the same as the pairwise Fsts for the populations. There are some things in the manual about how to convert the matrix into a correlation matrix. \n",
      "\n",
      "The fact that they look so weird is not good though, even if they do roughly follow the pattern that you would expect from the populations you do not want to use something that has not reached some stability- they should not be so different from eachother. \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Running Bayenv2 in test mode\n",
      "\n",
      "The manual for the program says that we have to have one file for each of the SNPs, but it seems like he has changed that in the newest version. The webssite says that they added a bash script which loops through all SNPs in a file. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is the covariance matrix that I'm using for the run of all the populations together, it is the very last of the covariance matrices in the file. named all_matrix.txt"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "2.671140e-01\t2.476034e-01\t2.479405e-01\t2.676491e-01\t2.504305e-01\t2.681293e-01\t2.689347e-01\t2.479830e-01\t2.479900e-01\t2.665939e-01\t2.564534e-01\t2.695237e-01\t2.479350e-01\t2.667363e-01\t\n",
      "2.476034e-01\t2.507577e-01\t2.487477e-01\t2.488835e-01\t2.522044e-01\t2.484559e-01\t2.491440e-01\t2.514451e-01\t2.488872e-01\t2.474580e-01\t2.605381e-01\t2.497338e-01\t2.490369e-01\t2.480980e-01\t\n",
      "2.479405e-01\t2.487477e-01\t2.486115e-01\t2.492353e-01\t2.511884e-01\t2.489061e-01\t2.495363e-01\t2.501700e-01\t2.478813e-01\t2.478282e-01\t2.594369e-01\t2.502319e-01\t2.480095e-01\t2.484203e-01\t\n",
      "2.676491e-01\t2.488835e-01\t2.492353e-01\t2.700200e-01\t2.519184e-01\t2.697335e-01\t2.702754e-01\t2.492430e-01\t2.492850e-01\t2.680888e-01\t2.581488e-01\t2.712016e-01\t2.492255e-01\t2.682166e-01\t\n",
      "2.504305e-01\t2.522044e-01\t2.511884e-01\t2.519184e-01\t2.589466e-01\t2.529508e-01\t2.516740e-01\t2.535831e-01\t2.514377e-01\t2.505001e-01\t2.690910e-01\t2.552291e-01\t2.514696e-01\t2.510296e-01\t\n",
      "2.681293e-01\t2.484559e-01\t2.489061e-01\t2.697335e-01\t2.529508e-01\t2.720504e-01\t2.706728e-01\t2.486882e-01\t2.490178e-01\t2.687104e-01\t2.602646e-01\t2.731009e-01\t2.488714e-01\t2.687477e-01\t\n",
      "2.689347e-01\t2.491440e-01\t2.495363e-01\t2.702754e-01\t2.516740e-01\t2.706728e-01\t2.727068e-01\t2.494709e-01\t2.495652e-01\t2.692565e-01\t2.573981e-01\t2.719873e-01\t2.494963e-01\t2.693758e-01\t\n",
      "2.479830e-01\t2.514451e-01\t2.501700e-01\t2.492430e-01\t2.535831e-01\t2.486882e-01\t2.494709e-01\t2.540928e-01\t2.503386e-01\t2.477950e-01\t2.620058e-01\t2.499371e-01\t2.505006e-01\t2.484480e-01\t\n",
      "2.479900e-01\t2.488872e-01\t2.478813e-01\t2.492850e-01\t2.514377e-01\t2.490178e-01\t2.495652e-01\t2.503386e-01\t2.489254e-01\t2.479099e-01\t2.597804e-01\t2.503591e-01\t2.481505e-01\t2.484992e-01\t\n",
      "2.665939e-01\t2.474580e-01\t2.478282e-01\t2.680888e-01\t2.505001e-01\t2.687104e-01\t2.692565e-01\t2.477950e-01\t2.479099e-01\t2.679651e-01\t2.566609e-01\t2.701958e-01\t2.478305e-01\t2.671574e-01\t\n",
      "2.564534e-01\t2.605381e-01\t2.594369e-01\t2.581488e-01\t2.690910e-01\t2.602646e-01\t2.573981e-01\t2.620058e-01\t2.597804e-01\t2.566609e-01\t2.840816e-01\t2.634333e-01\t2.597404e-01\t2.572251e-01\t\n",
      "2.695237e-01\t2.497338e-01\t2.502319e-01\t2.712016e-01\t2.552291e-01\t2.731009e-01\t2.719873e-01\t2.499371e-01\t2.503591e-01\t2.701958e-01\t2.634333e-01\t2.764342e-01\t2.501883e-01\t2.701996e-01\t\n",
      "2.479350e-01\t2.490369e-01\t2.480095e-01\t2.492255e-01\t2.514696e-01\t2.488714e-01\t2.494963e-01\t2.505006e-01\t2.481505e-01\t2.478305e-01\t2.597404e-01\t2.501883e-01\t2.491888e-01\t2.484280e-01\t\n",
      "2.667363e-01\t2.480980e-01\t2.484203e-01\t2.682166e-01\t2.510296e-01\t2.687477e-01\t2.693758e-01\t2.484480e-01\t2.484992e-01\t2.671574e-01\t2.572251e-01\t2.701996e-01\t2.484280e-01\t2.681819e-01\t"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}