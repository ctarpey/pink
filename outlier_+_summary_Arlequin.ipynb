{
 "metadata": {
  "name": "",
  "signature": "sha256:c62e93b21358c9463482ce96ed19b7ad9c0c7979a5be57a34684db76c3eba153"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Outlier test and Summary Statistics with Arlequin "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Arlequin"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "       $$ September 10 2015\n",
      "==================================================================\n",
      "lepMap at chr 4 and 5\n",
      "\n",
      "I talked to lisa about the problems I had had in Arlequin, and she said to forget that one locus 100029_14, let it go. Just make sure its the only  one that is missing from the files and just move on. (she also said to figure out if its the name or the fact that it is the last thing in the file) \n",
      "\n",
      "Then she looked at the results of the Arlequin and pulled up some of the projects that she had on her computer. It didnt look like she had used the hierarchical model, so that might be what was the difference? Anyway, she asked for my .arp file and is checking it out on her computer to see if she can get it to run.\n",
      "\n",
      "I went over to Garretts, and she is messing with the Asian arp so I started the ALL 16681 and the North American 15945- I think the secret is that they shouldnt be run with the hierarchical model because they were totally working. \n",
      "\n",
      "She said to plot the null distribution and then plot the outliers over top of it. \n",
      "\n",
      "I want to continue to put together the analysisMetaData sheet. \n",
      "\n",
      "Ok, So im looking for the results for NA to put in the sheet and in the only output that has the individual based F statistics, I find this: \n",
      "********************\n",
      "Locus by locus AMOVA:\n",
      "********************\n",
      "\n",
      "Distance method for locus-by-locus analysis: Pairwise differences\n",
      "\n",
      "List of loci with only one allele:\n",
      "----------------------------------\n",
      "\n",
      "94 172 180 200 206 228 249 266 294 307 377 515 528 576 642 692 700 743 746 747 812 817 818 829 889 916 925 962 996 1003 1023 1051 1057 1084 1095 1096 1121 1163 1213 1218 1248 1250 1274 1285 1358 1365 1402 1425 1450 1453 1457 1472 1490 1499 1574 1610 1662 1695 1704 1717 1719 1744 1750 1792 1807 1809 1813 1848 1899 1959 1983 1987 2000 2041 2075 2097 2111 2119 2124 2127 2144 2212 2216 2230 2242 2339 2373 2469 2478 2534 2537 2560 2566 2592 2613 2626 2644 2694 2724 2740 2751 2761 2792 2813 2833 2852 2865 2876 2884 2897 3060 3107 3128 3174 3189 3193 3203 3223 3237 3251 3356 3363 3414 3457 3472 3530 3541 3598 3654 3657 3671 3676 3687 3696 3772 3822 3843 3932 3936 3938 3940 4012 4034 4071 4086 4102 4125 4203 4247 4250 4266 4294 4302 4333 4362 4369 4371 4373 4478 4536 4554 4603 4620 4772 4851 4905 4909 4930 5043 5052 5066 5118 5161 5196 5282 5348 5354 5362 5385 5404 5412 5453 5466 5501 5549 5554 5559 5634 5635 5638 5794 5815 5860 5877 5879 5901 5902 5940 5947 5970 5975 5994 6028 6033 6059 6082 6093 6099 6108 6123 6131 6137 6212 6250 6329 6337 6368 6391 6486 6527 6546 6550 6556 6591 6612 6632 6636 6641 6649 6673 6680 6688 6709 6737 6755 6807 6814 6842 6845 6873 6889 6989 7041 7076 7086 7087 7108 7155 7185 7254 7277 7283 7284 7305 7334 7341 7527 7555 7619 7629 7630 7641 7663 7787 7809 7828 7839 7930 7950 7955 7992 8000 8022 8044 8049 8092 8168 8174 8194 8204 8235 8238 8246 8253 8270 8290 8302 8336 8372 8450 8458 8460 8515 8524 8569 8575 8576 8601 8606 8607 8632 8664 8684 8828 8836 8945 8972 8994 9052 9082 9152 9156 9197 9201 9216 9382 9395 9446 9475 9535 9584 9613 9615 9636 9648 9658 9669 9715 9724 9805 9852 9876 9990 10023 10083 10153 10161 10285 10287 10307 10343 10366 10406 10407 10410 10549 10561 10593 10646 10669 10705 10730 10748 10777 10780 10815 10818 10824 10855 10857 10883 10886 10890 10915 10979 11023 11034 11051 11074 11084 11153 11211 11224 11232 11301 11353 11365 11422 11444 11453 11462 11504 11517 11528 11596 11611 11674 11693 11858 11871 11906 11961 11985 12023 12028 12073 12078 12136 12156 12162 12166 12168 12174 12200 12219 12258 12281 12341 12410 12433 12446 12453 12474 12475 12494 12499 12546 12590 12615 12653 12711 12721 12728 12778 12800 12812 12828 12856 12861 12877 13018 13038 13045 13051 13086 13088 13109 13118 13207 13223 13246 13264 13290 13315 13332 13360 13388 13395 13396 13397 13461 13576 13598 13645 13746 13806 13858 13863 13874 13921 13953 14021 14059 14094 14103 14107 14130 14157 14209 14223 14227 14246 14259 14293 14300 14315 14342 14352 14353 14378 14379 14389 14400 14401 14403 14408 14455 14525 14531 14533 14551 14570 14592 14599 14626 14679 14725 14733 14737 14752 14754 14832 14860 14955 14965 14974 14975 15042 15056 15075 15081 15095 15111 15116 15123 15158 15178 15187 15202 15218 15232 15260 15265 15279 15283 15288 15293 15311 15315 15350 15354 15397 15408 15412 15416 15423 15457 15477 15488 15537 15545 15551 15574 15589 15595 15602 15607 15618 15665 15682 15704 15733 15744 15757 15798 15804 15811 15824 15835 15877 15900 15907 15917 15919 15923 15924 15926 15937 15997 16007 16016 16030 16047 16061 16096 16115 16124 16127 16146 16160 16189 16210 16236 16271 16312 16334 16361 16366 16370 16373 16389 16391 16402 \n",
      "\n",
      "which i totally thought I had already solved that problem when i made the sheets that cut out the monomorphic loci. what has changed since i cut it down from 16681 to 16409 that there are  now all these new monomorphic loci? could it be possible that I messed up and deleted the wrong populations? \n",
      "\n",
      "im sure that the original list came from the 16681_80_named_NA run and was using the NA pops:  \n",
      "\t 1   KOPPE96  -0.01734            0.615836\n",
      "     2    NOME94   0.01221            0.407625\n",
      "     3    SNOH96  -0.02381            0.770283\n",
      "     4   KOPPE91  -0.00009            0.517107\n",
      "     5    NOME91  -0.00913            0.621701\n",
      "     6    SNOH03  -0.01152            0.641251\n",
      "\t \n",
      "\t \n",
      "But it still doesnt solve the problem of like what is going on? \n",
      "\n",
      "\n",
      "From 15945_Asian2.res this is the results when you do  GENETIC STRUCTURE ANALYSIS\n",
      "\n",
      "\n",
      "Number of usable loci for distance computation : 15259\n",
      "Allowed level of missing data                  : 0.05\n",
      "\n",
      "\n",
      "List of loci with too much missing data :\n",
      "-----------------------------------------\n",
      "    4    41   142   150   156   172   195   241   286   287   292   307   330   344   353    432   434   436   453   497   504   512   514   519   546   558   577   584   602   618    625   628   662   666   706   745   746   772   785   790   796   802   808   809   813    820   821   831   865   879   900   917   944   965   975   999  1021  1033  1064  1066   1082  1105  1108  1133  1183  1239  1242  1289  1293  1308  1318  1331  1355  1361  1371   1377  1395  1411  1415  1425  1459  1480  1534  1566  1567  1577  1594  1604  1679  1736   1765  1796  1814  1835  1905  1917  1943  1961  2001  2011  2034  2040  2044  2060  2066   2087  2091  2171  2179  2180  2225  2256  2308  2357  2365  2366  2371  2402  2415  2511   2549  2593  2605  2610  2622  2624  2649  2655  2660  2674  2713  2752  2761  2783  2786   2801  2863  2887  2930  2936  2967  3038  3075  3111  3150  3159  3243  3252  3306  3312   3338  3380  3431  3437  3442  3462  3468  3473  3527  3563  3571  3599  3719  3744  3765   3766  3822  3835  3845  3849  3851  3878  3882  3884  3886  3956  3996  4045  4048  4079   4118  4145  4148  4172  4205  4242  4256  4270  4274  4333  4365  4368  4435  4452  4470   4477  4481  4489  4501  4518  4520  4538  4558  4566  4578  4586  4602  4609  4613  4636   4656  4679  4698  4719  4763  4788  4798  4835  4853  4858  4866  4876  4898  4918  4947   4957  4968  4993  4994  5001  5019  5050  5100  5117  5124  5140  5166  5198  5224  5257   5258  5264  5322  5331  5345  5346  5355  5360  5389  5423  5465  5475  5491  5512  5523   5525  5584  5594  5602  5624  5628  5659  5664  5666  5669  5672  5691  5719  5728  5748   5762  5800  5809  5854  5864  5868  5926  5933  5937  5940  5978  5980  5991  5998  6005   6034  6102  6114  6123  6152  6164  6175  6179  6181  6200  6238  6244  6269  6284  6289   6291  6295  6310  6321  6334  6366  6405  6456  6460  6486  6495  6504  6521  6580  6586   6632  6659  6666  6694  6719  6726  6745  6750  6771  6811  6836  6846  6860  6940  6996   7012  7013  7014  7041  7057  7113  7174  7187  7188  7205  7213  7214  7223  7228  7241   7273  7400  7429  7433  7439  7471  7474  7484  7503  7514  7515  7533  7561  7577  7583   7584  7627  7631  7634  7694  7696  7711  7769  7795  7918  7927  7939  7941  7943  7950   7951  7966  7973  7993  8011  8079  8084  8113  8147  8156  8181  8248  8264  8285  8294   8308  8319  8344  8351  8358  8376  8408  8441  8445  8448  8480  8551  8552  8662  8673   8721  8746  8757  8826  8892  8926  8979  8988  8990  9024  9037  9060  9062  9083  9103   9105  9165  9172  9186  9192  9195  9200  9228  9236  9245  9264  9270  9271  9309  9318   9353  9373  9379  9444  9467  9483  9500  9510  9556  9588  9597  9614  9684  9698  9729   9743  9782  9790  9841  9853  9871  9877  9935  9938  9952  9973  10009  10033  10078  10080   10099  10135  10136  10144  10192  10203  10219  10305  10308  10315  10337  10352  10435  10460  10491   10494  10515  10530  10538  10557  10574  10608  10610  10674  10740  10745  10780  10782  10798  10836   10858  10860  10870  10876  10901  10905  10917  10965  10996  11038  11047  11056  11066  11074  11086   11092  11115  11116  11179  11197  11239  11266  11274  11281  11321  11368  11385  11432  11433  11451   11465  11491  11559  11613  11690  11732  11782  11795  11842  11865  11916  11980  12162  12191  12204   12216  12217  12262  12326  12338  12415  12482  12540  12574  12613  12620  12656  12668  12715  12716   12749  12759  12763  12773  12878  12903  12957  12976  12977  13036  13043  13051  13082  13115  13206   13227  13241  13276  13285  13300  13401  13409  13416  13476  13502  13506  13519  13526  13529  13589   13591  13597  13606  13631  13634  13688  13697  13705  13707  13769  13806  13833  13880  13884  13890   13895  13957  14025  14026  14042  14101  14122  14135  14149  14159  14188  14208  14246  14263  14269   14282  14296  14321  14340  14444  14447  14454  14472  14483  14505  14506  14570  14623  14626  14631   14653  14708  14758  14772  14779  14810  14820  14823  14951  14966  14986  14993  14994  15020  15039   15052  15094  15118  15144  15145  15195  15196  15198  15204  15212  15232  15245  15330  15358  15391   15422  15434  15464  15470  15472  15474  15532  15545  15615  15654  15701  15703  15756  15757  15803   15808  15858  15876  15934  15935  15936  15937  15939  15941  15943  15944  \n",
      "\t\n",
      "And the output for the Asian 15045 original has the same list of loci with too much missing data. OOOOOugh what is going on?\n",
      " \n",
      "Ok, so the levels for the missing data are different for those. And the arlequin manual says that you can set that level in the general settings tab. \n",
      "\n",
      "Lisa talked with me a long time about the programs and what I should be looking for and doing, she couldnt get the hierarchical version of arlequin to run; she thought it was because she was using an newer version, I am running a version from 2010, she just downloaded the 2015 and it was giving the same errors that I was having all along. \n",
      "\n",
      "WE talked about using the program galaxy to do some summary stats on the files to explore what the data looks like. \n",
      "\n",
      "So I think i have to figure out what the MAF and missing data are in those new data sets, throw out that one locus that was giving me trouble in the conversion and go from there. I have the perl scripts that garrett wrote to count the missing and look at the hets, and I can do basic genepop stats with the files and throw shit out from that too. \n",
      "\n",
      "         $$ October 19  2015\n",
      "==================================================================\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}